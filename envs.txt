{
    "name": "titanx-utility",
    "version": "0.1.0",
    "type": "module",
    "private": true,
    "scripts": {
        "dev": "next dev",
        "build": "NODE_ENV=production next build --debug",
        "start": "NODE_ENV=production next start",
        "lint": "next lint",
        "tailwind": "tailwindcss build -i ./app/global.css -o ./app/output.css --watch",
        "test": "jest"
    },
    "dependencies": {
        "@heroicons/react": "^2.2.0",
        "@tanstack/react-query": "^5.59.13",
        "@upstash/redis": "^1.34.4",
        "alchemy-sdk": "^3.4.2",
        "async-mutex": "^0.5.0",
        "chalk": "^5.4.1",
        "chart.js": "^4.4.3",
        "dotenv": "^16.4.5",
        "framer-motion": "^12.6.3",
        "next": "^15.3.1",
        "node-cache": "^5.1.2",
        "node-fetch": "^3.3.2",
        "p-limit": "^6.2.0",
        "react": "^18.3.0",
        "react-chartjs-2": "^5.3.0",
        "react-dom": "^18.3.1",
        "react-virtualized": "^9.22.6",
        "uuid": "^11.1.0",
        "viem": "^2.28.0",
        "zod": "^3.24.3",
        "zustand": "^5.0.3"
    },
    "devDependencies": {
        "@babel/plugin-syntax-dynamic-import": "^7.8.3",
        "@babel/plugin-transform-modules-commonjs": "^7.27.1",
        "@babel/preset-env": "^7.27.1",
        "@eslint/js": "^9.13.0",
        "@jest/globals": "^29.7.0",
        "@testing-library/jest-dom": "^6.6.3",
        "@testing-library/react": "^16.3.0",
        "@typescript-eslint/eslint-plugin": "^8.31.0",
        "@typescript-eslint/parser": "^8.31.0",
        "autoprefixer": "^10.4.20",
        "babel-jest": "^29.7.0",
        "esbuild-jest": "^0.4.0",
        "eslint": "^8.57.0",
        "eslint-config-next": "^15.3.1",
        "jest": "^29.7.0",
        "postcss": "^8.4.47",
        "supertest": "^7.1.0",
        "tailwindcss": "^3.4.13"
    },
    "resolutions": {}
}
// File: next.config.mjs
/** @type {import('next').NextConfig} */
const nextConfig = {
  reactStrictMode: true,
  serverExternalPackages: ['viem', 'alchemy-sdk'],
  webpack: (config) => {
    config.resolve.fallback = { fs: false, path: false };
    return config;
  },
};

export default nextConfig;{
    "compilerOptions": {
      "baseUrl": ".",
      "paths": {
        "@/*": ["./*"]
      }
    }
  }

  

/** @type {import('tailwindcss').Config} */
module.exports = {
  content: [
    './app/**/*.{js,ts,jsx,tsx}',
    './components/**/*.{js,ts,jsx,tsx}',
  ],
  theme: {
    extend: {},
  },
  plugins: [],
};

# File: .env.local
NEXT_PUBLIC_ALCHEMY_API_KEY=rzv6zozYQsbMIjcRuHg8HA8a4O5IhYYI
ALCHEMY_API_KEY=rzv6zozYQsbMIjcRuHg8HA8a4O5IhYYI
ETHERSCAN_API_KEY=GZDQAWE7C9MKSWQ3ANT2BFPUW8SXXZJ9MF
NEXT_PUBLIC_WALLET_CONNECT_PROJECT_ID=1dd2a69d54ac94fdefad918243183710
UPSTASH_REDIS_REST_URL=https://splendid-sunbird-26504.upstash.io
UPSTASH_REDIS_REST_TOKEN=AWeIAAIjcDE5ODI2M2QyMGMzNWU0MmE1YWZmYjRhNTljZmQwMzU0YXAxMA
KV_REST_API_URL=https://splendid-sunbird-26504.upstash.io
KV_REST_API_TOKEN=AWeIAAIjcDE5ODI2M2QyMGMzNWU0MmE1YWZmYjRhNTljZmQwMzU0YXAxMA
KV_REST_API_READ_ONLY_TOKEN=AmeIAAIgcDFuapUIQ7Gfl8xCFpd9nryMqcpkq_DbU-d9DkuesRnhQg
KV_URL=rediss://default:AWeIAAIjcDE5ODI2M2QyMGMzNWU0MmE1YWZmYjRhNTljZmQwMzU0YXAxMA@splendid-sunbird-26504.upstash.io:6379
PERSIST_CACHE=true
DEBUG=true
LOG_LEVEL=info
USE_FALLBACK_DATA=false
ESLINT_NO_DEV_ERRORS=true
USE_ALCHEMY_FOR_OWNERS=true
NEXT_NO_WORKER_THREADS=true
NEXT_PUBLIC_API_BASE_URL=http://localhost:3000
DISABLE_ELEMENT369_REDIS=true
DISABLE_STAX_REDIS=true
DISABLE_ASCENDANT_REDIS=true
DISABLE_E280_REDIS=true
DISABLE_ELEMENT280_REDIS=false# File: .env.local
NEXT_PUBLIC_ALCHEMY_API_KEY=rzv6zozYQsbMIjcRuHg8HA8a4O5IhYYI
ALCHEMY_API_KEY=rzv6zozYQsbMIjcRuHg8HA8a4O5IhYYI
ETHERSCAN_API_KEY=GZDQAWE7C9MKSWQ3ANT2BFPUW8SXXZJ9MF
NEXT_PUBLIC_WALLET_CONNECT_PROJECT_ID=1dd2a69d54ac94fdefad918243183710
UPSTASH_REDIS_REST_URL=https://splendid-sunbird-26504.upstash.io
UPSTASH_REDIS_REST_TOKEN=AWeIAAIjcDE5ODI2M2QyMGMzNWU0MmE1YWZmYjRhNTljZmQwMzU0YXAxMA
KV_REST_API_URL=https://splendid-sunbird-26504.upstash.io
KV_REST_API_TOKEN=AWeIAAIjcDE5ODI2M2QyMGMzNWU0MmE1YWZmYjRhNTljZmQwMzU0YXAxMA
KV_REST_API_READ_ONLY_TOKEN=AmeIAAIgcDFuapUIQ7Gfl8xCFpd9nryMqcpkq_DbU-d9DkuesRnhQg
KV_URL=rediss://default:AWeIAAIjcDE5ODI2M2QyMGMzNWU0MmE1YWZmYjRhNTljZmQwMzU0YXAxMA@splendid-sunbird-26504.upstash.io:6379
PERSIST_CACHE=true
DEBUG=true
LOG_LEVEL=info
USE_FALLBACK_DATA=false
ESLINT_NO_DEV_ERRORS=true
USE_ALCHEMY_FOR_OWNERS=true
NEXT_NO_WORKER_THREADS=true
NEXT_PUBLIC_API_BASE_URL=http://localhost:3000
DISABLE_ELEMENT280_REDIS=false
DISABLE_ELEMENT369_REDIS=true
DISABLE_STAX_REDIS=true
DISABLE_ASCENDANT_REDIS=true
DISABLE_E280_REDIS=trueexport const barChartOptions = {
    responsive: true,
    plugins: {
      legend: { position: 'top', labels: { color: '#e5e7eb' } }, // Gray-200
      title: {
        display: true,
        text: 'NFT Tier Distribution',
        color: '#e5e7eb',
        font: { size: 16, weight: 'bold' },
      },
    },
    scales: {
      y: {
        beginAtZero: true,
        title: { display: true, text: 'Number of NFTs', color: '#e5e7eb' },
        ticks: { color: '#d1d5db' }, // Gray-300
      },
      x: {
        title: { display: true, text: 'Tiers', color: '#e5e7eb' },
        ticks: { color: '#d1d5db' },
      },
    },
  };// lib/clientLogger.js
export const clientLogger = {
    info: (scope, message, chain = 'eth', collection = 'general') => {
      console.log(`[${new Date().toISOString()}] [${scope}] [INFO] ${message}`);
    },
    warn: (scope, message, chain = 'eth', collection = 'general') => {
      console.warn(`[${new Date().toISOString()}] [${scope}] [WARN] ${message}`);
    },
    error: (scope, message, details = {}, chain = 'eth', collection = 'general') => {
      console.error(`[${new Date().toISOString()}] [${scope}] [ERROR] ${message}`, details);
    },
    debug: (scope, message, chain = 'eth', collection = 'general') => {
      if (process.env.DEBUG === 'true') {
        console.log(`[${new Date().toISOString()}] [${scope}] [DEBUG] ${message}`);
      }
    },
  };// lib/fetchCollectionData.js
import config from '@/config';
import { logger } from '@/lib/logger';

export async function fetchCollectionData(apiKey, apiEndpoint, pageSize) {
  logger.info(`[FetchCollectionData] [INFO] Fetching ${apiKey} from ${apiEndpoint}`);
  try {
    if (apiKey === 'e280' || config.contractDetails[apiKey]?.disabled) {
      return { holders: [], totalTokens: 0, totalBurned: 0, error: 'Contract not deployed' };
    }

    let endpoint = apiEndpoint.startsWith('http') ? apiEndpoint : `${process.env.NEXT_PUBLIC_API_BASE_URL || 'http://localhost:3000'}${apiEndpoint}`;
    
    // Poll /progress until cache is complete
    let progress;
    let attempts = 0;
    const maxAttempts = config.alchemy.maxRetries;
    while (attempts < maxAttempts) {
      try {
        const res = await fetch(`${endpoint}/progress`, { cache: 'force-cache', signal: AbortSignal.timeout(config.alchemy.timeoutMs) });
        if (!res.ok) {
          const errorText = await res.text();
          throw new Error(`Progress fetch failed: ${res.status} ${errorText}`);
        }
        progress = await res.json();
        if (progress.phase === 'Idle' || progress.totalOwners === 0) {
          logger.info(`[FetchCollectionData] [INFO] Triggering POST for ${apiKey}`);
          const postRes = await fetch(endpoint, { method: 'POST', cache: 'force-cache' });
          if (!postRes.ok) {
            const errorText = await postRes.text();
            console.error(`[FetchCollectionData] [ERROR] Cache population trigger failed: ${postRes.status} ${errorText}`);
            throw new Error(`Cache population trigger failed: ${postRes.status}`);
          }
        }
        break;
      } catch (error) {
        attempts++;
        console.error(`[FetchCollectionData] [ERROR] Progress fetch attempt ${attempts}/${maxAttempts} failed: ${error.message}`);
        if (attempts >= maxAttempts) {
          return { holders: [], totalTokens: 0, totalBurned: 0, error: 'Failed to fetch cache progress' };
        }
        await new Promise(resolve => setTimeout(resolve, config.alchemy.batchDelayMs * attempts));
      }
    }

    const maxPollTime = 60000; // 60 seconds max polling
    const startTime = Date.now();
    while (progress.phase !== 'Completed' && progress.phase !== 'Error') {
      if (Date.now() - startTime > maxPollTime) {
        console.error(`[FetchCollectionData] [ERROR] Cache population timeout for ${apiKey}`);
        return { holders: [], totalTokens: 0, totalBurned: 0, error: 'Cache population timed out' };
      }
      logger.info(`[FetchCollectionData] [INFO] Waiting for ${apiKey} cache: ${progress.phase} (${progress.progressPercentage}%)`);
      await new Promise(resolve => setTimeout(resolve, config.alchemy.batchDelayMs));
      const res = await fetch(`${endpoint}/progress`, { cache: 'force-cache', signal: AbortSignal.timeout(config.alchemy.timeoutMs) });
      if (!res.ok) {
        const errorText = await res.text();
        console.error(`[FetchCollectionData] [ERROR] Progress fetch failed: ${res.status} ${errorText}`);
        return { holders: [], totalTokens: 0, totalBurned: 0, error: 'Failed to fetch cache progress' };
      }
      progress = await res.json();
    }

    if (progress.phase === 'Error') {
      console.error(`[FetchCollectionData] [ERROR] Cache population failed for ${apiKey}: ${progress.error || 'Unknown error'}`);
      return { holders: [], totalTokens: 0, totalBurned: 0, error: `Cache population failed: ${progress.error || 'Unknown error'}` };
    }

    let allHolders = [];
    let totalTokens = 0;
    let totalShares = 0;
    let totalBurned = 0;
    let summary = {};
    let page = 0;
    let totalPages = Infinity;

    while (page < totalPages) {
      const url = `${endpoint}?page=${page}&pageSize=${pageSize}`;
      logger.info(`[FetchCollectionData] [DEBUG] Fetching ${url}`);
      const res = await fetch(url, { cache: 'force-cache' });
      logger.info(`[FetchCollectionData] [DEBUG] Status: ${res.status}, headers: ${JSON.stringify([...res.headers])}`);

      if (res.status === 202) {
        logger.info(`[FetchCollectionData] [INFO] Cache still populating for ${apiKey}, retrying...`);
        await new Promise(resolve => setTimeout(resolve, config.alchemy.batchDelayMs));
        continue;
      }

      if (!res.ok) {
        const errorText = await res.text();
        console.error(`[FetchCollectionData] [ERROR] Failed: ${res.status} ${errorText}`);
        throw new Error(`API request failed: ${res.status}`);
      }

      const json = await res.json();
      logger.info(`[FetchCollectionData] [DEBUG] Response: ${JSON.stringify(json, (k, v) => typeof v === 'bigint' ? v.toString() : v)}`);

      if (json.error) {
        console.error(`[FetchCollectionData] [ERROR] API error: ${json.error}`);
        throw new Error(json.error);
      }
      if (!json.holders || !Array.isArray(json.holders)) {
        console.error(`[FetchCollectionData] [ERROR] Invalid holders: ${JSON.stringify(json)}`);
        if (apiKey === 'ascendant') {
          logger.info(`[FetchCollectionData] [INFO] Triggering POST for ${apiKey}`);
          await fetch(endpoint, { method: 'POST', cache: 'force-cache' });
          const retryRes = await fetch(url, { cache: 'no-store' });
          if (!retryRes.ok) {
            const retryError = await retryRes.text();
            console.error(`[FetchCollectionData] [ERROR] Retry failed: ${retryRes.status} ${retryError}`);
            throw new Error(`Retry failed: ${retryRes.status}`);
          }
          const retryJson = await retryRes.json();
          logger.info(`[FetchCollectionData] [DEBUG] Retry response: ${JSON.stringify(retryJson, (k, v) => typeof v === 'bigint' ? v.toString() : v)}`);
          if (!retryJson.holders || !Array.isArray(retryJson.holders)) {
            console.error(`[FetchCollectionData] [ERROR] Retry invalid holders: ${JSON.stringify(retryJson)}`);
            throw new Error('Invalid holders data after retry');
          }
          json.holders = retryJson.holders;
          json.totalTokens = retryJson.totalTokens;
          json.totalShares = retryJson.totalShares;
          json.totalBurned = retryJson.totalBurned;
          json.summary = retryJson.summary;
          json.totalPages = retryJson.totalPages;
        } else {
          throw new Error('Invalid holders data');
        }
      }

      allHolders = allHolders.concat(json.holders);
      totalTokens = json.totalTokens || json.summary?.totalLive || totalTokens;
      totalShares = json.totalShares || json.summary?.multiplierPool || totalTokens;
      totalBurned = json.totalBurned || totalBurned;
      summary = json.summary || summary;
      totalPages = json.totalPages || 1;
      page++;
      logger.info(`[FetchCollectionData] [INFO] Fetched page ${page}: ${json.holders.length} holders`);
    }

    return { holders: allHolders, totalTokens, totalShares, totalBurned, summary };
  } catch (error) {
    console.error(`[FetchCollectionData] [ERROR] ${apiKey}: ${error.message}, stack: ${error.stack}`);
    return { holders: [], totalTokens: 0, totalBurned: 0, error: error.message };
  }
}// lib/logger.js
import fs from 'fs/promises';
import path from 'path';
import chalk from 'chalk';

// Use process.cwd() to reference the project root
const logDir = path.join(process.cwd(), 'logs');

console.log(chalk.cyan('[Logger] Initializing logger...'));
console.log(chalk.cyan('[Logger] process.env.DEBUG:'), process.env.DEBUG);
console.log(chalk.cyan('[Logger] process.env.NODE_ENV:'), process.env.NODE_ENV);
console.log(chalk.cyan('[Logger] Log directory:'), logDir);

const isDebug = process.env.DEBUG === 'true';
console.log(chalk.cyan('[Logger] isDebug:'), isDebug);

async function ensureLogDir() {
  try {
    await fs.mkdir(logDir, { recursive: true });
    await fs.chmod(logDir, 0o755);
    console.log(chalk.cyan('[Logger] Created or verified log directory:'), logDir);
  } catch (error) {
    console.error(chalk.red('[Logger] Failed to create log directory:'), error.message);
  }
}

ensureLogDir().catch(error => {
  console.error(chalk.red('[Logger] ensureLogDir error:'), error.message);
});

export const logger = {
  info: async (scope, message, chain = 'eth', collection = 'general') => {
    const timestamp = new Date().toISOString();
    const log = `[${timestamp}] [${scope}] [INFO] ${message}`;
    console.log(chalk.green(log));
    if (isDebug) {
      try {
        const logFile = path.join(logDir, `cache-${chain}-${collection.toLowerCase()}-${timestamp.split('T')[0]}.log`);
        await fs.appendFile(logFile, `${log}\n`);
        console.log(chalk.cyan('[Logger] Wrote INFO log to:'), logFile);
      } catch (error) {
        console.error(chalk.red('[Logger] Failed to write INFO log:'), error.message);
      }
    }
  },
  warn: async (scope, message, chain = 'eth', collection = 'general') => {
    const timestamp = new Date().toISOString();
    const log = `[${timestamp}] [${scope}] [WARN] ${message}`;
    console.log(chalk.yellow(log));
    if (isDebug) {
      try {
        const logFile = path.join(logDir, `cache-${chain}-${collection.toLowerCase()}-${timestamp.split('T')[0]}.log`);
        await fs.appendFile(logFile, `${log}\n`);
        console.log(chalk.cyan('[Logger] Wrote WARN log to:'), logFile);
      } catch (error) {
        console.error(chalk.red('[Logger] Failed to write WARN log:'), error.message);
      }
    }
  },
  error: async (scope, message, details = {}, chain = 'eth', collection = 'general') => {
    const timestamp = new Date().toISOString();
    const log = `[${timestamp}] [${scope}] [ERROR] ${message} ${JSON.stringify(details)}`;
    console.error(chalk.red(log));
    if (isDebug) {
      try {
        const logFile = path.join(logDir, `cache-${chain}-${collection.toLowerCase()}-${timestamp.split('T')[0]}.log`);
        await fs.appendFile(logFile, `${log}\n`);
        console.log(chalk.cyan('[Logger] Wrote ERROR log to:'), logFile);
      } catch (error) {
        console.error(chalk.red('[Logger] Failed to write ERROR log:'), error.message);
      }
    }
  },
  debug: async (scope, message, chain = 'eth', collection = 'general') => {
    if (!isDebug) return;
    const timestamp = new Date().toISOString();
    const log = `[${timestamp}] [${scope}] [DEBUG] ${message}`;
    console.log(chalk.blue(log));
    try {
      const logFile = path.join(logDir, `cache-${chain}-${collection.toLowerCase()}-${timestamp.split('T')[0]}.log`);
      await fs.appendFile(logFile, `${log}\n`);
      console.log(chalk.cyan('[Logger] Wrote DEBUG log to:'), logFile);
    } catch (error) {
      console.error(chalk.red('[Logger] Failed to write DEBUG log:'), error.message);
    }
  },
};

try {
  logger.info('startup', 'Logger module loaded').catch(error => {
    console.error(chalk.red('[Logger] Startup log error:'), error.message);
  });
} catch (error) {
  console.error(chalk.red('[Logger] Immediate log error:'), error.message);
}// File: lib/schemas.js
import { z } from 'zod';

export const HoldersResponseSchema = z.object({
  holders: z.array(z.object({
    wallet: z.string(),
    total: z.number().optional(),
    tiers: z.array(z.number()).optional(),
    shares: z.number().optional(),
  })),
  totalTokens: z.number().optional(),
  totalShares: z.number().optional(),
  totalBurned: z.number().optional(),
  summary: z.object({}).optional(),
  totalPages: z.number().optional(),
});// lib/useNFTData.js
'use client';
import { useQuery } from '@tanstack/react-query';
import { useNFTStore } from '@/app/store';
import config from '@/config';
import { HoldersResponseSchema } from '@/lib/schemas';

async function fetchNFTData(apiKey, apiEndpoint, pageSize, page = 0) {
  if (apiKey === 'e280' || config.contractDetails[apiKey]?.disabled) {
    return { holders: [], totalTokens: 0, totalBurned: 0, error: 'Contract not deployed' };
  }

  const endpoint = apiEndpoint.startsWith('http') ? apiEndpoint : `${process.env.NEXT_PUBLIC_API_BASE_URL || 'http://localhost:3000'}${apiEndpoint}`;
  const progressUrl = `${endpoint}/progress`;

  const progressRes = await fetch(progressUrl, { cache: 'no-store' });
  if (!progressRes.ok) throw new Error(`Progress fetch failed: ${progressRes.status}`);
  const progress = await progressRes.json();

  if (progress.isPopulating || progress.phase !== 'Completed') {
    throw new Error('Cache is populating');
  }

  let allHolders = [];
  let totalTokens = 0;
  let totalShares = 0;
  let totalBurned = 0;
  let summary = {};
  let totalPages = Infinity;

  while (page < totalPages) {
    const url = `${endpoint}?page=${page}&pageSize=${pageSize}`;
    const res = await fetch(url, { cache: 'force-cache' });
    if (!res.ok) throw new Error(`API request failed: ${res.status}`);
    const json = await res.json();

    if (json.message === 'Cache is populating' || json.isCachePopulating) {
      throw new Error('Cache is populating');
    }

    const validation = HoldersResponseSchema.safeParse(json);
    if (!validation.success) {
      throw new Error(`Invalid holders schema: ${JSON.stringify(validation.error.errors)}`);
    }

    allHolders = allHolders.concat(json.holders);
    totalTokens = json.totalTokens || json.summary?.totalLive || totalTokens;
    totalShares = json.totalShares || json.summary?.multiplierPool || totalShares;
    totalBurned = json.totalBurned || totalBurned;
    summary = json.summary || summary;
    totalPages = json.totalPages || 1;
    page++;
  }

  return { holders: allHolders, totalTokens, totalShares, totalBurned, summary };
}

export function useNFTData(apiKey, pageSize) {
  const { getCache, setCache } = useNFTStore();

  return useQuery({
    queryKey: ['nft', apiKey],
    queryFn: async () => {
      const cachedData = getCache(apiKey);
      if (cachedData) return cachedData;

      const data = await fetchNFTData(apiKey, config.contractDetails[apiKey].apiEndpoint, pageSize);
      setCache(apiKey, data);
      return data;
    },
    retry: config.alchemy.maxRetries,
    retryDelay: attempt => config.alchemy.batchDelayMs * (attempt + 1),
    staleTime: 30 * 60 * 1000, // 30 minutes
    refetchInterval: progress => (progress?.isPopulating ? 2000 : false),
    onError: error => console.error(`[useNFTData] [ERROR] ${apiKey}: ${error.message}`),
  });
}import { getCache, setCache } from '@/app/api/utils/index.js';

export const useNFTStore = create((set, get) => ({
  cache: {},
  getCache: async (key) => {
    const cacheData = await getCache(key, 'global');
    return cacheData;
  },
  setCache: async (key, value) => {
    await setCache(key, value, 0, 'global');
    set(state => ({
      cache: { ...state.cache, [key]: value },
    }));
  },
}));// config.js
import element280MainAbi from './abi/element280.json' with { type: 'json' };
import element280VaultAbi from './abi/element280Vault.json' with { type: 'json' };
import element369MainAbi from './abi/element369.json' with { type: 'json' };
import element369VaultAbi from './abi/element369Vault.json' with { type: 'json' };
import staxMainAbi from './abi/staxNFT.json' with { type: 'json' };
import staxVaultAbi from './abi/staxVault.json' with { type: 'json' };
import ascendantMainAbi from './abi/ascendantNFT.json' with { type: 'json' };
// E280 ABI placeholder (not deployed)
const e280MainAbi = [];

const config = {
  // Supported blockchain networks
  supportedChains: ['ETH', 'BASE'],

  // ABIs for all collections
  abis: {
    element280: {
      main: element280MainAbi,
      vault: element280VaultAbi,
    },
    element369: {
      main: element369MainAbi,
      vault: element369VaultAbi,
    },
    stax: {
      main: staxMainAbi,
      vault: staxVaultAbi,
    },
    ascendant: {
      main: ascendantMainAbi,
      vault: [], // No vault ABI provided for Ascendant
    },
    e280: {
      main: e280MainAbi,
      vault: [],
    },
  },

  // NFT contract configurations
  nftContracts: {
    element280: {
      name: 'Element 280',
      symbol: 'ELMNT',
      chain: 'ETH',
      address: '0x7F090d101936008a26Bf1F0a22a5f92fC0Cf46c9',
      vaultAddress: '0x44c4ADAc7d88f85d3D33A7f856Ebc54E60C31E97',
      deploymentBlock: '20945304',
      tiers: {
        1: { name: 'Common', multiplier: 10, allocation: '100000000000000000000000000' },
        2: { name: 'Common Amped', multiplier: 12, allocation: '100000000000000000000000000' },
        3: { name: 'Rare', multiplier: 100, allocation: '1000000000000000000000000000' },
        4: { name: 'Rare Amped', multiplier: 120, allocation: '1000000000000000000000000000' },
        5: { name: 'Legendary', multiplier: 1000, allocation: '10000000000000000000000000000' },
        6: { name: 'Legendary Amped', multiplier: 1200, allocation: '10000000000000000000000000000' },
      },
      description:
        'Element 280 NFTs can be minted with TitanX or ETH during a presale and redeemed for Element 280 tokens after a cooldown period. Multipliers contribute to a pool used for reward calculations.',
      expectedTotalSupply: 8107,
      expectedBurned: 8776,
      maxTokensPerOwnerQuery: 100,
    },
    element369: {
      name: 'Element 369',
      symbol: 'E369',
      chain: 'ETH',
      address: '0x024D64E2F65747d8bB02dFb852702D588A062575',
      vaultAddress: '0x4e3DBD6333e649AF13C823DAAcDd14f8507ECBc5',
      deploymentBlock: '21224418',
      tiers: {
        1: { name: 'Common', multiplier: 1, price: '100000000000000000000000000' },
        2: { name: 'Rare', multiplier: 10, price: '1000000000000000000000000000' },
        3: { name: 'Legendary', multiplier: 100, price: '10000000000000000000000000000' },
      },
      description:
        'Element 369 NFTs are minted with TitanX or ETH during specific sale cycles. Burning NFTs updates a multiplier pool and tracks burn cycles for reward distribution in the Holder Vault.',
    },
    stax: {
      name: 'Stax',
      symbol: 'STAX',
      chain: 'ETH',
      address: '0x74270Ca3a274B4dbf26be319A55188690CACE6E1',
      vaultAddress: '0x5D27813C32dD705404d1A78c9444dAb523331717',
      deploymentBlock: '21452667',
      tiers: {
        1: { name: 'Common', multiplier: 1, price: '100000000000000000000000000' },
        2: { name: 'Common Amped', multiplier: 1.2, price: '100000000000000000000000000', amplifier: '10000000000000000000000000' },
        3: { name: 'Common Super', multiplier: 1.4, price: '100000000000000000000000000', amplifier: '20000000000000000000000000' },
        4: { name: 'Common LFG', multiplier: 2, price: '100000000000000000000000000', amplifier: '50000000000000000000000000' },
        5: { name: 'Rare', multiplier: 10, price: '1000000000000000000000000000' },
        6: { name: 'Rare Amped', multiplier: 12, price: '1000000000000000000000000000', amplifier: '100000000000000000000000000' },
        7: { name: 'Rare Super', multiplier: 14, price: '1000000000000000000000000000', amplifier: '200000000000000000000000000' },
        8: { name: 'Rare LFG', multiplier: 20, price: '1000000000000000000000000000', amplifier: '500000000000000000000000000' },
        9: { name: 'Legendary', multiplier: 100, price: '10000000000000000000000000000' },
        10: { name: 'Legendary Amped', multiplier: 120, price: '10000000000000000000000000000', amplifier: '1000000000000000000000000000' },
        11: { name: 'Legendary Super', multiplier: 140, price: '10000000000000000000000000000', amplifier: '2000000000000000000000000000' },
        12: { name: 'Legendary LFG', multiplier: 200, price: '10000000000000000000000000000', amplifier: '5000000000000000000000000000' },
      },
      description:
        'Stax NFTs are minted with TitanX or ETH during a presale. Burning NFTs after a cooldown period claims backing rewards, with multipliers contributing to a pool for cycle-based reward calculations.',
    },
    ascendant: {
      name: 'Ascendant',
      symbol: 'ASCNFT',
      chain: 'ETH',
      address: '0x9da95c32c5869c84ba2c020b5e87329ec0adc97f',
      deploymentBlock: '21112535',
      tiers: {
        1: { name: 'Tier 1', price: '7812500000000000000000', multiplier: 1.01 },
        2: { name: 'Tier 2', price: '15625000000000000000000', multiplier: 1.02 },
        3: { name: 'Tier 3', price: '31250000000000000000000', multiplier: 1.03 },
        4: { name: 'Tier 4', price: '62500000000000000000000', multiplier: 1.04 },
        5: { name: 'Tier 5', price: '125000000000000000000000', multiplier: 1.05 },
        6: { name: 'Tier 6', price: '250000000000000000000000', multiplier: 1.06 },
        7: { name: 'Tier 7', price: '500000000000000000000000', multiplier: 1.07 },
        8: { name: 'Tier 8', price: '1000000000000000000000000', multiplier: 1.08 },
      },
      description:
        'Ascendant NFTs are minted with ASCENDANT tokens and offer staking rewards from DragonX pools over 8, 28, and 90-day periods. Features fusion mechanics to combine same-tier NFTs into higher tiers.',
    },
    e280: {
      name: 'E280',
      symbol: 'E280',
      chain: 'BASE',
      address: null,
      deploymentBlock: null,
      tiers: {},
      description: 'E280 NFTs on BASE chain. Contract not yet deployed.',
      disabled: true,
    },
  },

  // Contract addresses
  contractAddresses: {
    element280: { chain: 'ETH', address: '0x7F090d101936008a26Bf1F0a22a5f92fC0Cf46c9' },
    element369: { chain: 'ETH', address: '0x024D64E2F65747d8bB02dFb852702D588A062575' },
    stax: { chain: 'ETH', address: '0x74270Ca3a274B4dbf26be319A55188690CACE6E1' },
    ascendant: { chain: 'ETH', address: '0x9da95c32c5869c84ba2c020b5e87329ec0adc97f' },
    e280: { chain: 'BASE', address: null },
  },

  // Vault addresses
  vaultAddresses: {
    element280: { chain: 'ETH', address: '0x44c4ADAc7d88f85d3D33A7f856Ebc54E60C31E97' },
    element369: { chain: 'ETH', address: '0x4e3DBD6333e649AF13C823DAAcDd14f8507ECBc5' },
    stax: { chain: 'ETH', address: '0x5D27813C32dD705404d1A78c9444dAb523331717' },
    e280: { chain: 'BASE', address: null },
  },

  // Deployment blocks
  deploymentBlocks: {
    element280: { chain: 'ETH', block: '20945304' },
    element369: { chain: 'ETH', block: '21224418' },
    stax: { chain: 'ETH', block: '21452667' },
    ascendant: { chain: 'ETH', block: '21112535' },
    e280: { chain: 'BASE', block: null },
  },

  // Contract tiers
  contractTiers: {
    element280: {
      1: { name: 'Common', multiplier: 10 },
      2: { name: 'Common Amped', multiplier: 12 },
      3: { name: 'Rare', multiplier: 100 },
      4: { name: 'Rare Amped', multiplier: 120 },
      5: { name: 'Legendary', multiplier: 1000 },
      6: { name: 'Legendary Amped', multiplier: 1200 },
    },
    element369: {
      1: { name: 'Common', multiplier: 1 },
      2: { name: 'Rare', multiplier: 10 },
      3: { name: 'Legendary', multiplier: 100 },
      tierOrder: [
        { tierId: '3', name: 'Legendary' },
        { tierId: '2', name: 'Rare' },
        { tierId: '1', name: 'Common' },
      ],
    },
    stax: {
      1: { name: 'Common', multiplier: 1 },
      2: { name: 'Common Amped', multiplier: 1.2 },
      3: { name: 'Common Super', multiplier: 1.4 },
      4: { name: 'Common LFG', multiplier: 2 },
      5: { name: 'Rare', multiplier: 10 },
      6: { name: 'Rare Amped', multiplier: 12 },
      7: { name: 'Rare Super', multiplier: 14 },
      8: { name: 'Rare LFG', multiplier: 20 },
      9: { name: 'Legendary', multiplier: 100 },
      10: { name: 'Legendary Amped', multiplier: 120 },
      11: { name: 'Legendary Super', multiplier: 140 },
      12: { name: 'Legendary LFG', multiplier: 200 },
    },
    ascendant: {
      1: { name: 'Tier 1', multiplier: 1.01 },
      2: { name: 'Tier 2', multiplier: 1.02 },
      3: { name: 'Tier 3', multiplier: 1.03 },
      4: { name: 'Tier 4', multiplier: 1.04 },
      5: { name: 'Tier 5', multiplier: 1.05 },
      6: { name: 'Tier 6', multiplier: 1.06 },
      7: { name: 'Tier 7', multiplier: 1.07 },
      8: { name: 'Tier 8', multiplier: 1.08 },
    },
    e280: {},
  },

  // Contract details
  contractDetails: {
    element280: {
      name: 'Element 280',
      chain: 'ETH',
      pageSize: 100,
      apiEndpoint: '/api/holders/Element280',
      rewardToken: 'ELMNT',
    },
    element369: {
      name: 'Element 369',
      chain: 'ETH',
      pageSize: 1000,
      apiEndpoint: '/api/holders/Element369',
      rewardToken: 'INFERNO/FLUX/E280',
    },
    stax: {
      name: 'Stax',
      chain: 'ETH',
      pageSize: 1000,
      apiEndpoint: '/api/holders/Stax',
      rewardToken: 'X28',
    },
    ascendant: {
      name: 'Ascendant',
      chain: 'ETH',
      pageSize: 1000,
      apiEndpoint: '/api/holders/Ascendant',
      rewardToken: 'DRAGONX',
    },
    e280: {
      name: 'E280',
      chain: 'BASE',
      pageSize: 1000,
      apiEndpoint: '/api/holders/E280',
      rewardToken: 'E280',
      disabled: true,
    },
  },

  // Utility function to get contract details by name
  getContractDetails: (contractName) => {
    return config.nftContracts[contractName] || null;
  },

  alchemy: {
    apiKey: process.env.ALCHEMY_API_KEY || process.env.NEXT_PUBLIC_ALCHEMY_API_KEY,
    network: 'eth-mainnet',
    batchSize: 50, // Increased for larger batches
    batchDelayMs: 500, // Reduced for speed
    retryMaxDelayMs: 10000, // Reduced to fail faster
    maxRetries: 2, // Reduced to minimize retry overhead
    timeoutMs: 30000,
  },
  
  // Cache settings
  cache: {
    redis: {
      disableElement280: process.env.DISABLE_ELEMENT280_REDIS === 'true',
      disableElement369: process.env.DISABLE_ELEMENT369_REDIS === 'true',
      disableStax: process.env.DISABLE_STAX_REDIS === 'true',
      disableAscendant: process.env.DISABLE_ASCENDANT_REDIS === 'true',
      disableE280: process.env.DISABLE_E280_REDIS === 'true' || true,
    },
    nodeCache: {
      stdTTL: 3600,
      checkperiod: 120,
    },
  },

  // Debug settings
  debug: {
    enabled: process.env.DEBUG === 'true',
    logLevel: 'debug',
  },

  // Fallback data (optional, for testing)
  fallbackData: {
    element280: process.env.USE_FALLBACK_DATA === 'true' ? element280NftStatus : null,
  },
};

export default config;// app/api/utils.js
import { createPublicClient, http, parseAbi } from 'viem';
import { mainnet } from 'viem/chains';
import { Alchemy, Network } from 'alchemy-sdk';

// Shared cache for routes that import it
export const cache = {};

// Import all ABI JSON files using @ notation
import staxNFTAbi from '@/abi/staxNFT.json';
import element369Abi from '@/abi/element369.json';
import element369VaultAbi from '@/abi/element369Vault.json';
import staxVaultAbi from '@/abi/staxVault.json';
import ascendantNFTAbi from '@/abi/ascendantNFT.json';
import element280Abi from '@/abi/element280.json';
import element280VaultAbi from '@/abi/element280Vault.json';

export const alchemy = new Alchemy({
  apiKey: process.env.NEXT_PUBLIC_ALCHEMY_API_KEY || (() => { throw new Error('Alchemy API key missing'); })(),
  network: Network.ETH_MAINNET,
});

export const client = createPublicClient({
  chain: mainnet,
  transport: http(
    process.env.ETH_RPC_URL ||
    `https://eth-mainnet.g.alchemy.com/v2/${process.env.NEXT_PUBLIC_ALCHEMY_API_KEY}`
  ),
});

// Generic NFT ABI for common functions
export const nftAbi = parseAbi([
  'function ownerOf(uint256 tokenId) view returns (address)',
  'function getNftTier(uint256 tokenId) view returns (uint8)',
]);

// Ascendant NFT ABI with specific functions
export const ascendantAbi = parseAbi([
  'function ownerOf(uint256 tokenId) view returns (address)',
  'function getNFTAttribute(uint256 tokenId) view returns (uint256 rarityNumber, uint8 tier, uint8 rarity)',
  'function userRecords(uint256 tokenId) view returns (uint256 shares, uint256 lockedAscendant, uint256 rewardDebt, uint32 startTime, uint32 endTime)',
  'function totalShares() view returns (uint256)',
  'function toDistribute(uint8 pool) view returns (uint256)',
  'function rewardPerShare() view returns (uint256)',
  'error NonExistentToken(uint256 tokenId)',
]);

// Export all ABIs
export {
  staxNFTAbi,
  element369Abi,
  element369VaultAbi,
  staxVaultAbi,
  ascendantNFTAbi,
  element280Abi,
  element280VaultAbi,
};

export const CACHE_TTL = 5 * 60 * 1000; // 5 minutes

export function log(message) {
  console.log(`[PROD_DEBUG] ${message}`);
}

export async function batchMulticall(calls, batchSize = 50) {
  log(`batchMulticall: Processing ${calls.length} calls in batches of ${batchSize}`);
  const results = [];
  for (let i = 0; i < calls.length; i += batchSize) {
    const batch = calls.slice(i, i + batchSize);
    try {
      const batchResults = await client.multicall({ contracts: batch });
      results.push(...batchResults);
      log(`batchMulticall: Batch ${i}-${i + batchSize - 1} completed with ${batchResults.length} results`);
    } catch (error) {
      console.error(`[PROD_ERROR] batchMulticall failed for batch ${i}-${i + batchSize - 1}: ${error.message}`);
      results.push(...batch.map(() => ({ status: 'failure', result: null })));
    }
  }
  log(`batchMulticall: Completed with ${results.length} results`);
  return results;
}// .babelrc
{
  "presets": [
    [
      "@babel/preset-env",
      {
        "targets": {
          "node": "current"
        }
      }
    ]
  ],
  "plugins": [
    "@babel/plugin-transform-modules-commonjs",
    "@babel/plugin-syntax-dynamic-import"
  ]
}// jest.config.js
export default {
  testEnvironment: 'node',
  moduleNameMapper: { '^@/(.*)$': '<rootDir>/$1' },
  transform: { '^.+\\.(js|mjs)$': 'babel-jest' },
  transformIgnorePatterns: [
    '/node_modules/(?!(@upstash/redis|viem|alchemy-sdk|node-cache|p-limit|chalk|node-fetch|ansi-styles|supports-color|strip-ansi|has-flag)/)',
  ],
  testMatch: ['**/tests/**/*.test.js'],
  verbose: true,
  setupFilesAfterEnv: ['<rootDir>/tests/setup.js'],
  testPathIgnorePatterns: ['/tests/e2e/frontend.test.js'],
};