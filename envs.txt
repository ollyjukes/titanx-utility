{
    "name": "titanx-utility",
    "version": "0.1.0",
    "type": "module",
    "private": true,
    "scripts": {
        "dev": "next dev",
        "build": "NODE_ENV=production next build --debug",
        "start": "NODE_ENV=production next start",
        "lint": "next lint",
        "tailwind": "tailwindcss build -i ./app/global.css -o ./app/output.css --watch"
    },
    "dependencies": {
        "@heroicons/react": "^2.2.0",
        "@tanstack/react-query": "^5.59.13",
        "@upstash/redis": "^1.34.4",
        "alchemy-sdk": "^3.4.2",
        "async-mutex": "^0.5.0",
        "chalk": "^5.4.1",
        "chart.js": "^4.4.3",
        "dotenv": "^16.4.5",
        "framer-motion": "^12.6.3",
        "next": "^15.3.1",
        "node-cache": "^5.1.2",
        "node-fetch": "^3.3.2",
        "p-limit": "^6.2.0",
        "react": "^18.3.0",
        "react-chartjs-2": "^5.3.0",
        "react-dom": "^18.3.1",
        "react-virtualized": "^9.22.6",
        "uuid": "^11.1.0",
        "viem": "^2.28.0",
        "zod": "^3.24.3",
        "zustand": "^5.0.3"
    },
    "devDependencies": {
        "@eslint/js": "^9.13.0",
        "@typescript-eslint/eslint-plugin": "^8.31.0",
        "@typescript-eslint/parser": "^8.31.0",
        "autoprefixer": "^10.4.20",
        "eslint": "^8.57.0",
        "eslint-config-next": "^15.3.1",
        "postcss": "^8.4.47",
        "tailwindcss": "^3.4.13"
    },
    "resolutions": {}
}
// File: next.config.mjs
/** @type {import('next').NextConfig} */
const nextConfig = {
  reactStrictMode: true,
  serverExternalPackages: ['viem', 'alchemy-sdk'],
  webpack: (config) => {
    config.resolve.fallback = { fs: false, path: false };
    return config;
  },
};

export default nextConfig;{
    "compilerOptions": {
      "baseUrl": ".",
      "paths": {
        "@/*": ["./*"]
      }
    }
  }

/** @type {import('tailwindcss').Config} */
module.exports = {
  content: [
    './app/**/*.{js,ts,jsx,tsx}',
    './components/**/*.{js,ts,jsx,tsx}',
  ],
  theme: {
    extend: {},
  },
  plugins: [],
};

# File: .env.local
NEXT_PUBLIC_ALCHEMY_API_KEY=rzv6zozYQsbMIjcRuHg8HA8a4O5IhYYI
ALCHEMY_API_KEY=rzv6zozYQsbMIjcRuHg8HA8a4O5IhYYI
ETHERSCAN_API_KEY=GZDQAWE7C9MKSWQ3ANT2BFPUW8SXXZJ9MF
NEXT_PUBLIC_WALLET_CONNECT_PROJECT_ID=1dd2a69d54ac94fdefad918243183710
UPSTASH_REDIS_REST_URL=https://splendid-sunbird-26504.upstash.io
UPSTASH_REDIS_REST_TOKEN=AWeIAAIjcDE5ODI2M2QyMGMzNWU0MmE1YWZmYjRhNTljZmQwMzU0YXAxMA
KV_REST_API_URL=https://splendid-sunbird-26504.upstash.io
KV_REST_API_TOKEN=AWeIAAIjcDE5ODI2M2QyMGMzNWU0MmE1YWZmYjRhNTljZmQwMzU0YXAxMA
KV_REST_API_READ_ONLY_TOKEN=AmeIAAIgcDFuapUIQ7Gfl8xCFpd9nryMqcpkq_DbU-d9DkuesRnhQg
KV_URL=rediss://default:AWeIAAIjcDE5ODI2M2QyMGMzNWU0MmE1YWZmYjRhNTljZmQwMzU0YXAxMA@splendid-sunbird-26504.upstash.io:6379
PERSIST_CACHE=true
DEBUG=true
LOG_LEVEL=info
USE_FALLBACK_DATA=false
ESLINT_NO_DEV_ERRORS=true
USE_ALCHEMY_FOR_OWNERS=true
NEXT_NO_WORKER_THREADS=true
NEXT_PUBLIC_API_BASE_URL=http://localhost:3000
DISABLE_ELEMENT280_REDIS=true
DISABLE_ELEMENT369_REDIS=true
DISABLE_STAX_REDIS=true
DISABLE_ASCENDANT_REDIS=true
DISABLE_E280_REDIS=true# File: .env.local
NEXT_PUBLIC_ALCHEMY_API_KEY=rzv6zozYQsbMIjcRuHg8HA8a4O5IhYYI
ALCHEMY_API_KEY=rzv6zozYQsbMIjcRuHg8HA8a4O5IhYYI
ETHERSCAN_API_KEY=GZDQAWE7C9MKSWQ3ANT2BFPUW8SXXZJ9MF
NEXT_PUBLIC_WALLET_CONNECT_PROJECT_ID=1dd2a69d54ac94fdefad918243183710
UPSTASH_REDIS_REST_URL=https://splendid-sunbird-26504.upstash.io
UPSTASH_REDIS_REST_TOKEN=AWeIAAIjcDE5ODI2M2QyMGMzNWU0MmE1YWZmYjRhNTljZmQwMzU0YXAxMA
KV_REST_API_URL=https://splendid-sunbird-26504.upstash.io
KV_REST_API_TOKEN=AWeIAAIjcDE5ODI2M2QyMGMzNWU0MmE1YWZmYjRhNTljZmQwMzU0YXAxMA
KV_REST_API_READ_ONLY_TOKEN=AmeIAAIgcDFuapUIQ7Gfl8xCFpd9nryMqcpkq_DbU-d9DkuesRnhQg
KV_URL=rediss://default:AWeIAAIjcDE5ODI2M2QyMGMzNWU0MmE1YWZmYjRhNTljZmQwMzU0YXAxMA@splendid-sunbird-26504.upstash.io:6379
PERSIST_CACHE=true
DEBUG=true
LOG_LEVEL=info
USE_FALLBACK_DATA=false
ESLINT_NO_DEV_ERRORS=true
USE_ALCHEMY_FOR_OWNERS=true
NEXT_NO_WORKER_THREADS=true
NEXT_PUBLIC_API_BASE_URL=http://localhost:3000
DISABLE_ELEMENT280_REDIS=true
DISABLE_ELEMENT369_REDIS=true
DISABLE_STAX_REDIS=true
DISABLE_ASCENDANT_REDIS=true
DISABLE_E280_REDIS=trueexport const barChartOptions = {
    responsive: true,
    plugins: {
      legend: { position: 'top', labels: { color: '#e5e7eb' } }, // Gray-200
      title: {
        display: true,
        text: 'NFT Tier Distribution',
        color: '#e5e7eb',
        font: { size: 16, weight: 'bold' },
      },
    },
    scales: {
      y: {
        beginAtZero: true,
        title: { display: true, text: 'Number of NFTs', color: '#e5e7eb' },
        ticks: { color: '#d1d5db' }, // Gray-300
      },
      x: {
        title: { display: true, text: 'Tiers', color: '#e5e7eb' },
        ticks: { color: '#d1d5db' },
      },
    },
  };// lib/fetchCollectionData.js
import config from '@/config';

export async function fetchCollectionData(apiKey, apiEndpoint, pageSize) {
  console.log(`[FetchCollectionData] [INFO] Fetching ${apiKey} from ${apiEndpoint}`);
  try {
    if (apiKey === 'e280' || config.contractDetails[apiKey]?.disabled) {
      return { holders: [], totalTokens: 0, totalBurned: 0, error: 'Contract not deployed' };
    }

    let endpoint = apiEndpoint.startsWith('http') ? apiEndpoint : `${process.env.NEXT_PUBLIC_API_BASE_URL || 'http://localhost:3000'}${apiEndpoint}`;
    
    // Poll /progress until cache is complete
    let progress;
    let attempts = 0;
    const maxAttempts = config.alchemy.maxRetries;
    while (attempts < maxAttempts) {
      try {
        const res = await fetch(`${endpoint}/progress`, { cache: 'force-cache', signal: AbortSignal.timeout(config.alchemy.timeoutMs) });
        if (!res.ok) {
          const errorText = await res.text();
          throw new Error(`Progress fetch failed: ${res.status} ${errorText}`);
        }
        progress = await res.json();
        if (progress.phase === 'Idle' || progress.totalOwners === 0) {
          console.log(`[FetchCollectionData] [INFO] Triggering POST for ${apiKey}`);
          const postRes = await fetch(endpoint, { method: 'POST', cache: 'force-cache' });
          if (!postRes.ok) {
            const errorText = await postRes.text();
            console.error(`[FetchCollectionData] [ERROR] Cache population trigger failed: ${postRes.status} ${errorText}`);
            throw new Error(`Cache population trigger failed: ${postRes.status}`);
          }
        }
        break;
      } catch (error) {
        attempts++;
        console.error(`[FetchCollectionData] [ERROR] Progress fetch attempt ${attempts}/${maxAttempts} failed: ${error.message}`);
        if (attempts >= maxAttempts) {
          return { holders: [], totalTokens: 0, totalBurned: 0, error: 'Failed to fetch cache progress' };
        }
        await new Promise(resolve => setTimeout(resolve, config.alchemy.batchDelayMs * attempts));
      }
    }

    const maxPollTime = 60000; // 60 seconds max polling
    const startTime = Date.now();
    while (progress.phase !== 'Completed' && progress.phase !== 'Error') {
      if (Date.now() - startTime > maxPollTime) {
        console.error(`[FetchCollectionData] [ERROR] Cache population timeout for ${apiKey}`);
        return { holders: [], totalTokens: 0, totalBurned: 0, error: 'Cache population timed out' };
      }
      console.log(`[FetchCollectionData] [INFO] Waiting for ${apiKey} cache: ${progress.phase} (${progress.progressPercentage}%)`);
      await new Promise(resolve => setTimeout(resolve, config.alchemy.batchDelayMs));
      const res = await fetch(`${endpoint}/progress`, { cache: 'force-cache', signal: AbortSignal.timeout(config.alchemy.timeoutMs) });
      if (!res.ok) {
        const errorText = await res.text();
        console.error(`[FetchCollectionData] [ERROR] Progress fetch failed: ${res.status} ${errorText}`);
        return { holders: [], totalTokens: 0, totalBurned: 0, error: 'Failed to fetch cache progress' };
      }
      progress = await res.json();
    }

    if (progress.phase === 'Error') {
      console.error(`[FetchCollectionData] [ERROR] Cache population failed for ${apiKey}: ${progress.error || 'Unknown error'}`);
      return { holders: [], totalTokens: 0, totalBurned: 0, error: `Cache population failed: ${progress.error || 'Unknown error'}` };
    }

    let allHolders = [];
    let totalTokens = 0;
    let totalShares = 0;
    let totalBurned = 0;
    let summary = {};
    let page = 0;
    let totalPages = Infinity;

    while (page < totalPages) {
      const url = `${endpoint}?page=${page}&pageSize=${pageSize}`;
      console.log(`[FetchCollectionData] [DEBUG] Fetching ${url}`);
      const res = await fetch(url, { cache: 'force-cache' });
      console.log(`[FetchCollectionData] [DEBUG] Status: ${res.status}, headers: ${JSON.stringify([...res.headers])}`);

      if (res.status === 202) {
        console.log(`[FetchCollectionData] [INFO] Cache still populating for ${apiKey}, retrying...`);
        await new Promise(resolve => setTimeout(resolve, config.alchemy.batchDelayMs));
        continue;
      }

      if (!res.ok) {
        const errorText = await res.text();
        console.error(`[FetchCollectionData] [ERROR] Failed: ${res.status} ${errorText}`);
        throw new Error(`API request failed: ${res.status}`);
      }

      const json = await res.json();
      console.log(`[FetchCollectionData] [DEBUG] Response: ${JSON.stringify(json, (k, v) => typeof v === 'bigint' ? v.toString() : v)}`);

      if (json.error) {
        console.error(`[FetchCollectionData] [ERROR] API error: ${json.error}`);
        throw new Error(json.error);
      }
      if (!json.holders || !Array.isArray(json.holders)) {
        console.error(`[FetchCollectionData] [ERROR] Invalid holders: ${JSON.stringify(json)}`);
        if (apiKey === 'ascendant') {
          console.log(`[FetchCollectionData] [INFO] Triggering POST for ${apiKey}`);
          await fetch(endpoint, { method: 'POST', cache: 'force-cache' });
          const retryRes = await fetch(url, { cache: 'no-store' });
          if (!retryRes.ok) {
            const retryError = await retryRes.text();
            console.error(`[FetchCollectionData] [ERROR] Retry failed: ${retryRes.status} ${retryError}`);
            throw new Error(`Retry failed: ${retryRes.status}`);
          }
          const retryJson = await retryRes.json();
          console.log(`[FetchCollectionData] [DEBUG] Retry response: ${JSON.stringify(retryJson, (k, v) => typeof v === 'bigint' ? v.toString() : v)}`);
          if (!retryJson.holders || !Array.isArray(retryJson.holders)) {
            console.error(`[FetchCollectionData] [ERROR] Retry invalid holders: ${JSON.stringify(retryJson)}`);
            throw new Error('Invalid holders data after retry');
          }
          json.holders = retryJson.holders;
          json.totalTokens = retryJson.totalTokens;
          json.totalShares = retryJson.totalShares;
          json.totalBurned = retryJson.totalBurned;
          json.summary = retryJson.summary;
          json.totalPages = retryJson.totalPages;
        } else {
          throw new Error('Invalid holders data');
        }
      }

      allHolders = allHolders.concat(json.holders);
      totalTokens = json.totalTokens || json.summary?.totalLive || totalTokens;
      totalShares = json.totalShares || json.summary?.multiplierPool || totalTokens;
      totalBurned = json.totalBurned || totalBurned;
      summary = json.summary || summary;
      totalPages = json.totalPages || 1;
      page++;
      console.log(`[FetchCollectionData] [INFO] Fetched page ${page}: ${json.holders.length} holders`);
    }

    return { holders: allHolders, totalTokens, totalShares, totalBurned, summary };
  } catch (error) {
    console.error(`[FetchCollectionData] [ERROR] ${apiKey}: ${error.message}, stack: ${error.stack}`);
    return { holders: [], totalTokens: 0, totalBurned: 0, error: error.message };
  }
}import fs from 'fs/promises';
import path from 'path';
import chalk from 'chalk';

// Use process.cwd() to reference the project root
const logDir = path.join(process.cwd(), 'logs');

console.log(chalk.cyan('[Logger] Initializing logger...'));
console.log(chalk.cyan('[Logger] process.env.DEBUG:'), process.env.DEBUG);
console.log(chalk.cyan('[Logger] process.env.NODE_ENV:'), process.env.NODE_ENV);
console.log(chalk.cyan('[Logger] Log directory:'), logDir);

const isDebug = process.env.DEBUG === 'true';
console.log(chalk.cyan('[Logger] isDebug:'), isDebug);

async function ensureLogDir() {
  try {
    await fs.mkdir(logDir, { recursive: true });
    await fs.chmod(logDir, 0o755);
    console.log(chalk.cyan('[Logger] Created or verified log directory:'), logDir);
  } catch (error) {
    console.error(chalk.red('[Logger] Failed to create log directory:'), error.message);
  }
}

ensureLogDir().catch(error => {
  console.error(chalk.red('[Logger] ensureLogDir error:'), error.message);
});

export const logger = {
  info: async (scope, message, chain = 'eth', collection = 'general') => {
    const timestamp = new Date().toISOString();
    const log = `[${timestamp}] [${scope}] [INFO] ${message}`;
    console.log(chalk.green(log));
    if (isDebug) {
      try {
        const logFile = path.join(logDir, `cache-${chain}-${collection.toLowerCase()}-${timestamp.split('T')[0]}.log`);
        await fs.appendFile(logFile, `${log}\n`);
        console.log(chalk.cyan('[Logger] Wrote INFO log to:'), logFile);
      } catch (error) {
        console.error(chalk.red('[Logger] Failed to write INFO log:'), error.message);
      }
    }
  },
  error: async (scope, message, details = {}, chain = 'eth', collection = 'general') => {
    const timestamp = new Date().toISOString();
    const log = `[${timestamp}] [${scope}] [ERROR] ${message} ${JSON.stringify(details)}`;
    console.error(chalk.red(log));
    if (isDebug) {
      try {
        const logFile = path.join(logDir, `cache-${chain}-${collection.toLowerCase()}-${timestamp.split('T')[0]}.log`);
        await fs.appendFile(logFile, `${log}\n`);
        console.log(chalk.cyan('[Logger] Wrote ERROR log to:'), logFile);
      } catch (error) {
        console.error(chalk.red('[Logger] Failed to write ERROR log:'), error.message);
      }
    }
  },
  debug: async (scope, message, chain = 'eth', collection = 'general') => {
    if (!isDebug) return;
    const timestamp = new Date().toISOString();
    const log = `[${timestamp}] [${scope}] [DEBUG] ${message}`;
    console.log(chalk.blue(log));
    try {
      const logFile = path.join(logDir, `cache-${chain}-${collection.toLowerCase()}-${timestamp.split('T')[0]}.log`);
      await fs.appendFile(logFile, `${log}\n`);
      console.log(chalk.cyan('[Logger] Wrote DEBUG log to:'), logFile);
    } catch (error) {
      console.error(chalk.red('[Logger] Failed to write DEBUG log:'), error.message);
    }
  },
};

try {
  logger.info('startup', 'Logger module loaded').catch(error => {
    console.error(chalk.red('[Logger] Startup log error:'), error.message);
  });
} catch (error) {
  console.error(chalk.red('[Logger] Immediate log error:'), error.message);
}// File: lib/schemas.js
import { z } from 'zod';

export const HoldersResponseSchema = z.object({
  holders: z.array(z.object({
    wallet: z.string(),
    total: z.number().optional(),
    tiers: z.array(z.number()).optional(),
    shares: z.number().optional(),
  })),
  totalTokens: z.number().optional(),
  totalShares: z.number().optional(),
  totalBurned: z.number().optional(),
  summary: z.object({}).optional(),
  totalPages: z.number().optional(),
});// File: lib/serverInit.js
import { logger } from '@/lib/logger';
import { initializeCache } from '@/app/api/utils';
import chalk from 'chalk';

console.log(chalk.cyan('[ServerInit] Initializing server...'));

try {
  logger.info('serverInit', 'Server initialization started');
  await initializeCache();
} catch (error) {
  logger.error('serverInit', `Initialize cache error: ${error.message}`, { stack: error.stack });
  console.error(chalk.red('[ServerInit] Initialization error:'), error.message);
}

export const serverInit = true;// lib/useNFTData.js
'use client';
import { useQuery } from '@tanstack/react-query';
import { useNFTStore } from '@/app/store';
import config from '@/config';
import { HoldersResponseSchema } from '@/lib/schemas';

async function fetchNFTData(apiKey, apiEndpoint, pageSize, page = 0) {
  if (apiKey === 'e280' || config.contractDetails[apiKey]?.disabled) {
    return { holders: [], totalTokens: 0, totalBurned: 0, error: 'Contract not deployed' };
  }

  const endpoint = apiEndpoint.startsWith('http') ? apiEndpoint : `${process.env.NEXT_PUBLIC_API_BASE_URL || 'http://localhost:3000'}${apiEndpoint}`;
  const progressUrl = `${endpoint}/progress`;

  const progressRes = await fetch(progressUrl, { cache: 'no-store' });
  if (!progressRes.ok) throw new Error(`Progress fetch failed: ${progressRes.status}`);
  const progress = await progressRes.json();

  if (progress.isPopulating || progress.phase !== 'Completed') {
    throw new Error('Cache is populating');
  }

  let allHolders = [];
  let totalTokens = 0;
  let totalShares = 0;
  let totalBurned = 0;
  let summary = {};
  let totalPages = Infinity;

  while (page < totalPages) {
    const url = `${endpoint}?page=${page}&pageSize=${pageSize}`;
    const res = await fetch(url, { cache: 'force-cache' });
    if (!res.ok) throw new Error(`API request failed: ${res.status}`);
    const json = await res.json();

    if (json.message === 'Cache is populating' || json.isCachePopulating) {
      throw new Error('Cache is populating');
    }

    const validation = HoldersResponseSchema.safeParse(json);
    if (!validation.success) {
      throw new Error(`Invalid holders schema: ${JSON.stringify(validation.error.errors)}`);
    }

    allHolders = allHolders.concat(json.holders);
    totalTokens = json.totalTokens || json.summary?.totalLive || totalTokens;
    totalShares = json.totalShares || json.summary?.multiplierPool || totalShares;
    totalBurned = json.totalBurned || totalBurned;
    summary = json.summary || summary;
    totalPages = json.totalPages || 1;
    page++;
  }

  return { holders: allHolders, totalTokens, totalShares, totalBurned, summary };
}

export function useNFTData(apiKey, pageSize) {
  const { getCache, setCache } = useNFTStore();

  return useQuery({
    queryKey: ['nft', apiKey],
    queryFn: async () => {
      const cachedData = getCache(apiKey);
      if (cachedData) return cachedData;

      const data = await fetchNFTData(apiKey, config.contractDetails[apiKey].apiEndpoint, pageSize);
      setCache(apiKey, data);
      return data;
    },
    retry: config.alchemy.maxRetries,
    retryDelay: attempt => config.alchemy.batchDelayMs * (attempt + 1),
    staleTime: 30 * 60 * 1000, // 30 minutes
    refetchInterval: progress => (progress?.isPopulating ? 2000 : false),
    onError: error => console.error(`[useNFTData] [ERROR] ${apiKey}: ${error.message}`),
  });
}// File: app/store.js

'use client';
import { create } from 'zustand';

const CACHE_TTL = 30 * 60 * 1000; // 30 minutes

export const useNFTStore = create((set, get) => ({
  cache: {},
  setCache: (contractKey, data) => {
    const key = `nft:${contractKey}`;
    console.log(`[NFTStore] Setting cache for ${key}: ${data.holders?.length || 0} holders`);
    set((state) => ({
      cache: {
        ...state.cache,
        [key]: { data, timestamp: Date.now() },
      },
    }));
  },
  getCache: (contractKey) => {
    const key = `nft:${contractKey}`;
    console.log(`[NFTStore] Getting cache for ${key}`);
    const cachedEntry = get().cache[key];
    if (!cachedEntry) {
      console.log(`[NFTStore] Cache miss for ${key}`);
      return null;
    }
    const now = Date.now();
    if (now - cachedEntry.timestamp > CACHE_TTL) {
      console.log(`[NFTStore] Cache expired for ${key}`);
      set((state) => {
        const newCache = { ...state.cache };
        delete newCache[key];
        return { cache: newCache };
      });
      return null;
    }
    console.log(`[NFTStore] Cache hit for ${key}: ${cachedEntry.data.holders?.length || 0} holders`);
    return cachedEntry.data;
  },
}));// config.js
import element280MainAbi from './abi/element280.json' assert { type: 'json' };
import element280VaultAbi from './abi/element280Vault.json' assert { type: 'json' };
import element369MainAbi from './abi/element369.json' assert { type: 'json' };
import element369VaultAbi from './abi/element369Vault.json' assert { type: 'json' };
import staxMainAbi from './abi/staxNFT.json' assert { type: 'json' };
import staxVaultAbi from './abi/staxVault.json' assert { type: 'json' };
import ascendantMainAbi from './abi/ascendantNFT.json' assert { type: 'json' };
// E280 ABI placeholder (not deployed)
const e280MainAbi = [];

const config = {
  // Supported blockchain networks
  supportedChains: ['ETH', 'BASE'],

  // ABIs for all collections
  abis: {
    element280: {
      main: element280MainAbi,
      vault: element280VaultAbi,
    },
    element369: {
      main: element369MainAbi,
      vault: element369VaultAbi,
    },
    stax: {
      main: staxMainAbi,
      vault: staxVaultAbi,
    },
    ascendant: {
      main: ascendantMainAbi,
      vault: [], // No vault ABI provided for Ascendant
    },
    e280: {
      main: e280MainAbi,
      vault: [],
    },
  },

  // NFT contract configurations
  nftContracts: {
    element280: {
      name: 'Element 280',
      symbol: 'ELMNT',
      chain: 'ETH',
      address: '0x7F090d101936008a26Bf1F0a22a5f92fC0Cf46c9',
      vaultAddress: '0x44c4ADAc7d88f85d3D33A7f856Ebc54E60C31E97',
      deploymentBlock: '20945304',
      tiers: {
        1: { name: 'Common', multiplier: 10, allocation: '100000000000000000000000000' },
        2: { name: 'Common Amped', multiplier: 12, allocation: '100000000000000000000000000' },
        3: { name: 'Rare', multiplier: 100, allocation: '1000000000000000000000000000' },
        4: { name: 'Rare Amped', multiplier: 120, allocation: '1000000000000000000000000000' },
        5: { name: 'Legendary', multiplier: 1000, allocation: '10000000000000000000000000000' },
        6: { name: 'Legendary Amped', multiplier: 1200, allocation: '10000000000000000000000000000' },
      },
      description:
        'Element 280 NFTs can be minted with TitanX or ETH during a presale and redeemed for Element 280 tokens after a cooldown period. Multipliers contribute to a pool used for reward calculations.',
      expectedTotalSupply: 8107,
      expectedBurned: 8776,
      maxTokensPerOwnerQuery: 100,
    },
    element369: {
      name: 'Element 369',
      symbol: 'E369',
      chain: 'ETH',
      address: '0x024D64E2F65747d8bB02dFb852702D588A062575',
      vaultAddress: '0x4e3DBD6333e649AF13C823DAAcDd14f8507ECBc5',
      deploymentBlock: '21224418',
      tiers: {
        1: { name: 'Common', multiplier: 1, price: '100000000000000000000000000' },
        2: { name: 'Rare', multiplier: 10, price: '1000000000000000000000000000' },
        3: { name: 'Legendary', multiplier: 100, price: '10000000000000000000000000000' },
      },
      description:
        'Element 369 NFTs are minted with TitanX or ETH during specific sale cycles. Burning NFTs updates a multiplier pool and tracks burn cycles for reward distribution in the Holder Vault.',
    },
    stax: {
      name: 'Stax',
      symbol: 'STAX',
      chain: 'ETH',
      address: '0x74270Ca3a274B4dbf26be319A55188690CACE6E1',
      vaultAddress: '0x5D27813C32dD705404d1A78c9444dAb523331717',
      deploymentBlock: '21452667',
      tiers: {
        1: { name: 'Common', multiplier: 1, price: '100000000000000000000000000' },
        2: { name: 'Common Amped', multiplier: 1.2, price: '100000000000000000000000000', amplifier: '10000000000000000000000000' },
        3: { name: 'Common Super', multiplier: 1.4, price: '100000000000000000000000000', amplifier: '20000000000000000000000000' },
        4: { name: 'Common LFG', multiplier: 2, price: '100000000000000000000000000', amplifier: '50000000000000000000000000' },
        5: { name: 'Rare', multiplier: 10, price: '1000000000000000000000000000' },
        6: { name: 'Rare Amped', multiplier: 12, price: '1000000000000000000000000000', amplifier: '100000000000000000000000000' },
        7: { name: 'Rare Super', multiplier: 14, price: '1000000000000000000000000000', amplifier: '200000000000000000000000000' },
        8: { name: 'Rare LFG', multiplier: 20, price: '1000000000000000000000000000', amplifier: '500000000000000000000000000' },
        9: { name: 'Legendary', multiplier: 100, price: '10000000000000000000000000000' },
        10: { name: 'Legendary Amped', multiplier: 120, price: '10000000000000000000000000000', amplifier: '1000000000000000000000000000' },
        11: { name: 'Legendary Super', multiplier: 140, price: '10000000000000000000000000000', amplifier: '2000000000000000000000000000' },
        12: { name: 'Legendary LFG', multiplier: 200, price: '10000000000000000000000000000', amplifier: '5000000000000000000000000000' },
      },
      description:
        'Stax NFTs are minted with TitanX or ETH during a presale. Burning NFTs after a cooldown period claims backing rewards, with multipliers contributing to a pool for cycle-based reward calculations.',
    },
    ascendant: {
      name: 'Ascendant',
      symbol: 'ASCNFT',
      chain: 'ETH',
      address: '0x9da95c32c5869c84ba2c020b5e87329ec0adc97f',
      deploymentBlock: '21112535',
      tiers: {
        1: { name: 'Tier 1', price: '7812500000000000000000', multiplier: 1.01 },
        2: { name: 'Tier 2', price: '15625000000000000000000', multiplier: 1.02 },
        3: { name: 'Tier 3', price: '31250000000000000000000', multiplier: 1.03 },
        4: { name: 'Tier 4', price: '62500000000000000000000', multiplier: 1.04 },
        5: { name: 'Tier 5', price: '125000000000000000000000', multiplier: 1.05 },
        6: { name: 'Tier 6', price: '250000000000000000000000', multiplier: 1.06 },
        7: { name: 'Tier 7', price: '500000000000000000000000', multiplier: 1.07 },
        8: { name: 'Tier 8', price: '1000000000000000000000000', multiplier: 1.08 },
      },
      description:
        'Ascendant NFTs are minted with ASCENDANT tokens and offer staking rewards from DragonX pools over 8, 28, and 90-day periods. Features fusion mechanics to combine same-tier NFTs into higher tiers.',
    },
    e280: {
      name: 'E280',
      symbol: 'E280',
      chain: 'BASE',
      address: null,
      deploymentBlock: null,
      tiers: {},
      description: 'E280 NFTs on BASE chain. Contract not yet deployed.',
      disabled: true,
    },
  },

  // Contract addresses
  contractAddresses: {
    element280: { chain: 'ETH', address: '0x7F090d101936008a26Bf1F0a22a5f92fC0Cf46c9' },
    element369: { chain: 'ETH', address: '0x024D64E2F65747d8bB02dFb852702D588A062575' },
    stax: { chain: 'ETH', address: '0x74270Ca3a274B4dbf26be319A55188690CACE6E1' },
    ascendant: { chain: 'ETH', address: '0x9da95c32c5869c84ba2c020b5e87329ec0adc97f' },
    e280: { chain: 'BASE', address: null },
  },

  // Vault addresses
  vaultAddresses: {
    element280: { chain: 'ETH', address: '0x44c4ADAc7d88f85d3D33A7f856Ebc54E60C31E97' },
    element369: { chain: 'ETH', address: '0x4e3DBD6333e649AF13C823DAAcDd14f8507ECBc5' },
    stax: { chain: 'ETH', address: '0x5D27813C32dD705404d1A78c9444dAb523331717' },
    e280: { chain: 'BASE', address: null },
  },

  // Deployment blocks
  deploymentBlocks: {
    element280: { chain: 'ETH', block: '20945304' },
    element369: { chain: 'ETH', block: '21224418' },
    stax: { chain: 'ETH', block: '21452667' },
    ascendant: { chain: 'ETH', block: '21112535' },
    e280: { chain: 'BASE', block: null },
  },

  // Contract tiers
  contractTiers: {
    element280: {
      1: { name: 'Common', multiplier: 10 },
      2: { name: 'Common Amped', multiplier: 12 },
      3: { name: 'Rare', multiplier: 100 },
      4: { name: 'Rare Amped', multiplier: 120 },
      5: { name: 'Legendary', multiplier: 1000 },
      6: { name: 'Legendary Amped', multiplier: 1200 },
    },
    element369: {
      1: { name: 'Common', multiplier: 1 },
      2: { name: 'Rare', multiplier: 10 },
      3: { name: 'Legendary', multiplier: 100 },
      tierOrder: [
        { tierId: '3', name: 'Legendary' },
        { tierId: '2', name: 'Rare' },
        { tierId: '1', name: 'Common' },
      ],
    },
    stax: {
      1: { name: 'Common', multiplier: 1 },
      2: { name: 'Common Amped', multiplier: 1.2 },
      3: { name: 'Common Super', multiplier: 1.4 },
      4: { name: 'Common LFG', multiplier: 2 },
      5: { name: 'Rare', multiplier: 10 },
      6: { name: 'Rare Amped', multiplier: 12 },
      7: { name: 'Rare Super', multiplier: 14 },
      8: { name: 'Rare LFG', multiplier: 20 },
      9: { name: 'Legendary', multiplier: 100 },
      10: { name: 'Legendary Amped', multiplier: 120 },
      11: { name: 'Legendary Super', multiplier: 140 },
      12: { name: 'Legendary LFG', multiplier: 200 },
    },
    ascendant: {
      1: { name: 'Tier 1', multiplier: 1.01 },
      2: { name: 'Tier 2', multiplier: 1.02 },
      3: { name: 'Tier 3', multiplier: 1.03 },
      4: { name: 'Tier 4', multiplier: 1.04 },
      5: { name: 'Tier 5', multiplier: 1.05 },
      6: { name: 'Tier 6', multiplier: 1.06 },
      7: { name: 'Tier 7', multiplier: 1.07 },
      8: { name: 'Tier 8', multiplier: 1.08 },
    },
    e280: {},
  },

  // Contract details
  contractDetails: {
    element280: {
      name: 'Element 280',
      chain: 'ETH',
      pageSize: 100,
      apiEndpoint: '/api/holders/Element280',
      rewardToken: 'ELMNT',
    },
    element369: {
      name: 'Element 369',
      chain: 'ETH',
      pageSize: 1000,
      apiEndpoint: '/api/holders/Element369',
      rewardToken: 'INFERNO/FLUX/E280',
    },
    stax: {
      name: 'Stax',
      chain: 'ETH',
      pageSize: 1000,
      apiEndpoint: '/api/holders/Stax',
      rewardToken: 'X28',
    },
    ascendant: {
      name: 'Ascendant',
      chain: 'ETH',
      pageSize: 1000,
      apiEndpoint: '/api/holders/Ascendant',
      rewardToken: 'DRAGONX',
    },
    e280: {
      name: 'E280',
      chain: 'BASE',
      pageSize: 1000,
      apiEndpoint: '/api/holders/E280',
      rewardToken: 'E280',
      disabled: true,
    },
  },

  // Utility function to get contract details by name
  getContractDetails: (contractName) => {
    return config.nftContracts[contractName] || null;
  },

  alchemy: {
    apiKey: process.env.ALCHEMY_API_KEY || process.env.NEXT_PUBLIC_ALCHEMY_API_KEY,
    network: 'eth-mainnet',
    batchSize: 50, // Increased for larger batches
    batchDelayMs: 500, // Reduced for speed
    retryMaxDelayMs: 10000, // Reduced to fail faster
    maxRetries: 2, // Reduced to minimize retry overhead
    timeoutMs: 30000,
  },
  
  // Cache settings
  cache: {
    redis: {
      disableElement280: process.env.DISABLE_ELEMENT280_REDIS === 'true',
      disableElement369: process.env.DISABLE_ELEMENT369_REDIS === 'true',
      disableStax: process.env.DISABLE_STAX_REDIS === 'true',
      disableAscendant: process.env.DISABLE_ASCENDANT_REDIS === 'true',
      disableE280: process.env.DISABLE_E280_REDIS === 'true' || true,
    },
    nodeCache: {
      stdTTL: 3600,
      checkperiod: 120,
    },
  },

  // Debug settings
  debug: {
    enabled: process.env.DEBUG === 'true',
    logLevel: 'debug',
  },

  // Fallback data (optional, for testing)
  fallbackData: {
    element280: process.env.USE_FALLBACK_DATA === 'true' ? element280NftStatus : null,
  },
};

export default config;// File: app/api/utils.js
import NodeCache from 'node-cache';
import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';
import { Redis } from '@upstash/redis';
import { createPublicClient, http } from 'viem';
import { mainnet } from 'viem/chains';
import { Alchemy } from 'alchemy-sdk';
import config from '@/config';
import pLimit from 'p-limit';
import { logger } from '@/lib/logger';
import chalk from 'chalk';

console.log(chalk.cyan('[Utils] Initializing utils...'));
logger.info('utils', 'Utils module loaded', 'eth', 'general').catch(error => {
  console.error(chalk.red('[Utils] Logger error:'), error.message);
});

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const isDebug = process.env.DEBUG === 'true';
const isProduction = process.env.NODE_ENV === 'production';

const cache = new NodeCache({
  stdTTL: 0,
  checkperiod: 120,
});

const cacheDir = path.join(process.cwd(), 'cache');

const redisEnabled = Object.keys(config.nftContracts).some(
  contract => process.env[`DISABLE_${contract.toUpperCase()}_REDIS`] !== 'true' && process.env.UPSTASH_REDIS_REST_URL && process.env.UPSTASH_REDIS_REST_TOKEN
);
let redis = null;

if (redisEnabled) {
  try {
    redis = new Redis({
      url: process.env.UPSTASH_REDIS_REST_URL,
      token: process.env.UPSTASH_REDIS_REST_TOKEN,
    });
    logger.info('utils', 'Upstash Redis initialized', 'eth', 'general');
  } catch (error) {
    logger.error('utils', `Failed to initialize Upstash Redis: ${error.message}`, { stack: error.stack }, 'eth', 'general');
    redis = null;
  }
}

const alchemyApiKey = config.alchemy.apiKey || process.env.NEXT_PUBLIC_ALCHEMY_API_KEY;
if (!alchemyApiKey) {
  logger.error('utils', 'Alchemy API key is missing', {}, 'eth', 'general');
  throw new Error('Alchemy API key is missing');
}

const client = createPublicClient({
  chain: mainnet,
  transport: http(`https://eth-mainnet.g.alchemy.com/v2/${alchemyApiKey}`),
});

const alchemy = new Alchemy({
  apiKey: config.alchemy.apiKey,
  network: 'eth-mainnet',
});

async function ensureCacheDir() {
  try {
    await fs.mkdir(cacheDir, { recursive: true });
    await fs.chmod(cacheDir, 0o755);
    logger.info('utils', `Created/chmod cache directory: ${cacheDir}`, 'eth', 'general');
  } catch (error) {
    logger.error('utils', `Failed to create/chmod cache directory ${cacheDir}: ${error.message}`, { stack: error.stack }, 'eth', 'general');
    throw error;
  }
}

async function initializeCache() {
  try {
    logger.info('utils', 'Starting cache initialization', 'eth', 'general');
    await ensureCacheDir();

    // Check node-cache
    const testKey = 'test_node_cache';
    const testValue = { ready: true };
    const nodeCacheSuccess = cache.set(testKey, testValue);
    if (nodeCacheSuccess) {
      logger.info('utils', 'Node-cache is ready', 'eth', 'general');
      cache.del(testKey);
    } else {
      logger.error('utils', 'Node-cache failed to set test key', {}, 'eth', 'general');
    }

    // Check Redis
    if (redisEnabled && redis) {
      try {
        await redis.set('test_redis', JSON.stringify(testValue));
        const redisData = await redis.get('test_redis');
        if (redisData && JSON.parse(redisData).ready) {
          logger.info('utils', 'Redis cache is ready', 'eth', 'general');
          await redis.del('test_redis');
        } else {
          logger.error('utils', 'Redis cache test failed: invalid data', {}, 'eth', 'general');
        }
      } catch (error) {
        logger.error('utils', `Redis cache test failed: ${error.message}`, { stack: error.stack }, 'eth', 'general');
      }
    }

    // Create empty cache files for each collection
    const collections = Object.keys(config.nftContracts).filter(key => !config.nftContracts[key].disabled).map(key => key.toLowerCase());
    for (const collection of collections) {
      const cacheFile = path.join(cacheDir, `${collection}_holders.json`);
      try {
        await fs.access(cacheFile);
        logger.info('utils', `Cache file exists: ${cacheFile}`, 'eth', collection);
      } catch (error) {
        if (error.code === 'ENOENT') {
          await fs.writeFile(cacheFile, JSON.stringify({ holders: [], totalBurned: 0, timestamp: Date.now() }));
          await fs.chmod(cacheFile, 0o644);
          logger.info('utils', `Created empty cache file: ${cacheFile}`, 'eth', collection);
        } else {
          logger.error('utils', `Failed to access cache file ${cacheFile}: ${error.message}`, { stack: error.stack }, 'eth', collection);
        }
      }
    }

    logger.info('utils', 'Cache initialization completed', 'eth', 'general');
    return true;
  } catch (error) {
    logger.error('utils', `Cache initialization error: ${error.message}`, { stack: error.stack }, 'eth', 'general');
    return false;
  }
}

async function retry(operation, { retries, delay = 1000 }) {
  let lastError;
  for (let attempt = 1; attempt <= retries; attempt++) {
    try {
      return await operation();
    } catch (error) {
      lastError = error;
      if (error.message.includes('429') && attempt === retries) {
        logger.error('utils', `Circuit breaker: Rate limit exceeded after ${retries} attempts`, {}, 'eth', 'general');
        throw new Error('Rate limit exceeded');
      }
      logger.warn('utils', `Retry attempt ${attempt}/${retries} failed: ${error.message}`, 'eth', 'general');
      await new Promise(resolve => setTimeout(resolve, delay * Math.min(attempt, 3)));
    }
  }
  throw lastError;
}

async function batchMulticall(calls, batchSize = config.alchemy.batchSize || 10) {
  const results = [];
  const delay = async () => new Promise(resolve => setTimeout(resolve, config.alchemy.batchDelayMs || 500));

  const concurrencyLimit = pLimit(3);
  const batchPromises = [];
  for (let i = 0; i < calls.length; i += batchSize) {
    const batch = calls.slice(i, i + batchSize);
    batchPromises.push(
      concurrencyLimit(async () => {
        try {
          await delay();
          const batchResults = await client.multicall({
            contracts: batch.map(call => ({
              address: call.address,
              abi: call.abi,
              functionName: call.functionName,
              args: call.args || [],
            })),
            allowFailure: true,
          });

          const batchResult = batchResults.map((result, index) => ({
            status: result.status === 'success' ? 'success' : 'failure',
            result: result.status === 'success' ? result.result : null,
            error: result.status === 'failure' ? result.error?.message || 'Unknown error' : null,
          }));
          return batchResult;
        } catch (error) {
          logger.error('utils', `Batch multicall failed: ${error.message}`, { stack: error.stack }, 'eth', 'general');
          return batch.map(() => ({
            status: 'failure',
            result: null,
            error: error.message,
          }));
        }
      })
    );
  }

  const batchResults = (await Promise.all(batchPromises)).flat();
  results.push(...batchResults);
  return results;
}

async function getOwnersForContract(contractAddress, abi, options = {}) {
  let owners = [];
  let pageKey = options.pageKey || null;
  const maxPages = options.maxPages || 10; // Safety limit to prevent infinite loops
  let pageCount = 0;

  logger.debug('utils', `Fetching owners for contract: ${contractAddress} with options: ${JSON.stringify(options)}`, 'eth', 'general');

  do {
    try {
      const response = await alchemy.nft.getOwnersForContract(contractAddress, {
        withTokenBalances: options.withTokenBalances || false,
        pageKey,
      });

      logger.debug('utils', `Raw Alchemy response: ownersExists=${!!response.owners}, isArray=${Array.isArray(response.owners)}, ownersLength=${response.owners?.length || 0}, pageKey=${response.pageKey || null}, responseKeys=${Object.keys(response)}, sampleOwners=${JSON.stringify(response.owners?.slice(0, 2) || [])}`, 'eth', 'general');

      if (!response.owners || !Array.isArray(response.owners)) {
        logger.error('utils', `Invalid Alchemy response for ${contractAddress}: ${JSON.stringify(response)}`, {}, 'eth', 'general');
        throw new Error('Invalid owners response from Alchemy API');
      }

      for (const owner of response.owners) {
        const tokenBalances = owner.tokenBalances || [];
        logger.debug('utils', `Processing owner: ${owner.ownerAddress}, tokenBalancesCount=${tokenBalances.length}`, 'eth', 'general');

        if (tokenBalances.length > 0) {
          const validBalances = tokenBalances.filter(
            tb => tb.tokenId && Number(tb.balance) > 0
          );
          if (validBalances.length > 0) {
            owners.push({
              ownerAddress: owner.ownerAddress.toLowerCase(),
              tokenBalances: validBalances.map(tb => ({
                tokenId: Number(tb.tokenId),
                balance: Number(tb.balance),
              })),
            });
          }
        }
      }

      pageKey = response.pageKey || null;
      pageCount++;
      logger.debug('utils', `Fetched page ${pageCount}, owners: ${owners.length}, pageKey: ${pageKey}`, 'eth', 'general');

      if (pageCount >= maxPages) {
        logger.warn('utils', `Reached max pages (${maxPages}) for owner fetching`, 'eth', 'general');
        break;
      }
    } catch (error) {
      logger.error('utils', `Failed to fetch owners for ${contractAddress}: ${error.message}`, { stack: error.stack }, 'eth', 'general');
      throw error;
    }
  } while (pageKey);

  logger.debug('utils', `Processed owners: count=${owners.length}, sample=${JSON.stringify(owners.slice(0, 2))}`, 'eth', 'general');
  logger.info('utils', `Fetched ${owners.length} owners for contract: ${contractAddress}`, 'eth', 'general');
  return owners;
}

async function setCache(key, value, ttl, prefix) {
  try {
    const cacheKey = `${prefix}_${key}`;
    const success = cache.set(cacheKey, value);
    logger.info('utils', `Set in-memory cache: ${cacheKey}, success: ${success}, holders: ${value.holders?.length || 'unknown'}`, 'eth', prefix.toLowerCase());

    if (key === `${prefix.toLowerCase()}_holders` && Object.keys(config.nftContracts).map(k => k.toLowerCase()).includes(prefix.toLowerCase())) {
      if (redisEnabled && redis && process.env[`DISABLE_${prefix.toUpperCase()}_REDIS`] !== 'true') {
        try {
          await redis.set(cacheKey, JSON.stringify(value));
          logger.info('utils', `Persisted ${cacheKey} to Redis, holders: ${value.holders.length}`, 'eth', prefix.toLowerCase());
        } catch (error) {
          logger.error('utils', `Failed to persist ${cacheKey} to Redis: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
        }
      } else {
        const cacheFile = path.join(cacheDir, `${prefix.toLowerCase()}_holders.json`);
        logger.info('utils', `Writing to cache file: ${cacheFile}`, 'eth', prefix.toLowerCase());
        await ensureCacheDir();
        try {
          await fs.writeFile(cacheFile, JSON.stringify(value));
          await fs.chmod(cacheFile, 0o644);
          logger.info('utils', `Persisted ${cacheKey} to ${cacheFile}, holders: ${value.holders.length}`, 'eth', prefix.toLowerCase());
        } catch (error) {
          logger.error('utils', `Failed to write cache file ${cacheFile}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
          throw error;
        }
      }
    }
    return success;
  } catch (error) {
    logger.error('utils', `Failed to set cache ${prefix}_${key}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
    return false;
  }
}

async function getCache(key, prefix) {
  try {
    const cacheKey = `${prefix}_${key}`;
    let data = cache.get(cacheKey);
    if (data !== undefined) {
      logger.debug('utils', `Cache hit: ${cacheKey}, holders: ${data.holders?.length || 'unknown'}`, 'eth', prefix.toLowerCase());
      return data;
    }

    if (key === `${prefix.toLowerCase()}_holders` && Object.keys(config.nftContracts).map(k => k.toLowerCase()).includes(prefix.toLowerCase())) {
      if (redisEnabled && redis && process.env[`DISABLE_${prefix.toUpperCase()}_REDIS`] !== 'true') {
        try {
          const redisData = await redis.get(cacheKey);
          if (redisData) {
            const parsed = typeof redisData === 'string' ? JSON.parse(redisData) : redisData;
            if (parsed && Array.isArray(parsed.holders) && Number.isInteger(parsed.totalBurned)) {
              const success = cache.set(cacheKey, parsed);
              logger.info('utils', `Loaded ${cacheKey} from Redis, cached: ${success}, holders: ${parsed.holders.length}`, 'eth', prefix.toLowerCase());
              return parsed;
            } else {
              logger.warn('utils', `Invalid data in Redis for ${cacheKey}`, 'eth', prefix.toLowerCase());
            }
          }
        } catch (error) {
          logger.error('utils', `Failed to load cache from Redis for ${cacheKey}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
        }
      }

      const cacheFile = path.join(cacheDir, `${prefix.toLowerCase()}_holders.json`);
      try {
        const fileData = await fs.readFile(cacheFile, 'utf8');
        const parsed = JSON.parse(fileData);
        if (parsed && Array.isArray(parsed.holders) && Number.isInteger(parsed.totalBurned)) {
          const success = cache.set(cacheKey, parsed);
          logger.info('utils', `Loaded ${cacheKey} from ${cacheFile}, cached: ${success}, holders: ${parsed.holders.length}`, 'eth', prefix.toLowerCase());
          return parsed;
        } else {
          logger.warn('utils', `Invalid data in ${cacheFile}`, 'eth', prefix.toLowerCase());
        }
      } catch (error) {
        if (error.code !== 'ENOENT') {
          logger.error('utils', `Failed to load cache from ${cacheFile}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
        } else {
          logger.debug('utils', `No cache file at ${cacheFile}`, 'eth', prefix.toLowerCase());
        }
      }
    }

    logger.info('utils', `Cache miss: ${cacheKey}`, 'eth', prefix.toLowerCase());
    return null;
  } catch (error) {
    logger.error('utils', `Failed to get cache ${prefix}_${key}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
    return null;
  }
}

async function saveCacheState(collection, state, prefix) {
  try {
    const cacheFile = path.join(cacheDir, `cache_state_${prefix.toLowerCase()}.json`);
    await ensureCacheDir();
    await fs.writeFile(cacheFile, JSON.stringify(state));
    await fs.chmod(cacheFile, 0o644);
    logger.debug('utils', `Saved cache state for ${prefix}: ${cacheFile}`, 'eth', prefix.toLowerCase());
  } catch (error) {
    logger.error('utils', `Failed to save cache state for ${prefix}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
  }
}

async function loadCacheState(collection, prefix) {
  try {
    const cacheFile = path.join(cacheDir, `cache_state_${prefix.toLowerCase()}.json`);
    const data = await fs.readFile(cacheFile, 'utf8');
    const parsed = JSON.parse(data);
    logger.debug('utils', `Loaded cache state for ${prefix}: ${cacheFile}`, 'eth', prefix.toLowerCase());
    return parsed;
  } catch (error) {
    if (error.code === 'ENOENT') {
      logger.debug('utils', `No cache state found for ${prefix}`, 'eth', prefix.toLowerCase());
      return null;
    }
    logger.error('utils', `Failed to load cache state for ${prefix}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
    return null;
  }
}

async function getTransactionReceipt(transactionHash) {
  try {
    const receipt = await client.getTransactionReceipt({ hash: transactionHash });
    logger.debug('utils', `Fetched transaction receipt for ${transactionHash}`, 'eth', 'general');
    return receipt;
  } catch (error) {
    logger.error('utils', `Failed to fetch transaction receipt for ${transactionHash}: ${error.message}`, { stack: error.stack }, 'eth', 'general');
    return null;
  }
}

async function log(scope, message, chain = 'eth', collection = 'general') {
  await logger.info(scope, message, chain, collection);
}

export { client, retry, logger, getCache, setCache, saveCacheState, loadCacheState, batchMulticall, getOwnersForContract, getTransactionReceipt, log, initializeCache };