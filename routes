import { NextResponse } from 'next/server';
import { parseAbiItem, formatUnits } from 'viem';
import pLimit from 'p-limit';
import config from '@/config.js';
import { client, retry, logger, getCache, setCache, saveCacheState, loadCacheState, batchMulticall, getOwnersForContract } from '@/app/api/utils';
import { HoldersResponseSchema } from '@/lib/schemas';

const limit = pLimit(5);

async function getCacheState(contractKey) {
  const cacheState = {
    isPopulating: false,
    totalOwners: 0,
    progressState: { step: 'idle', processedNfts: 0, totalNfts: 0, processedTiers: 0, totalTiers: 0, error: null, errorLog: [] },
    lastUpdated: null,
    lastProcessedBlock: null,
  };
  try {
    const savedState = await loadCacheState(contractKey, contractKey.toLowerCase());
    if (savedState && typeof savedState === 'object') {
      cacheState.isPopulating = savedState.isPopulating ?? false;
      cacheState.totalOwners = savedState.totalOwners ?? 0;
      cacheState.progressState = {
        step: savedState.progressState?.step ?? 'idle',
        processedNfts: savedState.progressState?.processedNfts ?? 0,
        totalNfts: savedState.progressState?.totalNfts ?? 0,
        processedTiers: savedState.progressState?.processedTiers ?? 0,
        totalTiers: savedState.progressState?.totalTiers ?? 0,
        error: savedState.progressState?.error ?? null,
        errorLog: savedState.progressState?.errorLog ?? [],
      };
      cacheState.lastUpdated = savedState.lastUpdated ?? null;
      cacheState.lastProcessedBlock = savedState.lastProcessedBlock ?? null;
      logger.debug(contractKey, `Loaded cache state: totalOwners=${cacheState.totalOwners}, step=${cacheState.progressState.step}`);
    }
  } catch (error) {
    logger.error(contractKey, `Failed to load cache state: ${error.message}`, { stack: error.stack });
  }
  return cacheState;
}

async function saveCacheStateContract(contractKey, cacheState) {
  try {
    await saveCacheState(contractKey, cacheState, contractKey.toLowerCase());
    logger.debug(contractKey, `Saved cache state: totalOwners=${cacheState.totalOwners}, step=${cacheState.progressState.step}`);
  } catch (error) {
    logger.error(contractKey, `Failed to save cache state: ${error.message}`, { stack: error.stack });
  }
}

async function getNewEvents(contractKey, contractAddress, fromBlock, errorLog) {
  const burnAddress = config.burnAddress || '0x0000000000000000000000000000000000000000';
  const cacheKey = `${contractKey.toLowerCase()}_events_${contractAddress}_${fromBlock}`;
  let cachedEvents = await getCache(cacheKey, contractKey.toLowerCase());

  if (cachedEvents) {
    logger.info(contractKey, `Events cache hit: ${cacheKey}, count: ${cachedEvents.burnedTokenIds.length + (cachedEvents.transferTokenIds?.length || 0)}`);
    return cachedEvents;
  }

  let burnedTokenIds = [];
  let transferTokenIds = [];
  let endBlock;
  try {
    endBlock = await client.getBlockNumber();
  } catch (error) {
    logger.error(contractKey, `Failed to fetch block number: ${error.message}`, { stack: error.stack });
    errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_block_number', error: error.message });
    throw error;
  }

  if (fromBlock >= endBlock) {
    logger.info(contractKey, `No new blocks: fromBlock ${fromBlock} >= endBlock ${endBlock}`);
    return { burnedTokenIds, transferTokenIds, lastBlock: Number(endBlock) };
  }

  try {
    const logs = await client.getLogs({
      address: contractAddress,
      event: parseAbiItem('event Transfer(address indexed from, address indexed to, uint256 indexed tokenId)'),
      fromBlock: BigInt(fromBlock),
      toBlock: endBlock,
    });
    burnedTokenIds = logs
      .filter(log => log.args.to.toLowerCase() === burnAddress.toLowerCase())
      .map(log => Number(log.args.tokenId));
    transferTokenIds = logs
      .filter(log => log.args.to.toLowerCase() !== burnAddress.toLowerCase())
      .map(log => ({ tokenId: Number(log.args.tokenId), from: log.args.from.toLowerCase(), to: log.args.to.toLowerCase() }));
    const cacheData = { burnedTokenIds, transferTokenIds, lastBlock: Number(endBlock), timestamp: Date.now() };
    await setCache(cacheKey, cacheData, config.cache.nodeCache.stdTTL, contractKey.toLowerCase());
    logger.info(contractKey, `Cached events: ${cacheKey}, burns: ${burnedTokenIds.length}, transfers: ${transferTokenIds.length}`);
    return cacheData;
  } catch (error) {
    logger.error(contractKey, `Failed to fetch events: ${error.message}`, { stack: error.stack });
    errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_events', error: error.message });
    throw error;
  }
}

async function getHoldersMap(contractKey, contractAddress, abi, vaultAddress, vaultAbi, cacheState) {
  if (!contractAddress) throw new Error('Contract address missing');
  if (!abi) throw new Error(`${contractKey} ABI missing`);

  const requiredFunctions = contractKey === 'ascendant' ? ['getNFTAttribute', 'userRecords', 'totalShares', 'toDistribute'] : ['totalSupply', 'totalBurned', 'ownerOf', 'getNftTier'];
  const missingFunctions = requiredFunctions.filter(fn => !abi.some(item => item.name === fn));
  if (missingFunctions.length > 0) throw new Error(`Missing ABI functions: ${missingFunctions.join(', ')}`);

  const burnAddress = config.burnAddress || '0x0000000000000000000000000000000000000000';
  const holdersMap = new Map();
  let totalBurned = 0;
  const errorLog = [];

  cacheState.progressState.step = 'fetching_supply';
  await saveCacheStateContract(contractKey, cacheState);

  let currentBlock;
  try {
    currentBlock = await client.getBlockNumber();
    logger.debug(contractKey, `Fetched current block: ${currentBlock}`);
  } catch (error) {
    errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_block_number', error: error.message });
    logger.error(contractKey, `Failed to fetch block number: ${error.message}`, { stack: error.stack });
    throw error;
  }

  if (contractKey === 'ascendant') {
    const owners = await retry(
      () => getOwnersForContract(contractAddress, abi, { withTokenBalances: true }),
      { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
    );
    logger.debug(contractKey, `Fetched owners: count=${owners.length}, sample=${JSON.stringify(owners.slice(0, 2))}`);
    cacheState.progressState.totalNfts = owners.length;
    cacheState.progressState.totalTiers = owners.length;
    cacheState.lastProcessedBlock = Number(currentBlock);
    await saveCacheStateContract(contractKey, cacheState);

    const filteredOwners = owners.filter(owner => owner.ownerAddress?.toLowerCase() !== burnAddress.toLowerCase());
    logger.debug(contractKey, `Filtered owners: count=${filteredOwners.length}, sample=${JSON.stringify(filteredOwners.slice(0, 2))}`);
    const tokenOwnerMap = new Map();
    let totalTokens = 0;
    filteredOwners.forEach(owner => {
      if (!owner.ownerAddress) {
        logger.debug(contractKey, `Skipped owner: missing ownerAddress, tokenId=${owner.tokenId}`);
        totalBurned++;
        return;
      }
      const wallet = owner.ownerAddress.toLowerCase();
      const tokenId = Number(owner.tokenId);
      tokenOwnerMap.set(tokenId, wallet);
      totalTokens++;
      logger.debug(contractKey, `Added to tokenOwnerMap: wallet=${wallet}, tokenId=${tokenId}`);
    });
    logger.debug(contractKey, `Token owner map size: ${tokenOwnerMap.size}, totalTokens: ${totalTokens}, totalBurned: ${totalBurned}`);

    cacheState.progressState.step = 'fetching_tiers';
    await saveCacheStateContract(contractKey, cacheState);

    const allTokenIds = Array.from(tokenOwnerMap.keys());
    const tierCalls = allTokenIds.map(tokenId => ({
      address: contractAddress,
      abi,
      functionName: 'getNFTAttribute',
      args: [BigInt(tokenId)],
    }));
    const recordCalls = allTokenIds.map(tokenId => ({
      address: contractAddress,
      abi,
      functionName: 'userRecords',
      args: [BigInt(tokenId)],
    }));

    const [tierResults, recordResults] = await Promise.all([
      retry(
        () => batchMulticall(tierCalls, config.alchemy.batchSize),
        { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
      ),
      retry(
        () => batchMulticall(recordCalls, config.alchemy.batchSize),
        { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
      ),
    ]);
    logger.debug(contractKey, `Fetched tiers: count=${tierResults.length}, sample=${JSON.stringify(tierResults.slice(0, 2))}`);
    logger.debug(contractKey, `Fetched records: count=${recordResults.length}, sample=${JSON.stringify(recordResults.slice(0, 2))}`);

    cacheState.progressState.step = 'fetching_shares';
    await saveCacheStateContract(contractKey, cacheState);

    const totalSharesRaw = await retry(
      () => client.readContract({ address: contractAddress, abi, functionName: 'totalShares' }),
      { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
    );
    const totalShares = parseFloat(formatUnits(totalSharesRaw.toString(), 18));
    logger.debug(contractKey, `Total shares: ${totalShares}`);

    const toDistributeDay8Raw = await retry(
      () => client.readContract({ address: contractAddress, abi, functionName: 'toDistribute', args: [0] }),
      { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
    );
    const toDistributeDay8 = parseFloat(formatUnits(toDistributeDay8Raw.toString(), 18));
    logger.debug(contractKey, `To distribute day 8: ${toDistributeDay8}`);

    const toDistributeDay28Raw = await retry(
      () => client.readContract({ address: contractAddress, abi, functionName: 'toDistribute', args: [1] }),
      { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
    );
    const toDistributeDay28 = parseFloat(formatUnits(toDistributeDay28Raw.toString(), 18));
    logger.debug(contractKey, `To distribute day 28: ${toDistributeDay28}`);

    const toDistributeDay90Raw = await retry(
      () => client.readContract({ address: contractAddress, abi, functionName: 'toDistribute', args: [2] }),
      { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
    );
    const toDistributeDay90 = parseFloat(formatUnits(toDistributeDay90Raw.toString(), 18));
    logger.debug(contractKey, `To distribute day 90: ${toDistributeDay90}`);

    const maxTier = Math.max(...Object.keys(config.contractTiers[contractKey]).map(Number));
    const walletTokenIds = new Map();
    allTokenIds.forEach(tokenId => {
      const wallet = tokenOwnerMap.get(tokenId);
      if (!wallet) {
        logger.debug(contractKey, `Skipped tokenId ${tokenId}: no wallet found`);
        return;
      }
      if (!walletTokenIds.has(wallet)) walletTokenIds.set(wallet, []);
      walletTokenIds.get(wallet).push(tokenId);
    });
    logger.debug(contractKey, `Wallet token IDs map size: ${walletTokenIds.size}`);

    const claimableCalls = Array.from(walletTokenIds.entries()).map(([wallet, tokenIds]) => ({
      address: contractAddress,
      abi,
      functionName: 'batchClaimableAmount',
      args: [tokenIds.map(id => BigInt(id))],
    }));

    const claimableResults = await retry(
      () => batchMulticall(claimableCalls, config.alchemy.batchSize),
      { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
    );
    logger.debug(contractKey, `Fetched claimable results: count=${claimableResults.length}, sample=${JSON.stringify(claimableResults.slice(0, 2))}`);

    allTokenIds.forEach((tokenId, i) => {
      const wallet = tokenOwnerMap.get(tokenId);
      if (!wallet) {
        logger.debug(contractKey, `Skipped tokenId ${tokenId}: no wallet in tokenOwnerMap`);
        return;
      }
      if (!holdersMap.has(wallet)) {
        holdersMap.set(wallet, {
          wallet,
          total: 0,
          multiplierSum: 0,
          tiers: Array(maxTier + 1).fill(0),
          shares: 0,
          lockedAscendant: 0,
          pendingDay8: 0,
          pendingDay28: 0,
          pendingDay90: 0,
          claimableRewards: 0,
          percentage: 0,
          rank: 0,
          displayMultiplierSum: 0,
        });
      }
      const holder = holdersMap.get(wallet);

      const tierResult = tierResults[i];
      let tier;
      if (tierResult?.status === 'success') {
        if (Array.isArray(tierResult.result) && tierResult.result.length >= 2) {
          tier = Number(tierResult.result[1]);
        } else if (typeof tierResult.result === 'object' && tierResult.result.tier !== undefined) {
          tier = Number(tierResult.result.tier);
        }
      }
      if (tier >= 1 && tier <= maxTier) {
        holder.tiers[tier]++;
        holder.total += 1;
        holder.multiplierSum += config.contractTiers[contractKey][tier]?.multiplier || 0;
        logger.debug(contractKey, `Assigned tier ${tier} to wallet ${wallet} for tokenId ${tokenId}`);
      } else {
        logger.warn(contractKey, `Invalid tier ${tier} for tokenId ${tokenId}, wallet ${wallet}`);
      }

      const recordResult = recordResults[i];
      if (recordResult?.status === 'success' && Array.isArray(recordResult.result)) {
        const sharesRaw = recordResult.result[0] || '0';
        const lockedAscendantRaw = recordResult.result[1] || '0';
        holder.shares += parseFloat(formatUnits(sharesRaw, 18));
        holder.lockedAscendant += parseFloat(formatUnits(lockedAscendantRaw, 18));
        logger.debug(contractKey, `Updated shares=${holder.shares}, lockedAscendant=${holder.lockedAscendant} for wallet ${wallet}`);
      } else {
        logger.warn(contractKey, `Failed to fetch record for tokenId ${tokenId}, wallet ${wallet}`);
      }
    });

    let claimableIndex = 0;
    for (const [wallet, _tokenIds] of walletTokenIds.entries()) {
      const holder = holdersMap.get(wallet);
      if (!holder) {
        logger.debug(contractKey, `Skipped claimable for wallet ${wallet}: no holder in holdersMap`);
        claimableIndex++;
        continue;
      }
      if (claimableResults[claimableIndex]?.status === 'success') {
        holder.claimableRewards = parseFloat(formatUnits(claimableResults[claimableIndex].result || '0', 18));
        logger.debug(contractKey, `Assigned claimableRewards=${holder.claimableRewards} to wallet ${wallet}`);
      } else {
        logger.warn(contractKey, `Failed to fetch claimable rewards for wallet ${wallet}`);
      }
      claimableIndex++;
    }

    const holders = Array.from(holdersMap.values());
    const totalMultiplierSum = holders.reduce((sum, h) => sum + h.multiplierSum, 0);
    const pendingRewardPerShareDay8 = totalShares > 0 ? toDistributeDay8 / totalShares : 0;
    const pendingRewardPerShareDay28 = totalShares > 0 ? toDistributeDay28 / totalShares : 0;
    const pendingRewardPerShareDay90 = totalShares > 0 ? toDistributeDay90 / totalShares : 0;

    holders.forEach(holder => {
      holder.pendingDay8 = holder.shares * pendingRewardPerShareDay8;
      holder.pendingDay28 = holder.shares * pendingRewardPerShareDay28;
      holder.pendingDay90 = holder.shares * pendingRewardPerShareDay90;
      holder.percentage = totalMultiplierSum > 0 ? (holder.multiplierSum / totalMultiplierSum) * 100 : 0;
      holder.rank = 0;
      holder.displayMultiplierSum = holder.multiplierSum;
      logger.debug(contractKey, `Final holder metrics for wallet ${holder.wallet}: total=${holder.total}, multiplierSum=${holder.multiplierSum}, percentage=${holder.percentage}`);
    });

    holders.sort((a, b) => b.shares - a.shares || b.multiplierSum - a.multiplierSum || b.total - a.total);
    holders.forEach((holder, index) => (holder.rank = index + 1));
    logger.debug(contractKey, `Sorted holders: count=${holders.length}, topHolder=${JSON.stringify(holders[0])}`);

    cacheState.totalOwners = holders.length;
    cacheState.progressState.step = 'completed';
    cacheState.progressState.processedNfts = cacheState.progressState.totalNfts;
    cacheState.progressState.processedTiers = cacheState.progressState.totalTiers;
    cacheState.progressState.error = null;
    cacheState.progressState.errorLog = errorLog;
    await saveCacheStateContract(contractKey, cacheState);
    logger.debug(contractKey, `Holders map size: ${holdersMap.size}, totalBurned: ${owners.length - totalTokens}, totalOwners: ${cacheState.totalOwners}`);
    return {
      holdersMap,
      totalBurned: owners.length - totalTokens,
      lastBlock: Number(currentBlock),
      totalShares,
      toDistributeDay8,
      toDistributeDay28,
      toDistributeDay90,
      pendingRewards: toDistributeDay8 + toDistributeDay28 + toDistributeDay90,
    };
  } else {
    const totalSupply = await retry(
      async () => {
        const result = await client.readContract({
          address: contractAddress,
          abi,
          functionName: 'totalSupply',
        });
        if (result === null || result === undefined) throw new Error('totalSupply returned null');
        return Number(result);
      },
      { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
    );
    logger.debug(contractKey, `Total supply: ${totalSupply}`);

    const burnedCountContract = await retry(
      async () => {
        const result = await client.readContract({
          address: contractAddress,
          abi,
          functionName: 'totalBurned',
        });
        if (result === null || result === undefined) throw new Error('totalBurned returned null');
        return Number(result);
      },
      { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
    );
    totalBurned = burnedCountContract || 0;
    logger.debug(contractKey, `Burned count from contract: ${totalBurned}`);

    cacheState.progressState.totalNfts = totalSupply || 0;
    cacheState.progressState.totalTiers = totalSupply || 0;
    cacheState.lastProcessedBlock = Number(currentBlock);
    await saveCacheStateContract(contractKey, cacheState);

    if (totalSupply === 0) {
      cacheState.progressState.step = 'completed';
      await saveCacheStateContract(contractKey, cacheState);
      logger.debug(contractKey, `No NFTs (totalSupply=0), returning empty holdersMap`);
      return { holdersMap, totalBurned, lastBlock: Number(currentBlock) };
    }

    cacheState.progressState.step = 'fetching_owners';
    await saveCacheStateContract(contractKey, cacheState);

    logger.debug(contractKey, `Fetching owners for ${totalSupply} tokens using Alchemy NFT API`);
    let owners = [];
    let pageKey = null;
    try {
      do {
        const fetchedOwners = await retry(
          () => getOwnersForContract(contractAddress, abi, { withTokenBalances: true, pageKey }),
          { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
        );
        logger.debug(contractKey, `Fetched owners: count=${fetchedOwners.length}, pageKey=${pageKey}, sample=${JSON.stringify(fetchedOwners.slice(0, 2))}`);
        owners.push(...fetchedOwners);
        pageKey = fetchedOwners.pageKey;
      } while (pageKey);
    } catch (error) {
      errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_owners', error: error.message });
      logger.error(contractKey, `Failed to fetch owners: ${error.message}`, { stack: error.stack });
      throw error;
    }
    logger.debug(contractKey, `Total owners fetched: ${owners.length}, sample=${JSON.stringify(owners.slice(0, 2))}`);

    let processedTokens = 0;
    owners.forEach(owner => {
      const wallet = owner.ownerAddress?.toLowerCase();
      logger.debug(contractKey, `Processing owner: wallet=${wallet}, tokenId=${owner.tokenId}`);
      if (!wallet || wallet === burnAddress.toLowerCase()) {
        totalBurned++;
        logger.debug(contractKey, `Skipped token: tokenId=${owner.tokenId}, reason=${!wallet ? 'missing wallet' : 'burn address'}`);
        return;
      }
      const tokenId = Number(owner.tokenId);
      const holder = holdersMap.get(wallet) || {
        wallet,
        tokenIds: [],
        tiers: Array(Object.keys(config.contractTiers[contractKey]).length).fill(0),
        total: 0,
        multiplierSum: 0,
        ...(contractKey === 'element369' ? { infernoRewards: 0, fluxRewards: 0, e280Rewards: 0 } : {}),
        ...(contractKey === 'element280' || contractKey === 'stax' ? { claimableRewards: 0 } : {}),
      };
      holder.tokenIds.push(tokenId);
      holder.total += 1;
      holdersMap.set(wallet, holder);
      processedTokens++;
      logger.debug(contractKey, `Added to holdersMap: wallet=${wallet}, totalTokens=${holder.total}`);
      cacheState.progressState.processedNfts = processedTokens;
      if (processedTokens % 1000 === 0) {
        saveCacheStateContract(contractKey, cacheState);
      }
    });
    logger.debug(contractKey, `Holders map size: ${holdersMap.size}, totalBurned: ${totalBurned}, processedTokens: ${processedTokens}`);
    await saveCacheStateContract(contractKey, cacheState);
    await setCache(`${contractKey.toLowerCase()}_holders_partial`, { holders: Array.from(holdersMap.values()), totalBurned, timestamp: Date.now() }, 0, contractKey.toLowerCase());
    logger.info(contractKey, `Fetched ${processedTokens} owners, ${holdersMap.size} unique holders`);

    cacheState.progressState.step = 'fetching_tiers';
    cacheState.progressState.processedTiers = 0;
    await saveCacheStateContract(contractKey, cacheState);

    const allTokenIds = Array.from(holdersMap.values()).flatMap(h => h.tokenIds);
    const tierCalls = allTokenIds.map(tokenId => ({
      address: contractAddress,
      abi,
      functionName: 'getNftTier',
      args: [tokenId],
    }));

    if (tierCalls.length > 0) {
      const chunkSize = config.nftContracts[contractKey]?.maxTokensPerOwnerQuery || 1000;
      const concurrencyLimit = pLimit(4);
      logger.debug(contractKey, `Fetching tiers for ${tierCalls.length} tokens in chunks of ${chunkSize}`);
      const tierPromises = [];
      for (let i = 0; i < tierCalls.length; i += chunkSize) {
        const chunk = tierCalls.slice(i, i + chunkSize);
        tierPromises.push(
          concurrencyLimit(async () => {
            logger.debug(contractKey, `Processing tier batch ${i / chunkSize + 1} with ${chunk.length} calls`);
            try {
              const tierResults = await retry(() => batchMulticall(chunk, config.alchemy.batchSize), {
                retries: config.alchemy.maxRetries,
                delay: config.alchemy.batchDelayMs,
              });
              tierResults.forEach((result, index) => {
                const tokenId = allTokenIds[i + index];
                let owner;
                for (const h of holdersMap.values()) {
                  if (h.tokenIds.includes(tokenId)) {
                    owner = h.wallet;
                    break;
                  }
                }
                if (!owner) {
                  logger.debug(contractKey, `Skipped tier for tokenId ${tokenId}: no owner found`);
                  return;
                }

                const holder = holdersMap.get(owner);
                if (result.status === 'success') {
                  const tier = Number(result.result);
                  const maxTier = Object.keys(config.contractTiers[contractKey]).length;
                  if (tier >= 1 && tier <= maxTier) {
                    holder.tiers[tier - 1]++;
                    holder.multiplierSum += config.contractTiers[contractKey][tier]?.multiplier || 0;
                    logger.debug(contractKey, `Tier ${tier} for token ${tokenId} assigned to wallet ${owner}`);
                  } else {
                    logger.warn(contractKey, `Invalid tier ${tier} for token ${tokenId}, wallet ${owner}`);
                  }
                } else {
                  logger.error(contractKey, `Failed to fetch tier for token ${tokenId}: ${result.error || 'unknown error'}`);
                  errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_tier', tokenId, error: result.error || 'unknown error' });
                }
              });
              cacheState.progressState.processedTiers += chunk.length;
              await saveCacheStateContract(contractKey, cacheState);
              logger.debug(contractKey, `Processed ${cacheState.progressState.processedTiers}/${cacheState.progressState.totalTiers} tiers`);
            } catch (error) {
              logger.error(contractKey, `Tier batch ${i / chunkSize + 1} failed: ${error.message}`, { stack: error.stack });
              errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_tier_batch', batch: i / chunkSize + 1, error: error.message });
            }
          })
        );
      }
      await Promise.all(tierPromises);
    }

    if (contractKey === 'element369' && vaultAddress && vaultAbi) {
      cacheState.progressState.step = 'fetching_rewards';
      await saveCacheStateContract(contractKey, cacheState);

      const holders = Array.from(holdersMap.values());
      logger.debug(contractKey, `Fetching rewards for ${holders.length} holders`);

      const rewardBatchSize = config.alchemy.batchSize || 100;
      const concurrencyLimit = pLimit(4);
      const rewardPromises = [];
      let processedHolders = 0;
      for (let i = 0; i < holders.length; i += rewardBatchSize) {
        const batchHolders = holders.slice(i, i + rewardBatchSize);
        const rewardCalls = batchHolders.map(holder => {
          const tokenIds = holder.tokenIds;
          return {
            address: vaultAddress,
            abi: vaultAbi,
            functionName: 'getRewards',
            args: [tokenIds.map(id => BigInt(id)), holder.wallet, false],
          };
        });

        rewardPromises.push(
          concurrencyLimit(async () => {
            logger.debug(contractKey, `Processing reward batch ${i / rewardBatchSize + 1} with ${rewardCalls.length} calls`);
            try {
              const rewardsResults = await retry(() => batchMulticall(rewardCalls, rewardBatchSize), {
                retries: config.alchemy.maxRetries,
                delay: config.alchemy.batchDelayMs,
              });

              batchHolders.forEach((holder, j) => {
                const result = rewardsResults[j];
                if (result?.status === 'success' && result.result) {
                  const [, , infernoPool, fluxPool, e280Pool] = result.result;
                  holder.infernoRewards = Number(infernoPool || 0) / 1e18;
                  holder.fluxRewards = Number(fluxPool || 0) / 1e18;
                  holder.e280Rewards = Number(e280Pool || 0) / 1e18;
                  logger.debug(contractKey, `Rewards for wallet ${holder.wallet}: inferno=${holder.infernoRewards}, flux=${holder.fluxRewards}, e280=${holder.e280Rewards}`);
                } else {
                  holder.infernoRewards = 0;
                  holder.fluxRewards = 0;
                  holder.e280Rewards = 0;
                  const errorMsg = result?.error || 'Unknown error';
                  logger.error(contractKey, `Failed to fetch rewards for wallet ${holder.wallet}: ${errorMsg}`);
                  errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_rewards', wallet: holder.wallet, error: errorMsg });
                }
                holdersMap.set(holder.wallet, holder);
              });

              processedHolders += batchHolders.length;
              cacheState.progressState.processedNfts = processedHolders;
              cacheState.progressState.processedTiers = processedHolders;
              await saveCacheStateContract(contractKey, cacheState);
              logger.debug(contractKey, `Processed ${processedHolders}/${holders.length} holders for rewards`);
            } catch (error) {
              logger.error(contractKey, `Reward batch ${i / rewardBatchSize + 1} failed: ${error.message}`, { stack: error.stack });
              errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_rewards_batch', batch: i / rewardBatchSize + 1, error: error.message });
            }
          })
        );
      }
      await Promise.all(rewardPromises);
    }

    if ((contractKey === 'element280' || contractKey === 'stax') && vaultAddress && vaultAbi) {
      cacheState.progressState.step = 'fetching_rewards';
      await saveCacheStateContract(contractKey, cacheState);

      const holders = Array.from(holdersMap.values());
      logger.debug(contractKey, `Fetching rewards for ${holders.length} holders`);

      const rewardBatchSize = config.alchemy.batchSize || 100;
      const concurrencyLimit = pLimit(4);
      const rewardPromises = [];
      let processedHolders = 0;
      for (let i = 0; i < holders.length; i += rewardBatchSize) {
        const batchHolders = holders.slice(i, i + rewardBatchSize);
        const rewardCalls = batchHolders.map(holder => ({
          address: vaultAddress,
          abi: vaultAbi,
          functionName: 'getRewards',
          args: [holder.tokenIds.map(id => BigInt(id)), holder.wallet],
        }));

        rewardPromises.push(
          concurrencyLimit(async () => {
            logger.debug(contractKey, `Processing reward batch ${i / rewardBatchSize + 1} with ${rewardCalls.length} calls`);
            try {
              const rewardResults = await retry(() => batchMulticall(rewardCalls, rewardBatchSize), {
                retries: config.alchemy.maxRetries,
                delay: config.alchemy.batchDelayMs,
              });

              batchHolders.forEach((holder, j) => {
                const result = rewardResults[j];
                if (result.status === 'success') {
                  const rewardValue = BigInt(result.result[1] || 0);
                  holder.claimableRewards = Number(rewardValue) / 1e18;
                  if (isNaN(holder.claimableRewards)) {
                    holder.claimableRewards = 0;
                  }
                  logger.debug(contractKey, `Rewards for wallet ${holder.wallet}: claimable=${holder.claimableRewards}`);
                } else {
                  holder.claimableRewards = 0;
                  const errorMsg = result?.error || 'Unknown error';
                  logger.error(contractKey, `Failed to fetch rewards for wallet ${holder.wallet}: ${errorMsg}`);
                  errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_rewards', wallet: holder.wallet, error: errorMsg });
                }
                holdersMap.set(holder.wallet, holder);
              });

              processedHolders += batchHolders.length;
              cacheState.progressState.processedNfts = processedHolders;
              cacheState.progressState.processedTiers = processedHolders;
              await saveCacheStateContract(contractKey, cacheState);
              logger.debug(contractKey, `Processed ${processedHolders}/${holders.length} holders for rewards`);
            } catch (error) {
              logger.error(contractKey, `Reward batch ${i / rewardBatchSize + 1} failed: ${error.message}`, { stack: error.stack });
              errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_rewards_batch', batch: i / rewardBatchSize + 1, error: error.message });
            }
          })
        );
      }
      await Promise.all(rewardPromises);
    }

    cacheState.progressState.step = 'calculating_metrics';
    await saveCacheStateContract(contractKey, cacheState);

    const holders = Array.from(holdersMap.values());
    const totalMultiplierSum = holders.reduce((sum, h) => sum + h.multiplierSum, 0);
    holders.forEach(holder => {
      holder.percentage = totalMultiplierSum > 0 ? (holder.multiplierSum / totalMultiplierSum) * 100 : 0;
      holder.displayMultiplierSum = holder.multiplierSum / (contractKey === 'element280' ? 10 : 1);
      logger.debug(contractKey, `Calculated metrics for wallet ${holder.wallet}: percentage=${holder.percentage}, displayMultiplierSum=${holder.displayMultiplierSum}`);
    });

    holders.sort((a, b) => b.multiplierSum - a.multiplierSum || b.total - a.total);
    holders.forEach((holder, index) => (holder.rank = index + 1));
    logger.debug(contractKey, `Sorted holders: count=${holders.length}, topHolder=${JSON.stringify(holders[0])}`);

    cacheState.totalOwners = holders.length;
    cacheState.progressState.step = 'completed';
    cacheState.progressState.processedNfts = cacheState.progressState.totalNfts;
    cacheState.progressState.processedTiers = cacheState.progressState.totalTiers;
    cacheState.progressState.error = null;
    cacheState.progressState.errorLog = errorLog;
    await saveCacheStateContract(contractKey, cacheState);
    logger.info(contractKey, `Completed holders map with ${holders.length} holders, totalBurned=${totalBurned}`);
    return { holdersMap, totalBurned, lastBlock: Number(currentBlock) };
  }
}

async function populateHoldersMapCache(contractKey, contractAddress, abi, vaultAddress, vaultAbi, forceUpdate = false) {
  let cacheState = await getCacheState(contractKey);
  if (cacheState.isPopulating && !forceUpdate) {
    logger.info(contractKey, 'Cache population already in progress');
    return { status: 'in_progress', holders: null };
  }

  cacheState.isPopulating = true;
  cacheState.progressState.step = 'starting';
  cacheState.progressState.error = null;
  cacheState.progressState.errorLog = [];
  await saveCacheStateContract(contractKey, cacheState);

  const errorLog = [];

  try {
    const cachedData = await getCache(`${contractKey.toLowerCase()}_holders`, contractKey.toLowerCase());
    const isCacheValid = cachedData && Array.isArray(cachedData.holders) && Number.isInteger(cachedData.totalBurned) && !forceUpdate;

    if (isCacheValid) {
      const fromBlock = cacheState.lastProcessedBlock || config.deploymentBlocks[contractKey].block;
      const { burnedTokenIds, transferTokenIds, lastBlock } = await getNewEvents(contractKey, contractAddress, fromBlock, errorLog);

      let currentBlock;
      try {
        currentBlock = await client.getBlockNumber();
      } catch (error) {
        errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_block_number', error: error.message });
        throw error;
      }

      if (burnedTokenIds.length > 0 || transferTokenIds.length > 0) {
        const holdersMap = new Map();
        let totalBurned = cachedData.totalBurned || 0; // Initialize from cached data
        logger.debug(contractKey, `Initial totalBurned from cache: ${totalBurned}`);

        for (const holder of cachedData.holders) {
          const updatedTokenIds = holder.tokenIds.filter(id => !burnedTokenIds.includes(id));
          if (updatedTokenIds.length > 0) {
            const updatedHolder = {
              ...holder,
              tokenIds: updatedTokenIds,
              total: updatedTokenIds.length,
              tiers: Array(Object.keys(config.contractTiers[contractKey]).length).fill(0),
              multiplierSum: 0,
              ...(contractKey === 'element369' ? { infernoRewards: 0, fluxRewards: 0, e280Rewards: 0 } : {}),
              ...(contractKey === 'element280' || contractKey === 'stax' ? { claimableRewards: 0 } : {}),
              ...(contractKey === 'ascendant' ? {
                shares: 0,
                lockedAscendant: 0,
                pendingDay8: 0,
                pendingDay28: 0,
                pendingDay90: 0,
                claimableRewards: 0,
              } : {}),
            };
            const tierCalls = updatedTokenIds.map(tokenId => ({
              address: contractAddress,
              abi,
              functionName: contractKey === 'ascendant' ? 'getNFTAttribute' : 'getNftTier',
              args: [tokenId],
            }));
            const tierResults = await batchMulticall(tierCalls);
            tierResults.forEach((result, index) => {
              if (result.status === 'success') {
                const tier = contractKey === 'ascendant' ? 
                  (Array.isArray(result.result) ? Number(result.result[1]) : Number(result.result.tier)) : 
                  Number(result.result);
                const maxTier = Object.keys(config.contractTiers[contractKey]).length;
                if (tier >= 1 && tier <= maxTier) {
                  updatedHolder.tiers[tier - 1]++;
                  updatedHolder.multiplierSum += config.contractTiers[contractKey][tier]?.multiplier || 0;
                }
              }
            });
            holdersMap.set(holder.wallet, updatedHolder);
          } else {
            totalBurned += holder.total; // Increment totalBurned for removed holders
          }
        }

        for (const transfer of transferTokenIds) {
          const fromHolder = holdersMap.get(transfer.from);
          if (fromHolder) {
            fromHolder.tokenIds = fromHolder.tokenIds.filter(id => id !== transfer.tokenId);
            fromHolder.total = fromHolder.tokenIds.length;
            if (fromHolder.total === 0) {
              holdersMap.delete(transfer.from);
            } else {
              fromHolder.tiers = Array(Object.keys(config.contractTiers[contractKey]).length).fill(0);
              fromHolder.multiplierSum = 0;
              const tierResult = await client.readContract({
                address: contractAddress,
                abi,
                functionName: contractKey === 'ascendant' ? 'getNFTAttribute' : 'getNftTier',
                args: [transfer.tokenId],
              });
              const tier = contractKey === 'ascendant' ? 
                (Array.isArray(tierResult) ? Number(tierResult[1]) : Number(tierResult.tier)) : 
                Number(tierResult);
              if (tier >= 1 && tier <= Object.keys(config.contractTiers[contractKey]).length) {
                fromHolder.tiers[tier - 1]++;
                fromHolder.multiplierSum += config.contractTiers[contractKey][tier]?.multiplier || 0;
              }
              holdersMap.set(transfer.from, fromHolder);
            }
          }

          const toHolder = holdersMap.get(transfer.to) || {
            wallet: transfer.to,
            tokenIds: [],
            tiers: Array(Object.keys(config.contractTiers[contractKey]).length).fill(0),
            total: 0,
            multiplierSum: 0,
            ...(contractKey === 'element369' ? { infernoRewards: 0, fluxRewards: 0, e280Rewards: 0 } : {}),
            ...(contractKey === 'element280' || contractKey === 'stax' ? { claimableRewards: 0 } : {}),
            ...(contractKey === 'ascendant' ? {
              shares: 0,
              lockedAscendant: 0,
              pendingDay8: 0,
              pendingDay28: 0,
              pendingDay90: 0,
              claimableRewards: 0,
            } : {}),
          };
          toHolder.tokenIds.push(transfer.tokenId);
          toHolder.total = toHolder.tokenIds.length;
          const tierResult = await client.readContract({
            address: contractAddress,
            abi,
            functionName: contractKey === 'ascendant' ? 'getNFTAttribute' : 'getNftTier',
            args: [transfer.tokenId],
          });
          const tier = contractKey === 'ascendant' ? 
            (Array.isArray(tierResult) ? Number(tierResult[1]) : Number(tierResult.tier)) : 
            Number(tierResult);
          if (tier >= 1 && tier <= Object.keys(config.contractTiers[contractKey]).length) {
            toHolder.tiers[tier - 1]++;
            toHolder.multiplierSum += config.contractTiers[contractKey][tier]?.multiplier || 0;
          }
          holdersMap.set(transfer.to, toHolder);
        }

        if (contractKey === 'element369' && vaultAddress && vaultAbi) {
          const holders = Array.from(holdersMap.values());
          const rewardCalls = holders.map(holder => ({
            address: vaultAddress,
            abi: vaultAbi,
            functionName: 'getRewards',
            args: [holder.tokenIds.map(id => BigInt(id)), holder.wallet, false],
          }));
          const rewardsResults = await batchMulticall(rewardCalls);
          holders.forEach((holder, i) => {
            if (rewardsResults[i]?.status === 'success' && rewardsResults[i].result) {
              const [, , infernoPool, fluxPool, e280Pool] = rewardsResults[i].result;
              holder.infernoRewards = Number(infernoPool) / 1e18;
              holder.fluxRewards = Number(fluxPool) / 1e18;
              holder.e280Rewards = Number(e280Pool) / 1e18;
            } else {
              holder.infernoRewards = 0;
              holder.fluxRewards = 0;
              holder.e280Rewards = 0;
            }
          });
        }

        if ((contractKey === 'element280' || contractKey === 'stax') && vaultAddress && vaultAbi) {
          const holders = Array.from(holdersMap.values());
          const rewardCalls = holders.flatMap(holder =>
            holder.tokenIds.map(tokenId => ({
              address: vaultAddress,
              abi: vaultAbi,
              functionName: 'getRewards',
              args: [[BigInt(tokenId)], holder.wallet],
            }))
          );
          const rewardResults = await batchMulticall(rewardCalls);
          let resultIndex = 0;
          holders.forEach(holder => {
            let totalRewards = 0n;
            holder.tokenIds.forEach(() => {
              const result = rewardResults[resultIndex++];
              if (result.status === 'success') {
                const rewardValue = BigInt(result.result[1] || 0);
                totalRewards += rewardValue;
              }
            });
            holder.claimableRewards = Number(totalRewards) / 1e18;
            if (isNaN(holder.claimableRewards)) {
              holder.claimableRewards = 0;
            }
          });
        }

        if (contractKey === 'ascendant') {
          const holders = Array.from(holdersMap.values());
          const recordCalls = holders.flatMap(holder =>
            holder.tokenIds.map(tokenId => ({
              address: contractAddress,
              abi,
              functionName: 'userRecords',
              args: [BigInt(tokenId)],
            }))
          );
          const recordResults = await batchMulticall(recordCalls);
          const claimableCalls = holders.map(holder => ({
            address: contractAddress,
            abi,
            functionName: 'batchClaimableAmount',
            args: [holder.tokenIds.map(id => BigInt(id))],
          }));
          const claimableResults = await batchMulticall(claimableCalls);

          const totalSharesRaw = await client.readContract({ address: contractAddress, abi, functionName: 'totalShares' });
          const totalShares = parseFloat(formatUnits(totalSharesRaw.toString(), 18));
          const toDistributeDay8Raw = await client.readContract({ address: contractAddress, abi, functionName: 'toDistribute', args: [0] });
          const toDistributeDay8 = parseFloat(formatUnits(toDistributeDay8Raw.toString(), 18));
          const toDistributeDay28Raw = await client.readContract({ address: contractAddress, abi, functionName: 'toDistribute', args: [1] });
          const toDistributeDay28 = parseFloat(formatUnits(toDistributeDay28Raw.toString(), 18));
          const toDistributeDay90Raw = await client.readContract({ address: contractAddress, abi, functionName: 'toDistribute', args: [2] });
          const toDistributeDay90 = parseFloat(formatUnits(toDistributeDay90Raw.toString(), 18));

          let resultIndex = 0;
          holders.forEach((holder, i) => {
            holder.shares = 0;
            holder.lockedAscendant = 0;
            holder.tokenIds.forEach(() => {
              const recordResult = recordResults[resultIndex++];
              if (recordResult?.status === 'success' && Array.isArray(recordResult.result)) {
                const sharesRaw = recordResult.result[0] || '0';
                const lockedAscendantRaw = recordResult.result[1] || '0';
                holder.shares += parseFloat(formatUnits(sharesRaw, 18));
                holder.lockedAscendant += parseFloat(formatUnits(lockedAscendantRaw, 18));
              }
            });
            if (claimableResults[i]?.status === 'success') {
              holder.claimableRewards = parseFloat(formatUnits(claimableResults[i].result || '0', 18));
            }
            const pendingRewardPerShareDay8 = totalShares > 0 ? toDistributeDay8 / totalShares : 0;
            const pendingRewardPerShareDay28 = totalShares > 0 ? toDistributeDay28 / totalShares : 0;
            const pendingRewardPerShareDay90 = totalShares > 0 ? toDistributeDay90 / totalShares : 0;
            holder.pendingDay8 = holder.shares * pendingRewardPerShareDay8;
            holder.pendingDay28 = holder.shares * pendingRewardPerShareDay28;
            holder.pendingDay90 = holder.shares * pendingRewardPerShareDay90;
          });
        }

        let burnedCountContract;
        try {
          burnedCountContract = await retry(
            async () => {
              const result = await client.readContract({
                address: contractAddress,
                abi,
                functionName: 'totalBurned',
              });
              return Number(result);
            },
            { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
          );
          logger.debug(contractKey, `Fetched burnedCountContract: ${burnedCountContract}`);
        } catch (error) {
          logger.error(contractKey, `Failed to fetch totalBurned: ${error.message}`, { stack: error.stack });
          burnedCountContract = 0; // Fallback to 0 if contract call fails
        }
        totalBurned = burnedCountContract || totalBurned; // Use contract value or fallback to computed value
        logger.debug(contractKey, `Final totalBurned: ${totalBurned}`);

        const holders = Array.from(holdersMap.values());
        const totalMultiplierSum = holders.reduce((sum, h) => sum + h.multiplierSum, 0);
        holders.forEach(holder => {
          holder.percentage = totalMultiplierSum > 0 ? (holder.multiplierSum / totalMultiplierSum) * 100 : 0;
          holder.displayMultiplierSum = holder.multiplierSum / (contractKey === 'element280' ? 10 : 1);
        });

        holders.sort((a, b) => b.multiplierSum - a.multiplierSum || b.total - a.total);
        holders.forEach((holder, index) => (holder.rank = index + 1));

        const cacheData = { holders, totalBurned, timestamp: Date.now() };
        await setCache(`${contractKey.toLowerCase()}_holders`, cacheData, 0, contractKey.toLowerCase());
        cacheState.lastUpdated = Date.now();
        cacheState.totalOwners = holders.length;
        cacheState.lastProcessedBlock = lastBlock;
        cacheState.progressState = {
          step: 'completed',
          processedNfts: cacheState.progressState.totalNfts,
          totalNfts: cacheState.progressState.totalNfts,
          processedTiers: cacheState.progressState.totalTiers,
          totalTiers: cacheState.progressState.totalTiers,
          error: null,
          errorLog: [],
        };
        await saveCacheStateContract(contractKey, cacheState);
        logger.info(contractKey, `Cache updated: ${holders.length} holders, totalBurned: ${totalBurned}`);
        return { status: 'updated', holders };
      } else {
        cacheState.isPopulating = false;
        cacheState.progressState.step = 'completed';
        cacheState.lastProcessedBlock = Number(currentBlock);
        await saveCacheStateContract(contractKey, cacheState);
        logger.info(contractKey, 'Cache is up to date');
        return { status: 'up_to_date', holders: cachedData.holders };
      }
    }

    const result = await getHoldersMap(contractKey, contractAddress, abi, vaultAddress, vaultAbi, cacheState);
    const holders = Array.from(result.holdersMap.values());
    const totalBurned = result.totalBurned || 0; // Ensure totalBurned is defined
    logger.debug(contractKey, `getHoldersMap returned totalBurned: ${totalBurned}`);
    const cacheData = { holders, totalBurned, timestamp: Date.now() };
    await setCache(`${contractKey.toLowerCase()}_holders`, cacheData, 0, contractKey.toLowerCase());
    cacheState.lastUpdated = Date.now();
    cacheState.totalOwners = holders.length;
    cacheState.lastProcessedBlock = result.lastBlock;
    cacheState.progressState = {
      step: 'completed',
      processedNfts: cacheState.progressState.totalNfts,
      totalNfts: cacheState.progressState.totalNfts,
      processedTiers: cacheState.progressState.totalTiers,
      totalTiers: cacheState.progressState.totalTiers,
      error: null,
      errorLog: [],
    };
    await saveCacheStateContract(contractKey, cacheState);
    logger.info(contractKey, `Cache populated: ${holders.length} holders, totalBurned: ${totalBurned}`);
    return { status: 'completed', holders };
  } catch (error) {
    cacheState.progressState.step = 'error';
    cacheState.progressState.error = error.message;
    cacheState.progressState.errorLog = errorLog;
    await saveCacheStateContract(contractKey, cacheState);
    logger.error(contractKey, `Cache population failed: ${error.message}`, { stack: error.stack });
    return { status: 'error', holders: null, error: error.message };
  } finally {
    cacheState.isPopulating = false;
    await saveCacheStateContract(contractKey, cacheState);
  }
}

export async function GET(request, { params }) {
  const { contract } = await params; // Await params
  const contractKey = contract.toLowerCase();
  if (!config.contractDetails[contractKey]) {
    return NextResponse.json({ error: `Invalid contract: ${contractKey}` }, { status: 400 });
  }

  if (config.contractDetails[contractKey].disabled) {
    return NextResponse.json({ error: `${contractKey} contract not deployed` }, { status: 400 });
  }

  const contractAddress = config.contractAddresses[contractKey]?.address;
  const abi = config.abis[contractKey]?.main;
  const vaultAddress = config.vaultAddresses[contractKey]?.address;
  const vaultAbi = config.abis[contractKey]?.vault;

  try {
    const cacheState = await getCacheState(contractKey);
    if (cacheState.isPopulating) {
      return NextResponse.json({
        message: 'Cache is populating',
        isCachePopulating: true,
        totalOwners: cacheState.totalOwners,
        progressState: cacheState.progressState,
        lastProcessedBlock: cacheState.lastProcessedBlock,
        debugId: `state-${Math.random().toString(36).slice(2)}`,
      }, { status: 202 });
    }

    const { searchParams } = new URL(request.url);
    const page = parseInt(searchParams.get('page') || '0', 10);
    const pageSize = parseInt(searchParams.get('pageSize') || config.contractDetails[contractKey].pageSize, 10);
    const wallet = searchParams.get('wallet');

    if (wallet) {
      const cachedData = await getCache(`${contractKey}_holder_${contractAddress}-${wallet.toLowerCase()}`, contractKey.toLowerCase());
      if (cachedData) {
        return NextResponse.json(cachedData);
      }

      const nfts = await retry(
        () => getOwnersForContract(contractAddress, abi, { withTokenBalances: true }),
        { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
      ).then(owners =>
        owners.filter(o => o.ownerAddress.toLowerCase() === wallet.toLowerCase() && o.ownerAddress.toLowerCase() !== config.burnAddress.toLowerCase())
      );

      const holder = {
        wallet: wallet.toLowerCase(),
        total: nfts.length,
        tiers: Array(Object.keys(config.contractTiers[contractKey]).length).fill(0),
        tokenIds: nfts.map(nft => Number(nft.tokenId)),
        multiplierSum: 0,
        displayMultiplierSum: 0,
        percentage: 0,
        rank: 0,
        ...(contractKey === 'element369' ? { infernoRewards: 0, fluxRewards: 0, e280Rewards: 0 } : {}),
        ...(contractKey === 'element280' || contractKey === 'stax' ? { claimableRewards: 0 } : {}),
        ...(contractKey === 'ascendant' ? {
          shares: 0,
          lockedAscendant: 0,
          pendingDay8: 0,
          pendingDay28: 0,
          pendingDay90: 0,
          claimableRewards: 0,
        } : {}),
      };

      if (nfts.length === 0) {
        return NextResponse.json({ message: 'No holder data found for wallet' }, { status: 404 });
      }

      const calls = [];
      nfts.forEach(nft => {
        calls.push({
          address: contractAddress,
          abi,
          functionName: contractKey === 'ascendant' ? 'getNFTAttribute' : 'getNftTier',
          args: [BigInt(nft.tokenId)],
        });
        if (contractKey === 'ascendant') {
          calls.push({
            address: contractAddress,
            abi,
            functionName: 'userRecords',
            args: [BigInt(nft.tokenId)],
          });
        } else if (contractKey === 'element369' && vaultAddress && vaultAbi) {
          calls.push({
            address: vaultAddress,
            abi: vaultAbi,
            functionName: 'getRewards',
            args: [[BigInt(nft.tokenId)], wallet.toLowerCase(), false],
          });
        } else if ((contractKey === 'element280' || contractKey === 'stax') && vaultAddress && vaultAbi) {
          calls.push({
            address: vaultAddress,
            abi: vaultAbi,
            functionName: 'getRewards',
            args: [[BigInt(nft.tokenId)], wallet.toLowerCase()],
          });
        }
      });

      if (contractKey === 'ascendant') {
        calls.push({
          address: contractAddress,
          abi,
          functionName: 'batchClaimableAmount',
          args: [nfts.map(nft => BigInt(nft.tokenId))],
        });
      }

      const results = await batchMulticall(calls);
      let resultIndex = 0;
      nfts.forEach(() => {
        const tierResult = results[resultIndex++];
        if (tierResult.status === 'success') {
          const tier = contractKey === 'ascendant' ? 
            (Array.isArray(tierResult.result) ? Number(tierResult.result[1]) : Number(tierResult.result.tier)) : 
            Number(tierResult.result);
          const maxTier = Object.keys(config.contractTiers[contractKey]).length;
          if (tier >= 1 && tier <= maxTier) {
            holder.tiers[tier - 1]++;
            holder.multiplierSum += config.contractTiers[contractKey][tier]?.multiplier || 0;
          }
        }

        if (contractKey === 'ascendant') {
          const recordResult = results[resultIndex++];
          if (recordResult?.status === 'success' && Array.isArray(recordResult.result)) {
            const sharesRaw = recordResult.result[0] || '0';
            const lockedAscendantRaw = recordResult.result[1] || '0';
            holder.shares += parseFloat(formatUnits(sharesRaw, 18));
            holder.lockedAscendant += parseFloat(formatUnits(lockedAscendantRaw, 18));
          }
        } else if (contractKey === 'element369') {
          const rewardResult = results[resultIndex++];
          if (rewardResult.status === 'success' && rewardResult.result) {
            const [, , infernoPool, fluxPool, e280Pool] = rewardResult.result;
            holder.infernoRewards += Number(infernoPool) / 1e18;
            holder.fluxRewards += Number(fluxPool) / 1e18;
            holder.e280Rewards += Number(e280Pool) / 1e18;
          }
        } else if (contractKey === 'element280' || contractKey === 'stax') {
          const rewardResult = results[resultIndex++];
          if (rewardResult.status === 'success') {
            const rewardValue = BigInt(rewardResult.result[1] || 0);
            holder.claimableRewards += Number(rewardValue) / 1e18;
          }
        }
      });

      if (contractKey === 'ascendant') {
        const claimableResult = results[resultIndex];
        if (claimableResult?.status === 'success') {
          holder.claimableRewards = parseFloat(formatUnits(claimableResult.result || '0', 18));
        }

        const totalSharesRaw = await client.readContract({ address: contractAddress, abi, functionName: 'totalShares' });
        const totalShares = parseFloat(formatUnits(totalSharesRaw.toString(), 18));
        const toDistributeDay8Raw = await client.readContract({ address: contractAddress, abi, functionName: 'toDistribute', args: [0] });
        const toDistributeDay8 = parseFloat(formatUnits(toDistributeDay8Raw.toString(), 18));
        const toDistributeDay28Raw = await client.readContract({ address: contractAddress, abi, functionName: 'toDistribute', args: [1] });
        const toDistributeDay28 = parseFloat(formatUnits(toDistributeDay28Raw.toString(), 18));
        const toDistributeDay90Raw = await client.readContract({ address: contractAddress, abi, functionName: 'toDistribute', args: [2] });
        const toDistributeDay90 = parseFloat(formatUnits(toDistributeDay90Raw.toString(), 18));

        const pendingRewardPerShareDay8 = totalShares > 0 ? toDistributeDay8 / totalShares : 0;
        const pendingRewardPerShareDay28 = totalShares > 0 ? toDistributeDay28 / totalShares : 0;
        const pendingRewardPerShareDay90 = totalShares > 0 ? toDistributeDay90 / totalShares : 0;
        holder.pendingDay8 = holder.shares * pendingRewardPerShareDay8;
        holder.pendingDay28 = holder.shares * pendingRewardPerShareDay28;
        holder.pendingDay90 = holder.shares * pendingRewardPerShareDay90;
      }

      holder.displayMultiplierSum = holder.multiplierSum / (contractKey === 'element280' ? 10 : 1);
      const cachedHolders = await getCache(`${contractKey.toLowerCase()}_holders`, contractKey.toLowerCase());
      if (cachedHolders && cachedHolders.holders) {
        const totalMultiplierSum = cachedHolders.holders.reduce((sum, h) => sum + h.multiplierSum, 0);
        holder.percentage = totalMultiplierSum > 0 ? (holder.multiplierSum / totalMultiplierSum) * 100 : 0;
        const sortedHolders = cachedHolders.holders.sort((a, b) => b.multiplierSum - a.multiplierSum || b.total - a.total);
        holder.rank = sortedHolders.findIndex(h => h.multiplierSum <= holder.multiplierSum && h.total <= holder.total) + 1;
      }

      await setCache(`${contractKey}_holder_${wallet.toLowerCase()}`, holder, config.cache.nodeCache.stdTTL, contractKey.toLowerCase());
      return NextResponse.json(holder);
    }

    const cachedData = await getCache(`${contractKey.toLowerCase()}_holders`, contractKey.toLowerCase());
    if (cachedData) {
      const holders = cachedData.holders.slice(page * pageSize, (page + 1) * pageSize);
      const totalPages = Math.ceil(cachedData.holders.length / pageSize);
      const response = {
        holders,
        totalPages,
        totalTokens: cachedData.holders.reduce((sum, h) => sum + h.total, 0),
        totalBurned: cachedData.totalBurned,
        summary: {
          totalLive: cachedData.holders.reduce((sum, h) => sum + h.total, 0),
          totalBurned: cachedData.totalBurned,
          totalMinted: config.nftContracts[contractKey].expectedTotalSupply + config.nftContracts[contractKey].expectedBurned,
          tierDistribution: cachedData.holders.reduce((acc, h) => {
            h.tiers.forEach((count, i) => acc[i] = (acc[i] || 0) + count);
            return acc;
          }, []),
          multiplierPool: cachedData.holders.reduce((sum, h) => sum + h.multiplierSum, 0),
          totalRewardPool: cachedData.holders.reduce((sum, h) => sum + (h.claimableRewards || 0) + (h.infernoRewards || 0) + (h.fluxRewards || 0) + (h.e280Rewards || 0), 0),
        },
      };
      if (contractKey === 'ascendant') {
        response.summary.totalShares = cachedData.totalShares || 0;
        response.summary.pendingRewards = cachedData.pendingRewards || 0;
      }
      return NextResponse.json(response);
    }

    const { status, holders } = await populateHoldersMapCache(contractKey, contractAddress, abi, vaultAddress, vaultAbi);
    if (status === 'error') throw new Error('Cache population failed');

    const paginatedHolders = holders.slice(page * pageSize, (page + 1) * pageSize);
    const totalPages = Math.ceil(holders.length / pageSize);
    const response = {
      holders: paginatedHolders,
      totalPages,
      totalTokens: holders.reduce((sum, h) => sum + h.total, 0),
      totalBurned: (await getCache(`${contractKey.toLowerCase()}_holders`, contractKey.toLowerCase()))?.totalBurned || 0,
      summary: {
        totalLive: holders.reduce((sum, h) => sum + h.total, 0),
        totalBurned: (await getCache(`${contractKey.toLowerCase()}_holders`, contractKey.toLowerCase()))?.totalBurned || 0,
        totalMinted: config.nftContracts[contractKey].expectedTotalSupply + config.nftContracts[contractKey].expectedBurned,
        tierDistribution: holders.reduce((acc, h) => {
          h.tiers.forEach((count, i) => acc[i] = (acc[i] || 0) + count);
          return acc;
        }, []),
        multiplierPool: holders.reduce((sum, h) => sum + h.multiplierSum, 0),
        totalRewardPool: holders.reduce((sum, h) => sum + (h.claimableRewards || 0) + (h.infernoRewards || 0) + (h.fluxRewards || 0) + (h.e280Rewards || 0), 0),
      },
    };
    if (contractKey === 'ascendant') {
      response.summary.totalShares = (await getCache(`${contractKey.toLowerCase()}_holders`, contractKey.toLowerCase()))?.totalShares || 0;
      response.summary.pendingRewards = (await getCache(`${contractKey.toLowerCase()}_holders`, contractKey.toLowerCase()))?.pendingRewards || 0;
    }
    return NextResponse.json(response);
  } catch (error) {
    logger.error(contractKey, `GET error: ${error.message}`, { stack: error.stack });
    return NextResponse.json({ error: `Failed to fetch ${contractKey} holders`, details: error.message }, { status: 500 });
  }
}

// POST handler
export async function POST(request, { params }) {
  const { contract } = await params; // Await params
  const contractKey = contract.toLowerCase();
  if (!config.contractDetails[contractKey]) {
    return NextResponse.json({ error: `Invalid contract: ${contractKey}` }, { status: 400 });
  }

  if (config.contractDetails[contractKey].disabled) {
    return NextResponse.json({ error: `${contractKey} contract not deployed` }, { status: 400 });
  }

  const contractAddress = config.contractAddresses[contractKey]?.address;
  const abi = config.abis[contractKey]?.main;
  const vaultAddress = config.vaultAddresses[contractKey]?.address;
  const vaultAbi = config.abis[contractKey]?.vault;

  try {
    const { forceUpdate } = await request.json().catch(() => ({}));
    const cacheState = await getCacheState(contractKey);
    if (cacheState.isPopulating && !forceUpdate) {
      return NextResponse.json({ message: 'Cache population already in progress', status: 'in_progress' }, { status: 202 });
    }
    const { status, error } = await populateHoldersMapCache(contractKey, contractAddress, abi, vaultAddress, vaultAbi, forceUpdate === true);
    if (status === 'error') throw new Error(error || 'Cache population failed');
    return NextResponse.json({ 
      message: status === 'up_to_date' ? 'Cache is up to date' : `${contractKey} cache population triggered`, 
      status 
    });
  } catch (error) {
    logger.error(contractKey, `POST error: ${error.message}`, { stack: error.stack });
    return NextResponse.json({ error: `Failed to populate ${contractKey} cache`, details: error.message }, { status: 500 });
  }
}import { NextResponse } from 'next/server';
import { logger, loadCacheState } from '@/app/api/utils';
import config from '@/config';

async function getCacheState(contractKey) {
  const cacheState = {
    isPopulating: false,
    totalOwners: 0,
    progressState: { step: 'idle', processedNfts: 0, totalNfts: 0, processedTiers: 0, totalTiers: 0, error: null, errorLog: [] },
    lastUpdated: null,
    lastProcessedBlock: null,
  };
  try {
    const savedState = await loadCacheState(contractKey, contractKey.toLowerCase());
    if (savedState && typeof savedState === 'object') {
      cacheState.isPopulating = savedState.isPopulating ?? false;
      cacheState.totalOwners = savedState.totalOwners ?? 0;
      cacheState.progressState = {
        step: savedState.progressState?.step ?? 'idle',
        processedNfts: savedState.progressState?.processedNfts ?? 0,
        totalNfts: savedState.progressState?.totalNfts ?? 0,
        processedTiers: savedState.progressState?.processedTiers ?? 0,
        totalTiers: savedState.progressState?.totalTiers ?? 0,
        error: savedState.progressState?.error ?? null,
        errorLog: savedState.progressState?.errorLog ?? [],
      };
      cacheState.lastUpdated = savedState.lastUpdated ?? null;
      cacheState.lastProcessedBlock = savedState.lastProcessedBlock ?? null;
    }
  } catch (error) {
    logger.error(contractKey, `Failed to load cache state: ${error.message}`, { stack: error.stack });
  }
  return cacheState;
}

export async function GET(_request, { params }) {
  const { contract } = await params; // Await params
  const contractKey = contract.toLowerCase();

  if (!config.contractDetails[contractKey]) {
    logger.error(contractKey, `Invalid contract: ${contractKey}`);
    return NextResponse.json({ error: `Invalid contract: ${contractKey}` }, { status: 400 });
  }

  if (config.contractDetails[contractKey].disabled) {
    return NextResponse.json({ error: `${contractKey} contract not deployed` }, { status: 400 });
  }

  try {
    const state = await getCacheState(contractKey);
    if (!state || !state.progressState) {
      logger.error(contractKey, 'Invalid cache state');
      return NextResponse.json({ error: 'Cache state not initialized' }, { status: 500 });
    }

    let progressPercentage = '0.0';
    if (state.progressState.error) {
      progressPercentage = '0.0';
    } else if (state.progressState.step === 'completed') {
      progressPercentage = '100.0';
    } else if (state.progressState.totalNfts > 0) {
      if (state.progressState.step === 'fetching_owners') {
        const ownerProgress = (state.progressState.processedNfts / state.progressState.totalNfts) * 50;
        progressPercentage = Math.min(ownerProgress, 50).toFixed(1);
      } else if (state.progressState.step === 'fetching_tiers') {
        const tierProgress = (state.progressState.processedTiers / state.progressState.totalTiers) * 50;
        progressPercentage = Math.min(50 + tierProgress, 100).toFixed(1);
      }
    }

    return NextResponse.json({
      isPopulating: state.isPopulating,
      totalLiveHolders: state.totalOwners,
      totalOwners: state.totalOwners,
      phase: state.progressState.step.charAt(0).toUpperCase() + state.progressState.step.slice(1),
      progressPercentage,
      lastProcessedBlock: state.lastProcessedBlock,
      error: state.progressState.error || null,
      errorLog: state.progressState.errorLog || [],
    });
  } catch (error) {
    logger.error(contractKey, `Progress endpoint error: ${error.message}`, { stack: error.stack });
    return NextResponse.json({ error: `Failed to fetch ${contractKey} cache state`, details: error.message }, { status: 500 });
  }
}// app/api/holders/Element280/validate-burned/route.js
import { NextResponse } from 'next/server';
import config from '@/config';
import { getTransactionReceipt, log, client, getCache, setCache } from '@/app/api/utils.js';
import { parseAbiItem } from 'viem';

export async function POST(request) {
  if (process.env.DEBUG === 'true') {
    log(`[Element280-Validate-Burned] [DEBUG] Processing POST request for validate-burned`);
  }

  try {
    const { transactionHash } = await request.json();
    if (!transactionHash || typeof transactionHash !== 'string' || !transactionHash.match(/^0x[a-fA-F0-9]{64}$/)) {
      log(`[Element280-Validate-Burned] [VALIDATION] Invalid transaction hash: ${transactionHash || 'undefined'}`);
      return NextResponse.json({ error: 'Invalid transaction hash' }, { status: 400 });
    }

    const contractAddress = config.contractAddresses?.element280?.address;
    if (!contractAddress) {
      log(`[Element280-Validate-Burned] [VALIDATION] Element280 contract address not configured in config.js`);
      return NextResponse.json({ error: 'Contract address not configured' }, { status: 500 });
    }

    const cacheKey = `element280_burn_validation_${transactionHash}`;
    const cachedResult = await getCache(cacheKey, 'element280');
    if (cachedResult) {
      if (process.env.DEBUG === 'true') {
        log(`[Element280-Validate-Burned] [DEBUG] Cache hit for burn validation: ${transactionHash}`);
      }
      return NextResponse.json(cachedResult);
    }

    if (process.env.DEBUG === 'true') {
      log(`[Element280-Validate-Burned] [DEBUG] Fetching transaction receipt for hash: ${transactionHash}`);
    }
    const receipt = await getTransactionReceipt(transactionHash);
    if (!receipt) {
      log(`[Element280-Validate-Burned] [VALIDATION] Transaction receipt not found for hash: ${transactionHash}`);
      return NextResponse.json({ error: 'Transaction not found' }, { status: 404 });
    }

    const burnAddress = '0x0000000000000000000000000000000000000000';
    const transferEvent = parseAbiItem('event Transfer(address indexed from, address indexed to, uint256 indexed tokenId)');
    const burnedTokenIds = [];

    for (const logEntry of receipt.logs) {
      if (
        logEntry.address.toLowerCase() === contractAddress.toLowerCase() &&
        logEntry.topics[0] === transferEvent.topics[0]
      ) {
        try {
          const decodedLog = client.decodeEventLog({
            abi: [transferEvent],
            data: logEntry.data,
            topics: logEntry.topics,
          });
          if (decodedLog.args.to.toLowerCase() === burnAddress) {
            burnedTokenIds.push(decodedLog.args.tokenId.toString());
          }
        } catch (_decodeError) {
          log(`[Element280-Validate-Burned] [ERROR] Failed to decode log entry for transaction ${transactionHash}: ${_decodeError.message}`);
        }
      }
    }

    if (burnedTokenIds.length === 0) {
      log(`[Element280-Validate-Burned] [VALIDATION] No burn events found in transaction: ${transactionHash}`);
      return NextResponse.json({ error: 'No burn events found in transaction' }, { status: 400 });
    }

    const result = {
      transactionHash,
      burnedTokenIds,
      blockNumber: receipt.blockNumber.toString(),
    };

    await setCache(cacheKey, result, config.cache.nodeCache.stdTTL, 'element280');
    if (process.env.DEBUG === 'true') {
      log(`[Element280-Validate-Burned] [DEBUG] Found ${burnedTokenIds.length} burned tokens in transaction: ${transactionHash}`);
    }
    return NextResponse.json(result);
  } catch (error) {
    log(`[Element280-Validate-Burned] [ERROR] Error processing transaction: ${error.message}, stack: ${error.stack}`);
    return NextResponse.json({ error: 'Failed to validate transaction', details: error.message }, { status: 500 });
  }
}import { NextResponse } from 'next/server';
import { logger } from '@/lib/logger';

console.log('[Debug Route] Importing logger');
logger.info('debug', 'Debug route module loaded').catch(console.error);

export async function GET() {
  await logger.info('debug', 'Debug endpoint called');
  return NextResponse.json({
    message: 'Debug endpoint triggered',
    debug: process.env.DEBUG,
    nodeEnv: process.env.NODE_ENV,
  });
}// File: app/api/_init.js
import '@/lib/serverInit';// File: app/api/init/route.js
import { NextResponse } from 'next/server';
import { logger } from '@/lib/logger';
import { initializeCache } from '@/app/api/utils';
import chalk from 'chalk';

console.log(chalk.cyan('[Init Route] Importing logger and utils'));
logger.info('init', 'Init route module loaded', 'eth', 'general').catch(console.error);

export async function GET() {
  await logger.info('init', 'Init endpoint called', 'eth', 'general');
  await initializeCache();
  return NextResponse.json({
    message: 'Initialization triggered',
    debug: process.env.DEBUG,
    nodeEnv: process.env.NODE_ENV,
  });
}// File: app/store.js

'use client';
import { create } from 'zustand';

const CACHE_TTL = 30 * 60 * 1000; // 30 minutes

export const useNFTStore = create((set, get) => ({
  cache: {},
  setCache: (contractKey, data) => {
    const key = `nft:${contractKey}`;
    console.log(`[NFTStore] Setting cache for ${key}: ${data.holders?.length || 0} holders`);
    set((state) => ({
      cache: {
        ...state.cache,
        [key]: { data, timestamp: Date.now() },
      },
    }));
  },
  getCache: (contractKey) => {
    const key = `nft:${contractKey}`;
    console.log(`[NFTStore] Getting cache for ${key}`);
    const cachedEntry = get().cache[key];
    if (!cachedEntry) {
      console.log(`[NFTStore] Cache miss for ${key}`);
      return null;
    }
    const now = Date.now();
    if (now - cachedEntry.timestamp > CACHE_TTL) {
      console.log(`[NFTStore] Cache expired for ${key}`);
      set((state) => {
        const newCache = { ...state.cache };
        delete newCache[key];
        return { cache: newCache };
      });
      return null;
    }
    console.log(`[NFTStore] Cache hit for ${key}: ${cachedEntry.data.holders?.length || 0} holders`);
    return cachedEntry.data;
  },
}));// config.js
import element280MainAbi from './abi/element280.json' assert { type: 'json' };
import element280VaultAbi from './abi/element280Vault.json' assert { type: 'json' };
import element369MainAbi from './abi/element369.json' assert { type: 'json' };
import element369VaultAbi from './abi/element369Vault.json' assert { type: 'json' };
import staxMainAbi from './abi/staxNFT.json' assert { type: 'json' };
import staxVaultAbi from './abi/staxVault.json' assert { type: 'json' };
import ascendantMainAbi from './abi/ascendantNFT.json' assert { type: 'json' };
// E280 ABI placeholder (not deployed)
const e280MainAbi = [];

const config = {
  // Supported blockchain networks
  supportedChains: ['ETH', 'BASE'],

  // ABIs for all collections
  abis: {
    element280: {
      main: element280MainAbi,
      vault: element280VaultAbi,
    },
    element369: {
      main: element369MainAbi,
      vault: element369VaultAbi,
    },
    stax: {
      main: staxMainAbi,
      vault: staxVaultAbi,
    },
    ascendant: {
      main: ascendantMainAbi,
      vault: [], // No vault ABI provided for Ascendant
    },
    e280: {
      main: e280MainAbi,
      vault: [],
    },
  },

  // NFT contract configurations
  nftContracts: {
    element280: {
      name: 'Element 280',
      symbol: 'ELMNT',
      chain: 'ETH',
      address: '0x7F090d101936008a26Bf1F0a22a5f92fC0Cf46c9',
      vaultAddress: '0x44c4ADAc7d88f85d3D33A7f856Ebc54E60C31E97',
      deploymentBlock: '20945304',
      tiers: {
        1: { name: 'Common', multiplier: 10, allocation: '100000000000000000000000000' },
        2: { name: 'Common Amped', multiplier: 12, allocation: '100000000000000000000000000' },
        3: { name: 'Rare', multiplier: 100, allocation: '1000000000000000000000000000' },
        4: { name: 'Rare Amped', multiplier: 120, allocation: '1000000000000000000000000000' },
        5: { name: 'Legendary', multiplier: 1000, allocation: '10000000000000000000000000000' },
        6: { name: 'Legendary Amped', multiplier: 1200, allocation: '10000000000000000000000000000' },
      },
      description:
        'Element 280 NFTs can be minted with TitanX or ETH during a presale and redeemed for Element 280 tokens after a cooldown period. Multipliers contribute to a pool used for reward calculations.',
      expectedTotalSupply: 8107,
      expectedBurned: 8776,
      maxTokensPerOwnerQuery: 100,
    },
    element369: {
      name: 'Element 369',
      symbol: 'E369',
      chain: 'ETH',
      address: '0x024D64E2F65747d8bB02dFb852702D588A062575',
      vaultAddress: '0x4e3DBD6333e649AF13C823DAAcDd14f8507ECBc5',
      deploymentBlock: '21224418',
      tiers: {
        1: { name: 'Common', multiplier: 1, price: '100000000000000000000000000' },
        2: { name: 'Rare', multiplier: 10, price: '1000000000000000000000000000' },
        3: { name: 'Legendary', multiplier: 100, price: '10000000000000000000000000000' },
      },
      description:
        'Element 369 NFTs are minted with TitanX or ETH during specific sale cycles. Burning NFTs updates a multiplier pool and tracks burn cycles for reward distribution in the Holder Vault.',
    },
    stax: {
      name: 'Stax',
      symbol: 'STAX',
      chain: 'ETH',
      address: '0x74270Ca3a274B4dbf26be319A55188690CACE6E1',
      vaultAddress: '0x5D27813C32dD705404d1A78c9444dAb523331717',
      deploymentBlock: '21452667',
      tiers: {
        1: { name: 'Common', multiplier: 1, price: '100000000000000000000000000' },
        2: { name: 'Common Amped', multiplier: 1.2, price: '100000000000000000000000000', amplifier: '10000000000000000000000000' },
        3: { name: 'Common Super', multiplier: 1.4, price: '100000000000000000000000000', amplifier: '20000000000000000000000000' },
        4: { name: 'Common LFG', multiplier: 2, price: '100000000000000000000000000', amplifier: '50000000000000000000000000' },
        5: { name: 'Rare', multiplier: 10, price: '1000000000000000000000000000' },
        6: { name: 'Rare Amped', multiplier: 12, price: '1000000000000000000000000000', amplifier: '100000000000000000000000000' },
        7: { name: 'Rare Super', multiplier: 14, price: '1000000000000000000000000000', amplifier: '200000000000000000000000000' },
        8: { name: 'Rare LFG', multiplier: 20, price: '1000000000000000000000000000', amplifier: '500000000000000000000000000' },
        9: { name: 'Legendary', multiplier: 100, price: '10000000000000000000000000000' },
        10: { name: 'Legendary Amped', multiplier: 120, price: '10000000000000000000000000000', amplifier: '1000000000000000000000000000' },
        11: { name: 'Legendary Super', multiplier: 140, price: '10000000000000000000000000000', amplifier: '2000000000000000000000000000' },
        12: { name: 'Legendary LFG', multiplier: 200, price: '10000000000000000000000000000', amplifier: '5000000000000000000000000000' },
      },
      description:
        'Stax NFTs are minted with TitanX or ETH during a presale. Burning NFTs after a cooldown period claims backing rewards, with multipliers contributing to a pool for cycle-based reward calculations.',
    },
    ascendant: {
      name: 'Ascendant',
      symbol: 'ASCNFT',
      chain: 'ETH',
      address: '0x9da95c32c5869c84ba2c020b5e87329ec0adc97f',
      deploymentBlock: '21112535',
      tiers: {
        1: { name: 'Tier 1', price: '7812500000000000000000', multiplier: 1.01 },
        2: { name: 'Tier 2', price: '15625000000000000000000', multiplier: 1.02 },
        3: { name: 'Tier 3', price: '31250000000000000000000', multiplier: 1.03 },
        4: { name: 'Tier 4', price: '62500000000000000000000', multiplier: 1.04 },
        5: { name: 'Tier 5', price: '125000000000000000000000', multiplier: 1.05 },
        6: { name: 'Tier 6', price: '250000000000000000000000', multiplier: 1.06 },
        7: { name: 'Tier 7', price: '500000000000000000000000', multiplier: 1.07 },
        8: { name: 'Tier 8', price: '1000000000000000000000000', multiplier: 1.08 },
      },
      description:
        'Ascendant NFTs are minted with ASCENDANT tokens and offer staking rewards from DragonX pools over 8, 28, and 90-day periods. Features fusion mechanics to combine same-tier NFTs into higher tiers.',
    },
    e280: {
      name: 'E280',
      symbol: 'E280',
      chain: 'BASE',
      address: null,
      deploymentBlock: null,
      tiers: {},
      description: 'E280 NFTs on BASE chain. Contract not yet deployed.',
      disabled: true,
    },
  },

  // Contract addresses
  contractAddresses: {
    element280: { chain: 'ETH', address: '0x7F090d101936008a26Bf1F0a22a5f92fC0Cf46c9' },
    element369: { chain: 'ETH', address: '0x024D64E2F65747d8bB02dFb852702D588A062575' },
    stax: { chain: 'ETH', address: '0x74270Ca3a274B4dbf26be319A55188690CACE6E1' },
    ascendant: { chain: 'ETH', address: '0x9da95c32c5869c84ba2c020b5e87329ec0adc97f' },
    e280: { chain: 'BASE', address: null },
  },

  // Vault addresses
  vaultAddresses: {
    element280: { chain: 'ETH', address: '0x44c4ADAc7d88f85d3D33A7f856Ebc54E60C31E97' },
    element369: { chain: 'ETH', address: '0x4e3DBD6333e649AF13C823DAAcDd14f8507ECBc5' },
    stax: { chain: 'ETH', address: '0x5D27813C32dD705404d1A78c9444dAb523331717' },
    e280: { chain: 'BASE', address: null },
  },

  // Deployment blocks
  deploymentBlocks: {
    element280: { chain: 'ETH', block: '20945304' },
    element369: { chain: 'ETH', block: '21224418' },
    stax: { chain: 'ETH', block: '21452667' },
    ascendant: { chain: 'ETH', block: '21112535' },
    e280: { chain: 'BASE', block: null },
  },

  // Contract tiers
  contractTiers: {
    element280: {
      1: { name: 'Common', multiplier: 10 },
      2: { name: 'Common Amped', multiplier: 12 },
      3: { name: 'Rare', multiplier: 100 },
      4: { name: 'Rare Amped', multiplier: 120 },
      5: { name: 'Legendary', multiplier: 1000 },
      6: { name: 'Legendary Amped', multiplier: 1200 },
    },
    element369: {
      1: { name: 'Common', multiplier: 1 },
      2: { name: 'Rare', multiplier: 10 },
      3: { name: 'Legendary', multiplier: 100 },
      tierOrder: [
        { tierId: '3', name: 'Legendary' },
        { tierId: '2', name: 'Rare' },
        { tierId: '1', name: 'Common' },
      ],
    },
    stax: {
      1: { name: 'Common', multiplier: 1 },
      2: { name: 'Common Amped', multiplier: 1.2 },
      3: { name: 'Common Super', multiplier: 1.4 },
      4: { name: 'Common LFG', multiplier: 2 },
      5: { name: 'Rare', multiplier: 10 },
      6: { name: 'Rare Amped', multiplier: 12 },
      7: { name: 'Rare Super', multiplier: 14 },
      8: { name: 'Rare LFG', multiplier: 20 },
      9: { name: 'Legendary', multiplier: 100 },
      10: { name: 'Legendary Amped', multiplier: 120 },
      11: { name: 'Legendary Super', multiplier: 140 },
      12: { name: 'Legendary LFG', multiplier: 200 },
    },
    ascendant: {
      1: { name: 'Tier 1', multiplier: 1.01 },
      2: { name: 'Tier 2', multiplier: 1.02 },
      3: { name: 'Tier 3', multiplier: 1.03 },
      4: { name: 'Tier 4', multiplier: 1.04 },
      5: { name: 'Tier 5', multiplier: 1.05 },
      6: { name: 'Tier 6', multiplier: 1.06 },
      7: { name: 'Tier 7', multiplier: 1.07 },
      8: { name: 'Tier 8', multiplier: 1.08 },
    },
    e280: {},
  },

  // Contract details
  contractDetails: {
    element280: {
      name: 'Element 280',
      chain: 'ETH',
      pageSize: 100,
      apiEndpoint: '/api/holders/Element280',
      rewardToken: 'ELMNT',
    },
    element369: {
      name: 'Element 369',
      chain: 'ETH',
      pageSize: 1000,
      apiEndpoint: '/api/holders/Element369',
      rewardToken: 'INFERNO/FLUX/E280',
    },
    stax: {
      name: 'Stax',
      chain: 'ETH',
      pageSize: 1000,
      apiEndpoint: '/api/holders/Stax',
      rewardToken: 'X28',
    },
    ascendant: {
      name: 'Ascendant',
      chain: 'ETH',
      pageSize: 1000,
      apiEndpoint: '/api/holders/Ascendant',
      rewardToken: 'DRAGONX',
    },
    e280: {
      name: 'E280',
      chain: 'BASE',
      pageSize: 1000,
      apiEndpoint: '/api/holders/E280',
      rewardToken: 'E280',
      disabled: true,
    },
  },

  // Utility function to get contract details by name
  getContractDetails: (contractName) => {
    return config.nftContracts[contractName] || null;
  },

  alchemy: {
    apiKey: process.env.ALCHEMY_API_KEY || process.env.NEXT_PUBLIC_ALCHEMY_API_KEY,
    network: 'eth-mainnet',
    batchSize: 50, // Increased for larger batches
    batchDelayMs: 500, // Reduced for speed
    retryMaxDelayMs: 10000, // Reduced to fail faster
    maxRetries: 2, // Reduced to minimize retry overhead
    timeoutMs: 30000,
  },
  
  // Cache settings
  cache: {
    redis: {
      disableElement280: process.env.DISABLE_ELEMENT280_REDIS === 'true',
      disableElement369: process.env.DISABLE_ELEMENT369_REDIS === 'true',
      disableStax: process.env.DISABLE_STAX_REDIS === 'true',
      disableAscendant: process.env.DISABLE_ASCENDANT_REDIS === 'true',
      disableE280: process.env.DISABLE_E280_REDIS === 'true' || true,
    },
    nodeCache: {
      stdTTL: 3600,
      checkperiod: 120,
    },
  },

  // Debug settings
  debug: {
    enabled: process.env.DEBUG === 'true',
    logLevel: 'debug',
  },

  // Fallback data (optional, for testing)
  fallbackData: {
    element280: process.env.USE_FALLBACK_DATA === 'true' ? element280NftStatus : null,
  },
};

export default config;// File: app/api/utils.js
import NodeCache from 'node-cache';
import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';
import { Redis } from '@upstash/redis';
import { createPublicClient, http } from 'viem';
import { mainnet } from 'viem/chains';
import { Alchemy } from 'alchemy-sdk';
import config from '@/config';
import pLimit from 'p-limit';
import { logger } from '@/lib/logger';
import chalk from 'chalk';

console.log(chalk.cyan('[Utils] Initializing utils...'));
logger.info('utils', 'Utils module loaded', 'eth', 'general').catch(error => {
  console.error(chalk.red('[Utils] Logger error:'), error.message);
});

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const isDebug = process.env.DEBUG === 'true';
const isProduction = process.env.NODE_ENV === 'production';

const cache = new NodeCache({
  stdTTL: 0,
  checkperiod: 120,
});

const cacheDir = path.join(process.cwd(), 'cache');

const redisEnabled = Object.keys(config.nftContracts).some(
  contract => process.env[`DISABLE_${contract.toUpperCase()}_REDIS`] !== 'true' && process.env.UPSTASH_REDIS_REST_URL && process.env.UPSTASH_REDIS_REST_TOKEN
);
let redis = null;

if (redisEnabled) {
  try {
    redis = new Redis({
      url: process.env.UPSTASH_REDIS_REST_URL,
      token: process.env.UPSTASH_REDIS_REST_TOKEN,
    });
    logger.info('utils', 'Upstash Redis initialized', 'eth', 'general');
  } catch (error) {
    logger.error('utils', `Failed to initialize Upstash Redis: ${error.message}`, { stack: error.stack }, 'eth', 'general');
    redis = null;
  }
}

const alchemyApiKey = config.alchemy.apiKey || process.env.NEXT_PUBLIC_ALCHEMY_API_KEY;
if (!alchemyApiKey) {
  logger.error('utils', 'Alchemy API key is missing', {}, 'eth', 'general');
  throw new Error('Alchemy API key is missing');
}

const client = createPublicClient({
  chain: mainnet,
  transport: http(`https://eth-mainnet.g.alchemy.com/v2/${alchemyApiKey}`),
});

const alchemy = new Alchemy({
  apiKey: config.alchemy.apiKey,
  network: 'eth-mainnet',
});

async function ensureCacheDir() {
  try {
    await fs.mkdir(cacheDir, { recursive: true });
    await fs.chmod(cacheDir, 0o755);
    logger.info('utils', `Created/chmod cache directory: ${cacheDir}`, 'eth', 'general');
  } catch (error) {
    logger.error('utils', `Failed to create/chmod cache directory ${cacheDir}: ${error.message}`, { stack: error.stack }, 'eth', 'general');
    throw error;
  }
}

async function initializeCache() {
  try {
    logger.info('utils', 'Starting cache initialization', 'eth', 'general');
    await ensureCacheDir();

    // Check node-cache
    const testKey = 'test_node_cache';
    const testValue = { ready: true };
    const nodeCacheSuccess = cache.set(testKey, testValue);
    if (nodeCacheSuccess) {
      logger.info('utils', 'Node-cache is ready', 'eth', 'general');
      cache.del(testKey);
    } else {
      logger.error('utils', 'Node-cache failed to set test key', {}, 'eth', 'general');
    }

    // Check Redis
    if (redisEnabled && redis) {
      try {
        await redis.set('test_redis', JSON.stringify(testValue));
        const redisData = await redis.get('test_redis');
        if (redisData && JSON.parse(redisData).ready) {
          logger.info('utils', 'Redis cache is ready', 'eth', 'general');
          await redis.del('test_redis');
        } else {
          logger.error('utils', 'Redis cache test failed: invalid data', {}, 'eth', 'general');
        }
      } catch (error) {
        logger.error('utils', `Redis cache test failed: ${error.message}`, { stack: error.stack }, 'eth', 'general');
      }
    }

    // Create empty cache files for each collection
    const collections = Object.keys(config.nftContracts).filter(key => !config.nftContracts[key].disabled).map(key => key.toLowerCase());
    for (const collection of collections) {
      const cacheFile = path.join(cacheDir, `${collection}_holders.json`);
      try {
        await fs.access(cacheFile);
        logger.info('utils', `Cache file exists: ${cacheFile}`, 'eth', collection);
      } catch (error) {
        if (error.code === 'ENOENT') {
          await fs.writeFile(cacheFile, JSON.stringify({ holders: [], totalBurned: 0, timestamp: Date.now() }));
          await fs.chmod(cacheFile, 0o644);
          logger.info('utils', `Created empty cache file: ${cacheFile}`, 'eth', collection);
        } else {
          logger.error('utils', `Failed to access cache file ${cacheFile}: ${error.message}`, { stack: error.stack }, 'eth', collection);
        }
      }
    }

    logger.info('utils', 'Cache initialization completed', 'eth', 'general');
    return true;
  } catch (error) {
    logger.error('utils', `Cache initialization error: ${error.message}`, { stack: error.stack }, 'eth', 'general');
    return false;
  }
}

async function retry(operation, { retries, delay = 1000 }) {
  let lastError;
  for (let attempt = 1; attempt <= retries; attempt++) {
    try {
      return await operation();
    } catch (error) {
      lastError = error;
      if (error.message.includes('429') && attempt === retries) {
        logger.error('utils', `Circuit breaker: Rate limit exceeded after ${retries} attempts`, {}, 'eth', 'general');
        throw new Error('Rate limit exceeded');
      }
      logger.warn('utils', `Retry attempt ${attempt}/${retries} failed: ${error.message}`, 'eth', 'general');
      await new Promise(resolve => setTimeout(resolve, delay * Math.min(attempt, 3)));
    }
  }
  throw lastError;
}

async function batchMulticall(calls, batchSize = config.alchemy.batchSize || 10) {
  const results = [];
  const delay = async () => new Promise(resolve => setTimeout(resolve, config.alchemy.batchDelayMs || 500));

  const concurrencyLimit = pLimit(3);
  const batchPromises = [];
  for (let i = 0; i < calls.length; i += batchSize) {
    const batch = calls.slice(i, i + batchSize);
    batchPromises.push(
      concurrencyLimit(async () => {
        try {
          await delay();
          const batchResults = await client.multicall({
            contracts: batch.map(call => ({
              address: call.address,
              abi: call.abi,
              functionName: call.functionName,
              args: call.args || [],
            })),
            allowFailure: true,
          });

          const batchResult = batchResults.map((result, index) => ({
            status: result.status === 'success' ? 'success' : 'failure',
            result: result.status === 'success' ? result.result : null,
            error: result.status === 'failure' ? result.error?.message || 'Unknown error' : null,
          }));
          return batchResult;
        } catch (error) {
          logger.error('utils', `Batch multicall failed: ${error.message}`, { stack: error.stack }, 'eth', 'general');
          return batch.map(() => ({
            status: 'failure',
            result: null,
            error: error.message,
          }));
        }
      })
    );
  }

  const batchResults = (await Promise.all(batchPromises)).flat();
  results.push(...batchResults);
  return results;
}

async function getOwnersForContract(contractAddress, abi, options = {}) {
  try {
    logger.debug('utils', `Fetching owners for contract: ${contractAddress} with options: ${JSON.stringify(options)}`, 'eth', 'general');
    const response = await alchemy.nft.getOwnersForContract(contractAddress, {
      withTokenBalances: options.withTokenBalances || false,
      pageKey: options.pageKey || null,
    });
    logger.debug('utils', `Raw Alchemy response: ownersExists=${!!response.owners}, isArray=${Array.isArray(response.owners)}, ownersLength=${response.owners?.length || 0}, pageKey=${response.pageKey || null}, responseKeys=${Object.keys(response || {})}, sampleOwners=${JSON.stringify(response.owners?.slice(0, 2) || [])}`, 'eth', 'general');
    logger.debug('utils', `Token balances: exists=${!!response.tokenBalances}, length=${response.tokenBalances?.length || 0}, sample=${JSON.stringify(response.tokenBalances?.slice(0, 2) || [])}`, 'eth', 'general');
    if (!response.owners || !Array.isArray(response.owners)) {
      logger.error('utils', `Invalid Alchemy response for ${contractAddress}: ${JSON.stringify(response)}`, {}, 'eth', 'general');
      throw new Error('Invalid owners response from Alchemy API');
    }
    const owners = response.owners.flatMap(owner => {
      const tokenBalances = response.tokenBalances?.find(tb => tb.ownerAddress.toLowerCase() === owner.toLowerCase()) || { tokenBalances: [] };
      logger.debug('utils', `Processing owner: ${owner}, tokenBalancesCount=${tokenBalances.tokenBalances?.length || 0}`, 'eth', 'general');
      return (tokenBalances.tokenBalances || []).map(token => ({
        ownerAddress: owner.toLowerCase(),
        tokenId: Number(token.tokenId),
      }));
    });
    logger.debug('utils', `Processed owners: count=${owners.length}, sample=${JSON.stringify(owners.slice(0, 2))}`, 'eth', 'general');
    logger.info('utils', `Fetched ${owners.length} owners for contract: ${contractAddress}`, 'eth', 'general');
    return owners;
  } catch (error) {
    logger.error('utils', `Failed to fetch owners for ${contractAddress}: ${error.message}`, { stack: error.stack }, 'eth', 'general');
    throw error;
  }
}

async function setCache(key, value, ttl, prefix) {
  try {
    const cacheKey = `${prefix}_${key}`;
    const success = cache.set(cacheKey, value);
    logger.info('utils', `Set in-memory cache: ${cacheKey}, success: ${success}, holders: ${value.holders?.length || 'unknown'}`, 'eth', prefix.toLowerCase());

    if (key === `${prefix.toLowerCase()}_holders` && Object.keys(config.nftContracts).map(k => k.toLowerCase()).includes(prefix.toLowerCase())) {
      if (redisEnabled && redis && process.env[`DISABLE_${prefix.toUpperCase()}_REDIS`] !== 'true') {
        try {
          await redis.set(cacheKey, JSON.stringify(value));
          logger.info('utils', `Persisted ${cacheKey} to Redis, holders: ${value.holders.length}`, 'eth', prefix.toLowerCase());
        } catch (error) {
          logger.error('utils', `Failed to persist ${cacheKey} to Redis: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
        }
      } else {
        const cacheFile = path.join(cacheDir, `${prefix.toLowerCase()}_holders.json`);
        logger.info('utils', `Writing to cache file: ${cacheFile}`, 'eth', prefix.toLowerCase());
        await ensureCacheDir();
        try {
          await fs.writeFile(cacheFile, JSON.stringify(value));
          await fs.chmod(cacheFile, 0o644);
          logger.info('utils', `Persisted ${cacheKey} to ${cacheFile}, holders: ${value.holders.length}`, 'eth', prefix.toLowerCase());
        } catch (error) {
          logger.error('utils', `Failed to write cache file ${cacheFile}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
          throw error;
        }
      }
    }
    return success;
  } catch (error) {
    logger.error('utils', `Failed to set cache ${prefix}_${key}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
    return false;
  }
}

async function getCache(key, prefix) {
  try {
    const cacheKey = `${prefix}_${key}`;
    let data = cache.get(cacheKey);
    if (data !== undefined) {
      logger.debug('utils', `Cache hit: ${cacheKey}, holders: ${data.holders?.length || 'unknown'}`, 'eth', prefix.toLowerCase());
      return data;
    }

    if (key === `${prefix.toLowerCase()}_holders` && Object.keys(config.nftContracts).map(k => k.toLowerCase()).includes(prefix.toLowerCase())) {
      if (redisEnabled && redis && process.env[`DISABLE_${prefix.toUpperCase()}_REDIS`] !== 'true') {
        try {
          const redisData = await redis.get(cacheKey);
          if (redisData) {
            const parsed = typeof redisData === 'string' ? JSON.parse(redisData) : redisData;
            if (parsed && Array.isArray(parsed.holders) && Number.isInteger(parsed.totalBurned)) {
              const success = cache.set(cacheKey, parsed);
              logger.info('utils', `Loaded ${cacheKey} from Redis, cached: ${success}, holders: ${parsed.holders.length}`, 'eth', prefix.toLowerCase());
              return parsed;
            } else {
              logger.warn('utils', `Invalid data in Redis for ${cacheKey}`, 'eth', prefix.toLowerCase());
            }
          }
        } catch (error) {
          logger.error('utils', `Failed to load cache from Redis for ${cacheKey}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
        }
      }

      const cacheFile = path.join(cacheDir, `${prefix.toLowerCase()}_holders.json`);
      try {
        const fileData = await fs.readFile(cacheFile, 'utf8');
        const parsed = JSON.parse(fileData);
        if (parsed && Array.isArray(parsed.holders) && Number.isInteger(parsed.totalBurned)) {
          const success = cache.set(cacheKey, parsed);
          logger.info('utils', `Loaded ${cacheKey} from ${cacheFile}, cached: ${success}, holders: ${parsed.holders.length}`, 'eth', prefix.toLowerCase());
          return parsed;
        } else {
          logger.warn('utils', `Invalid data in ${cacheFile}`, 'eth', prefix.toLowerCase());
        }
      } catch (error) {
        if (error.code !== 'ENOENT') {
          logger.error('utils', `Failed to load cache from ${cacheFile}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
        } else {
          logger.debug('utils', `No cache file at ${cacheFile}`, 'eth', prefix.toLowerCase());
        }
      }
    }

    logger.info('utils', `Cache miss: ${cacheKey}`, 'eth', prefix.toLowerCase());
    return null;
  } catch (error) {
    logger.error('utils', `Failed to get cache ${prefix}_${key}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
    return null;
  }
}

async function saveCacheState(collection, state, prefix) {
  try {
    const cacheFile = path.join(cacheDir, `cache_state_${prefix.toLowerCase()}.json`);
    await ensureCacheDir();
    await fs.writeFile(cacheFile, JSON.stringify(state));
    await fs.chmod(cacheFile, 0o644);
    logger.debug('utils', `Saved cache state for ${prefix}: ${cacheFile}`, 'eth', prefix.toLowerCase());
  } catch (error) {
    logger.error('utils', `Failed to save cache state for ${prefix}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
  }
}

async function loadCacheState(collection, prefix) {
  try {
    const cacheFile = path.join(cacheDir, `cache_state_${prefix.toLowerCase()}.json`);
    const data = await fs.readFile(cacheFile, 'utf8');
    const parsed = JSON.parse(data);
    logger.debug('utils', `Loaded cache state for ${prefix}: ${cacheFile}`, 'eth', prefix.toLowerCase());
    return parsed;
  } catch (error) {
    if (error.code === 'ENOENT') {
      logger.debug('utils', `No cache state found for ${prefix}`, 'eth', prefix.toLowerCase());
      return null;
    }
    logger.error('utils', `Failed to load cache state for ${prefix}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
    return null;
  }
}

async function getTransactionReceipt(transactionHash) {
  try {
    const receipt = await client.getTransactionReceipt({ hash: transactionHash });
    logger.debug('utils', `Fetched transaction receipt for ${transactionHash}`, 'eth', 'general');
    return receipt;
  } catch (error) {
    logger.error('utils', `Failed to fetch transaction receipt for ${transactionHash}: ${error.message}`, { stack: error.stack }, 'eth', 'general');
    return null;
  }
}

async function log(scope, message, chain = 'eth', collection = 'general') {
  await logger.info(scope, message, chain, collection);
}

export { client, retry, logger, getCache, setCache, saveCacheState, loadCacheState, batchMulticall, getOwnersForContract, getTransactionReceipt, log, initializeCache };export const barChartOptions = {
    responsive: true,
    plugins: {
      legend: { position: 'top', labels: { color: '#e5e7eb' } }, // Gray-200
      title: {
        display: true,
        text: 'NFT Tier Distribution',
        color: '#e5e7eb',
        font: { size: 16, weight: 'bold' },
      },
    },
    scales: {
      y: {
        beginAtZero: true,
        title: { display: true, text: 'Number of NFTs', color: '#e5e7eb' },
        ticks: { color: '#d1d5db' }, // Gray-300
      },
      x: {
        title: { display: true, text: 'Tiers', color: '#e5e7eb' },
        ticks: { color: '#d1d5db' },
      },
    },
  };import fs from 'fs/promises';
import path from 'path';
import chalk from 'chalk';

// Use process.cwd() to reference the project root
const logDir = path.join(process.cwd(), 'logs');

console.log(chalk.cyan('[Logger] Initializing logger...'));
console.log(chalk.cyan('[Logger] process.env.DEBUG:'), process.env.DEBUG);
console.log(chalk.cyan('[Logger] process.env.NODE_ENV:'), process.env.NODE_ENV);
console.log(chalk.cyan('[Logger] Log directory:'), logDir);

const isDebug = process.env.DEBUG === 'true';
console.log(chalk.cyan('[Logger] isDebug:'), isDebug);

async function ensureLogDir() {
  try {
    await fs.mkdir(logDir, { recursive: true });
    await fs.chmod(logDir, 0o755);
    console.log(chalk.cyan('[Logger] Created or verified log directory:'), logDir);
  } catch (error) {
    console.error(chalk.red('[Logger] Failed to create log directory:'), error.message);
  }
}

ensureLogDir().catch(error => {
  console.error(chalk.red('[Logger] ensureLogDir error:'), error.message);
});

export const logger = {
  info: async (scope, message, chain = 'eth', collection = 'general') => {
    const timestamp = new Date().toISOString();
    const log = `[${timestamp}] [${scope}] [INFO] ${message}`;
    console.log(chalk.green(log));
    if (isDebug) {
      try {
        const logFile = path.join(logDir, `cache-${chain}-${collection.toLowerCase()}-${timestamp.split('T')[0]}.log`);
        await fs.appendFile(logFile, `${log}\n`);
        console.log(chalk.cyan('[Logger] Wrote INFO log to:'), logFile);
      } catch (error) {
        console.error(chalk.red('[Logger] Failed to write INFO log:'), error.message);
      }
    }
  },
  error: async (scope, message, details = {}, chain = 'eth', collection = 'general') => {
    const timestamp = new Date().toISOString();
    const log = `[${timestamp}] [${scope}] [ERROR] ${message} ${JSON.stringify(details)}`;
    console.error(chalk.red(log));
    if (isDebug) {
      try {
        const logFile = path.join(logDir, `cache-${chain}-${collection.toLowerCase()}-${timestamp.split('T')[0]}.log`);
        await fs.appendFile(logFile, `${log}\n`);
        console.log(chalk.cyan('[Logger] Wrote ERROR log to:'), logFile);
      } catch (error) {
        console.error(chalk.red('[Logger] Failed to write ERROR log:'), error.message);
      }
    }
  },
  debug: async (scope, message, chain = 'eth', collection = 'general') => {
    if (!isDebug) return;
    const timestamp = new Date().toISOString();
    const log = `[${timestamp}] [${scope}] [DEBUG] ${message}`;
    console.log(chalk.blue(log));
    try {
      const logFile = path.join(logDir, `cache-${chain}-${collection.toLowerCase()}-${timestamp.split('T')[0]}.log`);
      await fs.appendFile(logFile, `${log}\n`);
      console.log(chalk.cyan('[Logger] Wrote DEBUG log to:'), logFile);
    } catch (error) {
      console.error(chalk.red('[Logger] Failed to write DEBUG log:'), error.message);
    }
  },
};

try {
  logger.info('startup', 'Logger module loaded').catch(error => {
    console.error(chalk.red('[Logger] Startup log error:'), error.message);
  });
} catch (error) {
  console.error(chalk.red('[Logger] Immediate log error:'), error.message);
}// File: lib/schemas.js
import { z } from 'zod';

export const HoldersResponseSchema = z.object({
  holders: z.array(z.object({
    wallet: z.string(),
    total: z.number().optional(),
    tiers: z.array(z.number()).optional(),
    shares: z.number().optional(),
  })),
  totalTokens: z.number().optional(),
  totalShares: z.number().optional(),
  totalBurned: z.number().optional(),
  summary: z.object({}).optional(),
  totalPages: z.number().optional(),
});// File: lib/serverInit.js
import { logger } from '@/lib/logger';
import { initializeCache } from '@/app/api/utils';
import chalk from 'chalk';

console.log(chalk.cyan('[ServerInit] Initializing server...'));

try {
  logger.info('serverInit', 'Server initialization started');
  await initializeCache();
} catch (error) {
  logger.error('serverInit', `Initialize cache error: ${error.message}`, { stack: error.stack });
  console.error(chalk.red('[ServerInit] Initialization error:'), error.message);
}

export const serverInit = true;// lib/useNFTData.js
'use client';
import { useQuery } from '@tanstack/react-query';
import { useNFTStore } from '@/app/store';
import config from '@/config';
import { HoldersResponseSchema } from '@/lib/schemas';

async function fetchNFTData(apiKey, apiEndpoint, pageSize, page = 0) {
  if (apiKey === 'e280' || config.contractDetails[apiKey]?.disabled) {
    return { holders: [], totalTokens: 0, totalBurned: 0, error: 'Contract not deployed' };
  }

  const endpoint = apiEndpoint.startsWith('http') ? apiEndpoint : `${process.env.NEXT_PUBLIC_API_BASE_URL || 'http://localhost:3000'}${apiEndpoint}`;
  const progressUrl = `${endpoint}/progress`;

  const progressRes = await fetch(progressUrl, { cache: 'no-store' });
  if (!progressRes.ok) throw new Error(`Progress fetch failed: ${progressRes.status}`);
  const progress = await progressRes.json();

  if (progress.isPopulating || progress.phase !== 'Completed') {
    throw new Error('Cache is populating');
  }

  let allHolders = [];
  let totalTokens = 0;
  let totalShares = 0;
  let totalBurned = 0;
  let summary = {};
  let totalPages = Infinity;

  while (page < totalPages) {
    const url = `${endpoint}?page=${page}&pageSize=${pageSize}`;
    const res = await fetch(url, { cache: 'force-cache' });
    if (!res.ok) throw new Error(`API request failed: ${res.status}`);
    const json = await res.json();

    if (json.message === 'Cache is populating' || json.isCachePopulating) {
      throw new Error('Cache is populating');
    }

    const validation = HoldersResponseSchema.safeParse(json);
    if (!validation.success) {
      throw new Error(`Invalid holders schema: ${JSON.stringify(validation.error.errors)}`);
    }

    allHolders = allHolders.concat(json.holders);
    totalTokens = json.totalTokens || json.summary?.totalLive || totalTokens;
    totalShares = json.totalShares || json.summary?.multiplierPool || totalShares;
    totalBurned = json.totalBurned || totalBurned;
    summary = json.summary || summary;
    totalPages = json.totalPages || 1;
    page++;
  }

  return { holders: allHolders, totalTokens, totalShares, totalBurned, summary };
}

export function useNFTData(apiKey, pageSize) {
  const { getCache, setCache } = useNFTStore();

  return useQuery({
    queryKey: ['nft', apiKey],
    queryFn: async () => {
      const cachedData = getCache(apiKey);
      if (cachedData) return cachedData;

      const data = await fetchNFTData(apiKey, config.contractDetails[apiKey].apiEndpoint, pageSize);
      setCache(apiKey, data);
      return data;
    },
    retry: config.alchemy.maxRetries,
    retryDelay: attempt => config.alchemy.batchDelayMs * (attempt + 1),
    staleTime: 30 * 60 * 1000, // 30 minutes
    refetchInterval: progress => (progress?.isPopulating ? 2000 : false),
    onError: error => console.error(`[useNFTData] [ERROR] ${apiKey}: ${error.message}`),
  });
}