// app/api/holders/Ascendant/route.js
import { NextResponse } from 'next/server';
import config from '@/config.js';
import { client, getOwnersForContract, getNftsForOwner, log, batchMulticall, getCache, setCache, safeSerialize, retry } from '@/app/api/utils';
import { formatUnits, getAddress } from 'viem';
import ascendant from '@/abi/ascendantNFT.json';

let cacheState = {
  isPopulating: false,
  totalOwners: 0,
  totalNfts: 0,
  processedNfts: 0,
  step: 'idle',
  debugId: `state-${Math.random().toString(36).slice(2)}`,
};

export async function getCacheState(_address) {
  return {
    isCachePopulating: cacheState.isPopulating,
    totalOwners: cacheState.totalOwners,
    progressState: {
      step: cacheState.step,
      totalNfts: cacheState.totalNfts,
      processedNfts: cacheState.processedNfts,
    },
    debugId: cacheState.debugId,
  };
}

async function getAllHolders(page = 0, pageSize = config.contractDetails.ascendant.pageSize, _requestId = '') {
  const contractAddress = config.contractAddresses.ascendant.address;
  const tiers = config.contractTiers.ascendant;
  const cacheKey = `ascendant_holders_${contractAddress}-${page}-${pageSize}`;

  try {
    let cached;
    if (cacheState.isPopulating) {
      return { message: 'Cache is populating', ...await getCacheState(contractAddress) };
    }
    cached = await getCache(cacheKey);
    if (cached) {
      return cached;
    }
  } catch (cacheError) {
    log(`[Ascendant] [ERROR] Cache read error: ${cacheError.message}`);
  }

  if (!contractAddress || !tiers) {
    log(`[Ascendant] [VALIDATION] Config error: contractAddress=${contractAddress}, tiers=${JSON.stringify(tiers)}`);
    throw new Error('Missing contract address or tiers');
  }

  cacheState = { ...cacheState, isPopulating: true, step: 'fetching_owners', processedNfts: 0, totalNfts: 0, totalOwners: 0 };
  const owners = await retry(() => getOwnersForContract(contractAddress, ascendant.abi));
  cacheState = { ...cacheState, step: 'filtering_owners', totalNfts: owners.length, totalOwners: new Set(owners.map(o => o.ownerAddress.toLowerCase())).size };

  const burnAddress = '0x0000000000000000000000000000000000000000';
  const filteredOwners = owners.filter(
    owner => owner.ownerAddress && owner.ownerAddress.toLowerCase() !== burnAddress
  );

  const tokenOwnerMap = new Map();
  let totalTokens = 0;

  filteredOwners.forEach(owner => {
    if (!owner.ownerAddress) return;
    let wallet;
    try {
      wallet = getAddress(owner.ownerAddress);
    } catch (error) {
      log(`[Ascendant] [ERROR] Invalid wallet address: ${owner.ownerAddress}, Error: ${error.message}`);
      return;
    }
    const tokenId = Number(owner.tokenId);
    tokenOwnerMap.set(tokenId, wallet);
    totalTokens++;
  });
  cacheState = { ...cacheState, step: 'building_token_map' };

  const allTokenIds = Array.from(tokenOwnerMap.keys());
  const start = page * pageSize;
  const end = Math.min(start + pageSize, allTokenIds.length);
  const paginatedTokenIds = allTokenIds.slice(start, end);

  if (paginatedTokenIds.length === 0) {
    const result = {
      holders: [],
      totalTokens,
      totalShares: 0,
      toDistributeDay8: 0,
      toDistributeDay28: 0,
      toDistributeDay90: 0,
      pendingRewards: 0,
      page,
      pageSize,
      totalPages: Math.ceil(totalTokens / pageSize),
    };
    await setCache(cacheKey, result);
    cacheState = { ...cacheState, isPopulating: false, step: 'completed' };
    return result;
  }

  cacheState = { ...cacheState, step: 'fetching_tiers' };
  const tierCalls = paginatedTokenIds.map(tokenId => ({
    address: contractAddress,
    abi: config.abis.ascendant.main,
    functionName: 'getNFTAttribute',
    args: [BigInt(tokenId)],
  }));
  const recordCalls = paginatedTokenIds.map(tokenId => ({
    address: contractAddress,
    abi: config.abis.ascendant.main,
    functionName: 'userRecords',
    args: [BigInt(tokenId)],
  }));

  const [tierResults, recordResults] = await Promise.all([
    retry(() => batchMulticall(tierCalls, config.alchemy.batchSize)),
    retry(() => batchMulticall(recordCalls, config.alchemy.batchSize)),
  ]);

  cacheState = { ...cacheState, step: 'fetching_shares' };
  const totalSharesRaw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: config.abis.ascendant.main,
      functionName: 'totalShares',
    })
  );
  const totalShares = parseFloat(formatUnits(totalSharesRaw.toString(), 18));

  const toDistributeDay8Raw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: config.abis.ascendant.main,
      functionName: 'toDistribute',
      args: [0],
    })
  );
  const toDistributeDay8 = parseFloat(formatUnits(toDistributeDay8Raw.toString(), 18));

  const toDistributeDay28Raw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: config.abis.ascendant.main,
      functionName: 'toDistribute',
      args: [1],
    })
  );
  const toDistributeDay28 = parseFloat(formatUnits(toDistributeDay28Raw.toString(), 18));

  const toDistributeDay90Raw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: config.abis.ascendant.main,
      functionName: 'toDistribute',
      args: [2],
    })
  );
  const toDistributeDay90 = parseFloat(formatUnits(toDistributeDay90Raw.toString(), 18));

  const maxTier = Math.max(...Object.keys(tiers).map(Number));
  const holdersMap = new Map();

  cacheState = { ...cacheState, step: 'processing_holders', processedNfts: paginatedTokenIds.length };
  const walletTokenIds = new Map();
  paginatedTokenIds.forEach(tokenId => {
    const wallet = tokenOwnerMap.get(tokenId);
    if (!wallet) return;
    if (!walletTokenIds.has(wallet)) {
      walletTokenIds.set(wallet, []);
    }
    walletTokenIds.get(wallet).push(tokenId);
  });

  const claimableCalls = Array.from(walletTokenIds.entries()).map(([_wallet, tokenIds]) => ({
    address: contractAddress,
    abi: config.abis.ascendant.main,
    functionName: 'batchClaimableAmount',
    args: [tokenIds.map(id => BigInt(id))],
  }));

  const claimableResults = await retry(() => batchMulticall(claimableCalls, config.alchemy.batchSize));

  paginatedTokenIds.forEach((tokenId, i) => {
    const wallet = tokenOwnerMap.get(tokenId);
    if (!wallet) return;
    if (!holdersMap.has(wallet)) {
      holdersMap.set(wallet, {
        wallet,
        total: 0,
        multiplierSum: 0,
        tiers: Array(maxTier + 1).fill(0),
        shares: 0,
        lockedAscendant: 0,
        pendingDay8: 0,
        pendingDay28: 0,
        pendingDay90: 0,
        claimableRewards: 0,
      });
    }
    const holder = holdersMap.get(wallet);

    const tierResult = tierResults[i];
    let tier;
    if (tierResult?.status === 'success') {
      if (Array.isArray(tierResult.result) && tierResult.result.length >= 2) {
        tier = Number(tierResult.result[1]);
      } else if (typeof tierResult.result === 'object' && tierResult.result.tier !== undefined) {
        tier = Number(tierResult.result.tier);
      } else {
        log(`[Ascendant] [ERROR] Unexpected tier result format for token ${tokenId}`);
      }
    }
    if (tier >= 1 && tier <= maxTier) {
      holder.tiers[tier] += 1;
      holder.total += 1;
      holder.multiplierSum += tiers[tier]?.multiplier || 0;
    }

    const recordResult = recordResults[i];
    if (recordResult?.status === 'success' && Array.isArray(recordResult.result)) {
      const sharesRaw = recordResult.result[0] || '0';
      const lockedAscendantRaw = recordResult.result[1] || '0';
      const shares = parseFloat(formatUnits(sharesRaw, 18));
      const lockedAscendant = parseFloat(formatUnits(lockedAscendantRaw, 18));
      holder.shares += shares;
      holder.lockedAscendant += lockedAscendant;
    }
  });

  let claimableIndex = 0;
  for (const [_wallet, _tokenIds] of walletTokenIds.entries()) {
    const holder = holdersMap.get(_wallet);
    if (!holder) {
      claimableIndex++;
      continue;
    }
    if (claimableResults[claimableIndex]?.status === 'success') {
      const claimableRaw = claimableResults[claimableIndex].result || '0';
      holder.claimableRewards = parseFloat(formatUnits(claimableRaw, 18));
    }
    claimableIndex++;
  }

  const holders = Array.from(holdersMap.values());
  const totalMultiplierSum = holders.reduce((sum, h) => sum + h.multiplierSum, 0);
  const pendingRewardPerShareDay8 = totalShares > 0 ? toDistributeDay8 / totalShares : 0;
  const pendingRewardPerShareDay28 = totalShares > 0 ? toDistributeDay28 / totalShares : 0;
  const pendingRewardPerShareDay90 = totalShares > 0 ? toDistributeDay90 / totalShares : 0;

  holders.forEach(holder => {
    holder.pendingDay8 = holder.shares * pendingRewardPerShareDay8;
    holder.pendingDay28 = holder.shares * pendingRewardPerShareDay28;
    holder.pendingDay90 = holder.shares * pendingRewardPerShareDay90;
    holder.percentage = totalMultiplierSum > 0 ? (holder.multiplierSum / totalMultiplierSum) * 100 : 0;
    holder.rank = 0;
    holder.displayMultiplierSum = holder.multiplierSum;
  });

  holders.sort((a, b) => b.shares - a.shares || b.multiplierSum - a.shares || b.total - a.total);
  holders.forEach((holder, index) => (holder.rank = index + 1));

  const result = {
    holders,
    totalTokens,
    totalShares,
    toDistributeDay8,
    toDistributeDay28,
    toDistributeDay90,
    pendingRewards: toDistributeDay8 + toDistributeDay28 + toDistributeDay90,
    page,
    pageSize,
    totalPages: Math.ceil(totalTokens / pageSize),
  };

  await setCache(cacheKey, result);
  cacheState = { ...cacheState, isPopulating: false, step: 'completed' };
  return result;
}

// ... (getHolderData and GET handler remain unchanged)

async function getHolderData(_wallet, _requestId = '') {
  const contractAddress = config.contractAddresses.ascendant.address;
  const tiers = config.contractTiers.ascendant;
  const cacheKey = `ascendant_holder_${contractAddress}-${_wallet.toLowerCase()}`;

  try {
    let cached;
    if (cacheState.isPopulating) {
      return { message: 'Cache is populating', ...await getCacheState(contractAddress) };
    }
    cached = await getCache(cacheKey);
    if (cached) {
      return cached;
    }
  } catch (cacheError) {
    log(`[Ascendant] [ERROR] Cache read error: ${cacheError.message}`);
  }

  if (!contractAddress || !tiers) {
    log(`[Ascendant] [VALIDATION] Config error: contractAddress=${contractAddress}, tiers=${JSON.stringify(tiers)}`);
    throw new Error('Missing contract address or tiers');
  }

  cacheState = { ...cacheState, isPopulating: true, step: 'fetching_nfts', processedNfts: 0, totalNfts: 0, totalOwners: 0 };
  const nfts = await retry(() => getNftsForOwner(_wallet.toLowerCase(), contractAddress, ascendant.abi));
  if (nfts.length === 0) {
    cacheState = { ...cacheState, isPopulating: false, step: 'completed' };
    return null;
  }
  cacheState = { ...cacheState, step: 'processing_nfts', totalNfts: nfts.length, totalOwners: 1 };

  const maxTier = Math.max(...Object.keys(tiers).map(Number));
  const holder = {
    wallet: _wallet.toLowerCase(),
    total: 0,
    multiplierSum: 0,
    tiers: Array(maxTier + 1).fill(0),
    shares: 0,
    lockedAscendant: 0,
    pendingDay8: 0,
    pendingDay28: 0,
    pendingDay90: 0,
    claimableRewards: 0,
    percentage: 0,
    rank: 0,
    displayMultiplierSum: 0,
  };

  const tokenIds = nfts.map(nft => BigInt(nft.tokenId));
  const tierCalls = tokenIds.map(tokenId => ({
    address: contractAddress,
    abi: config.abis.ascendant.main,
    functionName: 'getNFTAttribute',
    args: [tokenId],
  }));
  const recordCalls = tokenIds.map(tokenId => ({
    address: contractAddress,
    abi: config.abis.ascendant.main,
    functionName: 'userRecords',
    args: [tokenId],
  }));
  const claimableCall = {
    address: contractAddress,
    abi: config.abis.ascendant.main,
    functionName: 'batchClaimableAmount',
    args: [tokenIds],
  };

  cacheState = { ...cacheState, step: 'fetching_attributes' };
  const [tierResults, recordResults, claimableResults] = await Promise.all([
    retry(() => batchMulticall(tierCalls, config.alchemy.batchSize)),
    retry(() => batchMulticall(recordCalls, config.alchemy.batchSize)),
    retry(() => batchMulticall([claimableCall], config.alchemy.batchSize)),
  ]);

  tierResults.forEach((result, i) => {
    if (result?.status === 'success') {
      let tier;
      if (Array.isArray(result.result) && result.result.length >= 2) {
        tier = Number(result.result[1]);
      } else if (typeof result.result === 'object' && result.result.tier !== undefined) {
        tier = Number(result.result.tier);
      } else {
        log(`[Ascendant] [ERROR] Unexpected tier result format for token ${tokenIds[i]}`);
        return;
      }
      if (tier >= 1 && tier <= maxTier) {
        holder.tiers[tier] += 1;
        holder.total += 1;
        holder.multiplierSum += tiers[tier]?.multiplier || 0;
      }
    } else {
      log(`[Ascendant] [ERROR] Tier fetch failed for token ${tokenIds[i]}: ${result?.error || 'Unknown'}`);
    }
  });

  let totalShares = 0;
  recordResults.forEach((result, i) => {
    if (result?.status === 'success' && Array.isArray(result.result)) {
      const sharesRaw = result.result[0] || '0';
      const lockedAscendantRaw = result.result[1] || '0';
      const shares = parseFloat(formatUnits(sharesRaw, 18));
      const lockedAscendant = parseFloat(formatUnits(lockedAscendantRaw, 18));
      holder.shares += shares;
      holder.lockedAscendant += lockedAscendant;
      totalShares += shares;
    } else {
      log(`[Ascendant] [ERROR] Record fetch failed for token ${tokenIds[i]}: ${result?.error || 'Unknown'}`);
    }
  });

  if (claimableResults[0]?.status === 'success') {
    const claimableRaw = claimableResults[0].result || '0';
    holder.claimableRewards = parseFloat(formatUnits(claimableRaw, 18));
  } else {
    log(`[Ascendant] [ERROR] Claimable fetch failed for wallet ${_wallet}: ${claimableResults[0]?.error || 'Unknown'}`);
  }

  cacheState = { ...cacheState, step: 'fetching_shares' };
  const totalSharesRaw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: config.abis.ascendant.main,
      functionName: 'totalShares',
    })
  );
  const totalMultiplierSum = parseFloat(formatUnits(totalSharesRaw.toString(), 18));

  const toDistributeDay8Raw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: config.abis.ascendant.main,
      functionName: 'toDistribute',
      args: [0],
    })
  );
  const toDistributeDay8 = parseFloat(formatUnits(toDistributeDay8Raw.toString(), 18));

  const toDistributeDay28Raw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: config.abis.ascendant.main,
      functionName: 'toDistribute',
      args: [1],
    })
  );
  const toDistributeDay28 = parseFloat(formatUnits(toDistributeDay28Raw.toString(), 18));

  const toDistributeDay90Raw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: config.abis.ascendant.main,
      functionName: 'toDistribute',
      args: [2],
    })
  );
  const toDistributeDay90 = parseFloat(formatUnits(toDistributeDay90Raw.toString(), 18));

  const pendingRewardPerShareDay8 = totalShares > 0 ? toDistributeDay8 / totalShares : 0;
  const pendingRewardPerShareDay28 = totalShares > 0 ? toDistributeDay28 / totalShares : 0;
  const pendingRewardPerShareDay90 = totalShares > 0 ? toDistributeDay90 / totalShares : 0;

  holder.pendingDay8 = holder.shares * pendingRewardPerShareDay8;
  holder.pendingDay28 = holder.shares * pendingRewardPerShareDay28;
  holder.pendingDay90 = holder.shares * pendingRewardPerShareDay90;
  holder.percentage = totalMultiplierSum > 0 ? (holder.multiplierSum / totalMultiplierSum) * 100 : 0;
  holder.displayMultiplierSum = holder.multiplierSum;

  await setCache(cacheKey, holder);
  cacheState = { ...cacheState, isPopulating: false, step: 'completed' };
  return holder;
}

export async function GET(request) {
  const _requestId = crypto.randomUUID();
  const { searchParams, pathname } = new URL(request.url);
  const page = parseInt(searchParams.get('page') || '0', 10);
  const pageSize = parseInt(searchParams.get('pageSize') || config.contractDetails.ascendant.pageSize, 10);
  const wallet = searchParams.get('wallet');

  if (pathname.endsWith('/progress')) {
    const state = await getCacheState(config.contractAddresses.ascendant.address);
    const progressPercentage = state.progressState.totalNfts > 0
      ? ((state.progressState.processedNfts / state.progressState.totalNfts) * 100).toFixed(1)
      : '0.0';
    return NextResponse.json({
      isPopulating: state.isCachePopulating,
      totalLiveHolders: state.totalOwners,
      totalOwners: state.totalOwners,
      phase: state.progressState.step.charAt(0).toUpperCase() + state.progressState.step.slice(1),
      progressPercentage,
    });
  }

  try {
    if (wallet) {
      const holder = await getHolderData(wallet);
      if (!holder) {
        log(`[Ascendant] [ERROR] No holder data found for wallet ${wallet}`);
        return NextResponse.json({ message: 'No holder data found for wallet' }, { status: 404 });
      }
      return NextResponse.json(safeSerialize(holder));
    }

    const data = await getAllHolders(page, pageSize);
    return NextResponse.json(safeSerialize(data));
  } catch (error) {
    log(`[Ascendant] [ERROR] Error: ${error.message}`);
    let status = 500;
    let message = 'Failed to fetch Ascendant data';
    if (error.message.includes('Rate limit')) {
      status = 429;
      message = 'Rate limit exceeded';
    }
    return NextResponse.json({ error: message, details: error.message }, { status });
  }
}// ./app/api/holders/Ascendant/progress/route.js
import { NextResponse } from 'next/server';
import { log } from '@/app/api/utils';
import { getCacheState } from '@/app/api/holders/Ascendant/route';
import config from '@/config';

export async function GET() {
  const address = config.contractAddresses.ascendant.address;
  if (!address) {
    log(`[ascendant] [VALIDATION] Ascendant contract address not found`);
    return NextResponse.json({ error: 'Ascendant contract address not found' }, { status: 400 });
  }

  try {
    const state = await getCacheState(address);
    if (!state || !state.progressState) {
      log(`[ascendant] [VALIDATION] Invalid cache state for ${address}`);
      return NextResponse.json({ error: 'Cache state not initialized' }, { status: 500 });
    }
    const progressPercentage = state.progressState.totalNfts > 0
      ? ((state.progressState.processedNfts / state.progressState.totalNfts) * 100).toFixed(1)
      : '0.0';

    return NextResponse.json({
      isPopulating: state.isCachePopulating,
      totalLiveHolders: state.totalOwners,
      totalOwners: state.totalOwners,
      phase: state.progressState.step.charAt(0).toUpperCase() + state.progressState.step.slice(1),
      progressPercentage,
    });
  } catch (error) {
    log(`[ascendant] [ERROR] Progress endpoint error: ${error.message}, stack: ${error.stack}`);
    return NextResponse.json({ error: `Server error: ${error.message}` }, { status: 500 });
  }
}// app/api/holders/E280/route.js
import { NextResponse } from 'next/server';
import { log, getCache } from '@/app/api/utils';

export async function GET(request) {
  const { searchParams } = new URL(request.url);
  const page = parseInt(searchParams.get('page') || '0');
  const pageSize = parseInt(searchParams.get('pageSize') || '1000');
  const wallet = searchParams.get('wallet')?.toLowerCase();

  log(`[E280] GET Request: page=${page}, pageSize=${pageSize}, wallet=${wallet}`);

  const cacheKey = `e280_holders_${page}_${pageSize}_${wallet || 'all'}`;
  let cachedData;
  try {
    cachedData = await getCache(cacheKey);
    if (cachedData) {
      log(`[E280] Cache hit: ${cacheKey}`);
      return NextResponse.json(cachedData);
    }
    log(`[E280] Cache miss: ${cacheKey}`);
  } catch (cacheError) {
    log(`[E280] Cache read error: ${cacheError.message}`);
  }

  log('[E280] GET: Contract not yet deployed');
  return NextResponse.json({ error: 'E280 contract not yet deployed' }, { status: 400 });
}

export async function POST(_request) {
  log('[E280] POST: Contract not yet deployed');
  return NextResponse.json({ error: 'E280 contract not yet deployed' }, { status: 400 });
}// app/api/holders/E280/progress/route.js
import { NextResponse } from 'next/server';
import { log } from '@/app/api/utils';

export async function GET(_request) {
  try {
    log('[E280] [INFO] Progress: Contract not yet deployed');
    return NextResponse.json({
      isPopulating: false,
      totalLiveHolders: 0,
      totalOwners: 0,
      phase: 'Not Deployed',
      progressPercentage: '0.0',
    });
  } catch (error) {
    log(`[E280] [ERROR] Progress endpoint error: ${error.message}`);
    return NextResponse.json({ error: 'Failed to fetch progress state' }, { status: 500 });
  }
}// app/api/holders/Element369/route.js
import { NextResponse } from 'next/server';
import config from '@/config.js';
import { getOwnersForContract, log, batchMulticall, getCache, setCache } from '@/app/api/utils';

const contractAddress = config.contractAddresses.element369.address;
const vaultAddress = config.vaultAddresses.element369.address;
const tiersConfig = config.contractDetails.element369.tiers;
const defaultPageSize = config.contractDetails.element369.pageSize;
const element369MinimalAbi = config.abis.element369.main;
const element369VaultMinimalAbi = config.abis.element369.vault;

let cacheState = {
  isPopulating: false,
  totalOwners: 0,
  totalNfts: 0,
  processedNfts: 0,
  step: 'idle',
  debugId: `state-${Math.random().toString(36).slice(2)}`,
};

export async function getCacheState(_address) {
  return {
    isCachePopulating: cacheState.isPopulating,
    totalOwners: cacheState.totalOwners,
    progressState: {
      step: cacheState.step,
      totalNfts: cacheState.totalNfts,
      processedNfts: cacheState.processedNfts,
    },
    debugId: cacheState.debugId,
  };
}

export async function GET(request) {
  const { searchParams, pathname } = new URL(request.url);
  const page = parseInt(searchParams.get('page') || '0');
  const pageSize = parseInt(searchParams.get('pageSize') || defaultPageSize);
  const wallet = searchParams.get('wallet')?.toLowerCase();

  if (!contractAddress || !vaultAddress || !tiersConfig || !defaultPageSize) {
    log(`[Element369] [VALIDATION] Config error: contractAddress=${contractAddress}, vaultAddress=${vaultAddress}, tiersConfig=${tiersConfig}, pageSize=${defaultPageSize}`);
    return NextResponse.json({ error: 'Element369 contract, vault address, tiers config, or page size missing' }, { status: 400 });
  }

  if (pathname.endsWith('/progress')) {
    const state = await getCacheState(contractAddress);
    const progressPercentage = state.progressState.totalNfts > 0
      ? ((state.progressState.processedNfts / state.progressState.totalNfts) * 100).toFixed(1)
      : '0.0';
    return NextResponse.json({
      isPopulating: state.isCachePopulating,
      totalLiveHolders: state.totalOwners,
      totalOwners: state.totalOwners,
      phase: state.progressState.step.charAt(0).toUpperCase() + state.progressState.step.slice(1),
      progressPercentage,
    });
  }

  log(`[Element369] Request: page=${page}, pageSize=${pageSize}, wallet=${wallet}, contract=${contractAddress}`);

  try {
    const cacheKey = `element369_holders_${page}_${pageSize}_${wallet || 'all'}`;
    let cachedData;
    try {
      if (cacheState.isPopulating) {
        log(`[Element369] [INFO] Waiting for cache population to complete`);
        return NextResponse.json({ message: 'Cache is populating', ...await getCacheState(contractAddress) });
      }
      cachedData = await getCache(cacheKey);
      if (cachedData) {
        log(`[Element369] [INFO] Cache hit: ${cacheKey}`);
        return NextResponse.json(cachedData);
      }
      log(`[Element369] [INFO] Cache miss: ${cacheKey}`);
    } catch (cacheError) {
      log(`[Element369] [ERROR] Cache read error: ${cacheError.message}`);
    }

    cacheState = { ...cacheState, isPopulating: true, step: 'fetching_owners', processedNfts: 0, totalNfts: 0, totalOwners: 0 };
    log(`[Element369] Fetching owners...`);
    const owners = await getOwnersForContract(contractAddress, element369MinimalAbi);
    cacheState = { ...cacheState, step: 'filtering_owners', totalNfts: owners.length, totalOwners: new Set(owners.map(o => o.ownerAddress.toLowerCase())).size };

    const burnAddress = '0x0000000000000000000000000000000000000000';
    const filteredOwners = wallet
      ? owners.filter(owner => owner.ownerAddress.toLowerCase() === wallet && owner.ownerAddress.toLowerCase() !== burnAddress)
      : owners.filter(owner => owner.ownerAddress.toLowerCase() !== burnAddress);
    log(`[Element369] Live owners: ${filteredOwners.length}`);
    cacheState = { ...cacheState, step: 'building_token_map' };

    const tokenOwnerMap = new Map();
    const ownerTokens = new Map();
    let totalTokens = 0;
    filteredOwners.forEach(owner => {
      const walletAddr = owner.ownerAddress.toLowerCase();
      const tokenId = owner.tokenId;
      tokenOwnerMap.set(tokenId, walletAddr);
      totalTokens++;
      const tokens = ownerTokens.get(walletAddr) || [];
      tokens.push(tokenId);
      ownerTokens.set(walletAddr, tokens);
    });
    log(`[Element369] Total tokens: ${totalTokens}, tokenOwnerMap size: ${tokenOwnerMap.size}`);
    cacheState = { ...cacheState, step: 'fetching_tiers' };

    const allTokenIds = Array.from(tokenOwnerMap.keys());
    const start = page * pageSize;
    const end = Math.min(start + pageSize, allTokenIds.length);
    const paginatedTokenIds = allTokenIds.slice(start, end);
    log(`[Element369] Paginated tokens: ${paginatedTokenIds.length}`);

    const tierCalls = paginatedTokenIds.map(tokenId => ({
      address: contractAddress,
      abi: element369MinimalAbi,
      functionName: 'getNftTier',
      args: [BigInt(tokenId)],
    }));
    const tierResults = await batchMulticall(tierCalls);
    log(`[Element369] Tiers fetched for ${tierResults.length} tokens`);
    cacheState = { ...cacheState, step: 'processing_holders', processedNfts: tierResults.length };

    const maxTier = Math.max(...Object.keys(tiersConfig).filter(key => !isNaN(key)).map(Number));
    const holdersMap = new Map();

    tierResults.forEach((result, i) => {
      if (result?.status === 'success') {
        const tokenId = paginatedTokenIds[i];
        const walletAddr = tokenOwnerMap.get(tokenId);
        const tier = Number(result.result);

        if (walletAddr && walletAddr !== burnAddress) {
          if (!holdersMap.has(walletAddr)) {
            holdersMap.set(walletAddr, {
              wallet: walletAddr,
              total: 0,
              multiplierSum: 0,
              tiers: Array(maxTier).fill(0),
              infernoRewards: 0,
              fluxRewards: 0,
              e280Rewards: 0,
            });
          }
          const holder = holdersMap.get(walletAddr);
          holder.total += 1;
          if (tier >= 1 && tier <= maxTier) {
            holder.multiplierSum += tiersConfig[tier]?.multiplier || 0;
            holder.tiers[tier - 1] += 1;
          } else {
            log(`[Element369] [ERROR] Invalid tier ${tier} for token ${tokenId}`);
            holder.multiplierSum += tiersConfig[1]?.multiplier || 0;
          }
        }
      } else {
        log(`[Element369] [ERROR] Tier fetch failed for token ${paginatedTokenIds[i]}: ${result?.error || 'Unknown'}`);
      }
    });

    let holders = Array.from(holdersMap.values());
    cacheState = { ...cacheState, step: 'fetching_rewards' };

    const rewardCalls = holders.map(holder => {
      const tokenIds = ownerTokens.get(holder.wallet) || [];
      return {
        address: vaultAddress,
        abi: element369VaultMinimalAbi,
        functionName: 'getRewards',
        args: [tokenIds.map(id => BigInt(id)), holder.wallet, false],
      };
    });

    log(`[Element369] Fetching rewards for ${holders.length} holders`);
    const rewardsResults = await batchMulticall(rewardCalls);

    holders.forEach((holder, i) => {
      if (rewardsResults[i]?.status === 'success' && rewardsResults[i].result) {
        const [, , infernoPool, fluxPool, e280Pool] = rewardsResults[i].result;
        holder.infernoRewards = Number(infernoPool) / 1e18;
        holder.fluxRewards = Number(fluxPool) / 1e18;
        holder.e280Rewards = Number(e280Pool) / 1e18;
      } else {
        holder.infernoRewards = 0;
        holder.fluxRewards = 0;
        holder.e280Rewards = 0;
        log(`[Element369] [ERROR] Reward fetch failed for ${holder.wallet.slice(0, 6)}...: ${rewardsResults[i]?.error || 'Unknown'}`);
      }
      holder.displayMultiplierSum = holder.multiplierSum;
      holder.percentage = 0;
      holder.rank = 0;
    });

    const totalMultiplierSum = holders.reduce((sum, h) => sum + h.multiplierSum, 0);
    holders.forEach((holder, index) => {
      holder.percentage = totalMultiplierSum > 0 ? (holder.multiplierSum / totalMultiplierSum) * 100 : 0;
      holder.rank = index + 1;
      holder.displayMultiplierSum = holder.multiplierSum;
    });

    holders.sort((a, b) => b.multiplierSum - a.multiplierSum || b.total - a.total);

    const response = {
      holders,
      totalTokens,
      page,
      pageSize,
      totalPages: wallet ? 1 : Math.ceil(totalTokens / pageSize),
    };

    await setCache(cacheKey, response);
    log(`[Element369] [INFO] Cached response: ${cacheKey}`);

    cacheState = { ...cacheState, isPopulating: false, step: 'completed' };
    log(`[Element369] Success: ${holders.length} holders`);
    return NextResponse.json(response);
  } catch (error) {
    log(`[Element369] [ERROR] Error: ${error.message}`);
    cacheState = { ...cacheState, isPopulating: false, step: 'error' };
    return NextResponse.json({ error: 'Failed to fetch Element369 data', details: error.message }, { status: 500 });
  }
}// ./app/api/holders/Element369/progress/route.js
import { NextResponse } from 'next/server';
import { log } from '@/app/api/utils';
import { getCacheState } from '@/app/api/holders/Element369/route';
import config from '@/config';

export async function GET() {
  const address = config.contractAddresses.element369.address;
  if (!address) {
    log(`[element369] [VALIDATION] Element369 contract address not found`);
    return NextResponse.json({ error: 'Element369 contract address not found' }, { status: 400 });
  }

  try {
    const state = await getCacheState(address);
    if (!state || !state.progressState) {
      log(`[element369] [VALIDATION] Invalid cache state for ${address}`);
      return NextResponse.json({ error: 'Cache state not initialized' }, { status: 500 });
    }
    const progressPercentage = state.progressState.totalNfts > 0
      ? ((state.progressState.processedNfts / state.progressState.totalNfts) * 100).toFixed(1)
      : '0.0';

    return NextResponse.json({
      isPopulating: state.isCachePopulating,
      totalLiveHolders: state.totalOwners,
      totalOwners: state.totalOwners,
      phase: state.progressState.step.charAt(0).toUpperCase() + state.progressState.step.slice(1),
      progressPercentage,
    });
  } catch (error) {
    log(`[element369] [ERROR] Progress endpoint error: ${error.message}, stack: ${error.stack}`);
    return NextResponse.json({ error: `Server error: ${error.message}` }, { status: 500 });
  }
}// File: app/store.js

'use client';
import { create } from 'zustand';

const CACHE_TTL = 30 * 60 * 1000; // 30 minutes

export const useNFTStore = create((set, get) => ({
  cache: {},
  setCache: (contractKey, data) => {
    const key = `nft:${contractKey}`;
    console.log(`[NFTStore] Setting cache for ${key}: ${data.holders?.length || 0} holders`);
    set((state) => ({
      cache: {
        ...state.cache,
        [key]: { data, timestamp: Date.now() },
      },
    }));
  },
  getCache: (contractKey) => {
    const key = `nft:${contractKey}`;
    console.log(`[NFTStore] Getting cache for ${key}`);
    const cachedEntry = get().cache[key];
    if (!cachedEntry) {
      console.log(`[NFTStore] Cache miss for ${key}`);
      return null;
    }
    const now = Date.now();
    if (now - cachedEntry.timestamp > CACHE_TTL) {
      console.log(`[NFTStore] Cache expired for ${key}`);
      set((state) => {
        const newCache = { ...state.cache };
        delete newCache[key];
        return { cache: newCache };
      });
      return null;
    }
    console.log(`[NFTStore] Cache hit for ${key}: ${cachedEntry.data.holders?.length || 0} holders`);
    return cachedEntry.data;
  },
}));// config.js
import element280NftStatus from './element280_nft_status.json' assert { type: 'json' };
import element280MainAbi from './abi/element280.json' assert { type: 'json' };
import element280VaultAbi from './abi/element280Vault.json' assert { type: 'json' };
import element369MainAbi from './abi/element369.json' assert { type: 'json' };
import element369VaultAbi from './abi/element369Vault.json' assert { type: 'json' };
import staxMainAbi from './abi/staxNFT.json' assert { type: 'json' };
import staxVaultAbi from './abi/staxVault.json' assert { type: 'json' };
import ascendantMainAbi from './abi/ascendantNFT.json' assert { type: 'json' };
// E280 ABI placeholder (not deployed)
const e280MainAbi = [];

const config = {
  // Supported blockchain networks
  supportedChains: ['ETH', 'BASE'],

  // ABIs for all collections
  abis: {
    element280: {
      main: element280MainAbi,
      vault: element280VaultAbi,
    },
    element369: {
      main: element369MainAbi,
      vault: element369VaultAbi,
    },
    stax: {
      main: staxMainAbi,
      vault: staxVaultAbi,
    },
    ascendant: {
      main: ascendantMainAbi,
      vault: [], // No vault ABI provided for Ascendant
    },
    e280: {
      main: e280MainAbi,
      vault: [],
    },
  },

  // NFT contract configurations
  nftContracts: {
    element280: {
      name: 'Element 280',
      symbol: 'ELMNT',
      chain: 'ETH',
      address: '0x7F090d101936008a26Bf1F0a22a5f92fC0Cf46c9',
      vaultAddress: '0x44c4ADAc7d88f85d3D33A7f856Ebc54E60C31E97',
      deploymentBlock: '20945304',
      tiers: {
        1: { name: 'Common', multiplier: 10, allocation: '100000000000000000000000000' },
        2: { name: 'Common Amped', multiplier: 12, allocation: '100000000000000000000000000' },
        3: { name: 'Rare', multiplier: 100, allocation: '1000000000000000000000000000' },
        4: { name: 'Rare Amped', multiplier: 120, allocation: '1000000000000000000000000000' },
        5: { name: 'Legendary', multiplier: 1000, allocation: '10000000000000000000000000000' },
        6: { name: 'Legendary Amped', multiplier: 1200, allocation: '10000000000000000000000000000' },
      },
      description:
        'Element 280 NFTs can be minted with TitanX or ETH during a presale and redeemed for Element 280 tokens after a cooldown period. Multipliers contribute to a pool used for reward calculations.',
      expectedTotalSupply: 8107,
      expectedBurned: 8776,
      maxTokensPerOwnerQuery: 100,
    },
    element369: {
      name: 'Element 369',
      symbol: 'E369',
      chain: 'ETH',
      address: '0x024D64E2F65747d8bB02dFb852702D588A062575',
      vaultAddress: '0x4e3DBD6333e649AF13C823DAAcDd14f8507ECBc5',
      deploymentBlock: '21224418',
      tiers: {
        1: { name: 'Common', multiplier: 1, price: '100000000000000000000000000' },
        2: { name: 'Rare', multiplier: 10, price: '1000000000000000000000000000' },
        3: { name: 'Legendary', multiplier: 100, price: '10000000000000000000000000000' },
      },
      description:
        'Element 369 NFTs are minted with TitanX or ETH during specific sale cycles. Burning NFTs updates a multiplier pool and tracks burn cycles for reward distribution in the Holder Vault.',
    },
    stax: {
      name: 'Stax',
      symbol: 'STAX',
      chain: 'ETH',
      address: '0x74270Ca3a274B4dbf26be319A55188690CACE6E1',
      vaultAddress: '0x5D27813C32dD705404d1A78c9444dAb523331717',
      deploymentBlock: '21452667',
      tiers: {
        1: { name: 'Common', multiplier: 1, price: '100000000000000000000000000' },
        2: { name: 'Common Amped', multiplier: 1.2, price: '100000000000000000000000000', amplifier: '10000000000000000000000000' },
        3: { name: 'Common Super', multiplier: 1.4, price: '100000000000000000000000000', amplifier: '20000000000000000000000000' },
        4: { name: 'Common LFG', multiplier: 2, price: '100000000000000000000000000', amplifier: '50000000000000000000000000' },
        5: { name: 'Rare', multiplier: 10, price: '1000000000000000000000000000' },
        6: { name: 'Rare Amped', multiplier: 12, price: '1000000000000000000000000000', amplifier: '100000000000000000000000000' },
        7: { name: 'Rare Super', multiplier: 14, price: '1000000000000000000000000000', amplifier: '200000000000000000000000000' },
        8: { name: 'Rare LFG', multiplier: 20, price: '1000000000000000000000000000', amplifier: '500000000000000000000000000' },
        9: { name: 'Legendary', multiplier: 100, price: '10000000000000000000000000000' },
        10: { name: 'Legendary Amped', multiplier: 120, price: '10000000000000000000000000000', amplifier: '1000000000000000000000000000' },
        11: { name: 'Legendary Super', multiplier: 140, price: '10000000000000000000000000000', amplifier: '2000000000000000000000000000' },
        12: { name: 'Legendary LFG', multiplier: 200, price: '10000000000000000000000000000', amplifier: '5000000000000000000000000000' },
      },
      description:
        'Stax NFTs are minted with TitanX or ETH during a presale. Burning NFTs after a cooldown period claims backing rewards, with multipliers contributing to a pool for cycle-based reward calculations.',
    },
    ascendant: {
      name: 'Ascendant',
      symbol: 'ASCNFT',
      chain: 'ETH',
      address: '0x9da95c32c5869c84ba2c020b5e87329ec0adc97f',
      deploymentBlock: '21112535',
      tiers: {
        1: { name: 'Tier 1', price: '7812500000000000000000', multiplier: 1.01 },
        2: { name: 'Tier 2', price: '15625000000000000000000', multiplier: 1.02 },
        3: { name: 'Tier 3', price: '31250000000000000000000', multiplier: 1.03 },
        4: { name: 'Tier 4', price: '62500000000000000000000', multiplier: 1.04 },
        5: { name: 'Tier 5', price: '125000000000000000000000', multiplier: 1.05 },
        6: { name: 'Tier 6', price: '250000000000000000000000', multiplier: 1.06 },
        7: { name: 'Tier 7', price: '500000000000000000000000', multiplier: 1.07 },
        8: { name: 'Tier 8', price: '1000000000000000000000000', multiplier: 1.08 },
      },
      description:
        'Ascendant NFTs are minted with ASCENDANT tokens and offer staking rewards from DragonX pools over 8, 28, and 90-day periods. Features fusion mechanics to combine same-tier NFTs into higher tiers.',
    },
    e280: {
      name: 'E280',
      symbol: 'E280',
      chain: 'BASE',
      address: null,
      deploymentBlock: null,
      tiers: {},
      description: 'E280 NFTs on BASE chain. Contract not yet deployed.',
      disabled: true,
    },
  },

  // Contract addresses
  contractAddresses: {
    element280: { chain: 'ETH', address: '0x7F090d101936008a26Bf1F0a22a5f92fC0Cf46c9' },
    element369: { chain: 'ETH', address: '0x024D64E2F65747d8bB02dFb852702D588A062575' },
    stax: { chain: 'ETH', address: '0x74270Ca3a274B4dbf26be319A55188690CACE6E1' },
    ascendant: { chain: 'ETH', address: '0x9da95c32c5869c84ba2c020b5e87329ec0adc97f' },
    e280: { chain: 'BASE', address: null },
  },

  // Vault addresses
  vaultAddresses: {
    element280: { chain: 'ETH', address: '0x44c4ADAc7d88f85d3D33A7f856Ebc54E60C31E97' },
    element369: { chain: 'ETH', address: '0x4e3DBD6333e649AF13C823DAAcDd14f8507ECBc5' },
    stax: { chain: 'ETH', address: '0x5D27813C32dD705404d1A78c9444dAb523331717' },
    e280: { chain: 'BASE', address: null },
  },

  // Deployment blocks
  deploymentBlocks: {
    element280: { chain: 'ETH', block: '20945304' },
    element369: { chain: 'ETH', block: '21224418' },
    stax: { chain: 'ETH', block: '21452667' },
    ascendant: { chain: 'ETH', block: '21112535' },
    e280: { chain: 'BASE', block: null },
  },

  // Contract tiers
  contractTiers: {
    element280: {
      1: { name: 'Common', multiplier: 10 },
      2: { name: 'Common Amped', multiplier: 12 },
      3: { name: 'Rare', multiplier: 100 },
      4: { name: 'Rare Amped', multiplier: 120 },
      5: { name: 'Legendary', multiplier: 1000 },
      6: { name: 'Legendary Amped', multiplier: 1200 },
    },
    element369: {
      1: { name: 'Common', multiplier: 1 },
      2: { name: 'Rare', multiplier: 10 },
      3: { name: 'Legendary', multiplier: 100 },
      tierOrder: [
        { tierId: '3', name: 'Legendary' },
        { tierId: '2', name: 'Rare' },
        { tierId: '1', name: 'Common' },
      ],
    },
    stax: {
      1: { name: 'Common', multiplier: 1 },
      2: { name: 'Common Amped', multiplier: 1.2 },
      3: { name: 'Common Super', multiplier: 1.4 },
      4: { name: 'Common LFG', multiplier: 2 },
      5: { name: 'Rare', multiplier: 10 },
      6: { name: 'Rare Amped', multiplier: 12 },
      7: { name: 'Rare Super', multiplier: 14 },
      8: { name: 'Rare LFG', multiplier: 20 },
      9: { name: 'Legendary', multiplier: 100 },
      10: { name: 'Legendary Amped', multiplier: 120 },
      11: { name: 'Legendary Super', multiplier: 140 },
      12: { name: 'Legendary LFG', multiplier: 200 },
    },
    ascendant: {
      1: { name: 'Tier 1', multiplier: 1.01 },
      2: { name: 'Tier 2', multiplier: 1.02 },
      3: { name: 'Tier 3', multiplier: 1.03 },
      4: { name: 'Tier 4', multiplier: 1.04 },
      5: { name: 'Tier 5', multiplier: 1.05 },
      6: { name: 'Tier 6', multiplier: 1.06 },
      7: { name: 'Tier 7', multiplier: 1.07 },
      8: { name: 'Tier 8', multiplier: 1.08 },
    },
    e280: {},
  },

  // Contract details
  contractDetails: {
    element280: {
      name: 'Element 280',
      chain: 'ETH',
      pageSize: 100,
      apiEndpoint: '/api/holders/Element280',
      rewardToken: 'ELMNT',
    },
    element369: {
      name: 'Element 369',
      chain: 'ETH',
      pageSize: 1000,
      apiEndpoint: '/api/holders/Element369',
      rewardToken: 'INFERNO/FLUX/E280',
    },
    stax: {
      name: 'Stax',
      chain: 'ETH',
      pageSize: 1000,
      apiEndpoint: '/api/holders/Stax',
      rewardToken: 'X28',
    },
    ascendant: {
      name: 'Ascendant',
      chain: 'ETH',
      pageSize: 1000,
      apiEndpoint: '/api/holders/Ascendant',
      rewardToken: 'DRAGONX',
    },
    e280: {
      name: 'E280',
      chain: 'BASE',
      pageSize: 1000,
      apiEndpoint: '/api/holders/E280',
      rewardToken: 'E280',
      disabled: true,
    },
  },

  // Utility function to get contract details by name
  getContractDetails: (contractName) => {
    return config.nftContracts[contractName] || null;
  },

  // Alchemy settings (optimized for free tier)
  alchemy: {
    network: 'eth-mainnet',
    batchSize: 10,
    batchDelayMs: 1000,
    retryMaxDelayMs: 30000,
    maxRetries: 3,
    timeoutMs: 30000 // 30 seconds
  },

  // Cache settings
  cache: {
    redis: {
      disableElement280: process.env.DISABLE_ELEMENT280_REDIS === 'true',
      disableElement369: process.env.DISABLE_ELEMENT369_REDIS === 'true',
      disableStax: process.env.DISABLE_STAX_REDIS === 'true',
      disableAscendant: process.env.DISABLE_ASCENDANT_REDIS === 'true',
      disableE280: process.env.DISABLE_E280_REDIS === 'true' || true,
    },
    nodeCache: {
      stdTTL: 3600,
      checkperiod: 120,
    },
  },

  // Debug settings
  debug: {
    enabled: process.env.DEBUG === 'true',
    logLevel: 'debug',
  },

  // Fallback data (optional, for testing)
  fallbackData: {
    element280: process.env.USE_FALLBACK_DATA === 'true' ? element280NftStatus : null,
  },
};

export default config;// app/api/utils.js
import { createPublicClient, http, parseAbi } from 'viem';
import { mainnet } from 'viem/chains';
import { Redis } from '@upstash/redis';
import NodeCache from 'node-cache';
import pino from 'pino';
import { promises as fs } from 'fs';
import config from '@/config.js';
import { Network, Alchemy } from 'alchemy-sdk';

let loggerInstance = null;

const ALCHEMY_API_KEY = config.alchemy.apiKey || process.env.NEXT_PUBLIC_ALCHEMY_API_KEY;

export const client = createPublicClient({
  chain: mainnet,
  transport: http(`https://eth-mainnet.g.alchemy.com/v2/${ALCHEMY_API_KEY}`, {
    timeout: 60000,
  }),
});

const alchemy = new Alchemy({
  apiKey: ALCHEMY_API_KEY,
  network: Network.ETH_MAINNET,
});

const DEBUG = process.env.DEBUG === 'true';
const isProduction = process.env.NODE_ENV === 'production';

export const logger = (() => {
  if (loggerInstance) {
    if (DEBUG) console.log('[utils] [DEBUG] Reusing existing logger instance');
    return loggerInstance;
  }
  loggerInstance = pino({
    level: DEBUG ? 'debug' : 'error',
    formatters: {
      level: (label) => ({ level: label.toUpperCase() }),
    },
    timestamp: pino.stdTimeFunctions.isoTime,
    ...(!isProduction
      ? {
          transport: {
            target: 'pino-pretty',
            options: {
              colorize: true,
              translateTime: 'yyyy-mm-dd HH:MM:ss',
              ignore: 'pid,hostname',
            },
          },
        }
      : {}),
  });
  try {
    if (DEBUG) loggerInstance.debug('[utils] Pino logger initialized');
    console.log('[utils] Pino logger initialized (console)');
  } catch (_error) {
    console.error('[utils] Failed to initialize logger:', _error.message);
  }
  return loggerInstance;
})();

export function log(message) {
  try {
    if (message.includes('[ERROR]') || message.includes('[VALIDATION]')) {
      logger.error(message);
    } else if (DEBUG) {
      logger.debug(message);
    }
  } catch (_error) {
    console.error('[utils] Logger error:', _error.message);
  }
}

const cache = new NodeCache({
  stdTTL: config.cache.nodeCache.stdTTL,
  checkperiod: config.cache.nodeCache.checkperiod,
});

const redisDisableFlags = {
  element280: process.env.DISABLE_ELEMENT280_REDIS === 'true',
  element369: process.env.DISABLE_ELEMENT369_REDIS === 'true',
  stax: process.env.DISABLE_STAX_REDIS === 'true',
  ascendant: process.env.DISABLE_ASCENDANT_REDIS === 'true',
  e280: process.env.DISABLE_E280_REDIS === 'true' || true,
};

let redis = null;
const allRedisDisabled = Object.values(redisDisableFlags).every(flag => flag);
if (!allRedisDisabled) {
  try {
    const redisUrl = process.env.UPSTASH_REDIS_REST_URL || config.redis?.url;
    const redisToken = process.env.UPSTASH_REDIS_REST_TOKEN || config.redis?.token;
    if (redisUrl && redisToken) {
      redis = new Redis({
        url: redisUrl,
        token: redisToken,
      });
      if (DEBUG) log('[utils] [DEBUG] Upstash Redis initialized');
    } else {
      log('[utils] [ERROR] Redis initialization skipped: missing UPSTASH_REDIS_REST_URL or UPSTASH_REDIS_REST_TOKEN');
    }
  } catch (_error) {
    log(`[utils] [ERROR] Failed to initialize Redis: ${_error.message}`);
    redis = null;
  }
} else {
  if (DEBUG) log('[utils] [DEBUG] Redis skipped: all collections have Redis disabled');
}

const delay = ms => new Promise(resolve => setTimeout(resolve, ms));

export async function batchMulticall(calls, batchSize = config.alchemy.batchSize, options = { retryCount: 0 }) {
  const batches = [];
  for (let i = 0; i < calls.length; i += batchSize) {
    batches.push(calls.slice(i, i + batchSize));
  }
  const results = [];
  for (const batch of batches) {
    try {
      const batchResults = await client.multicall({ contracts: batch });
      results.push(...batchResults);
      await delay(config.alchemy.batchDelayMs);
    } catch (error) {
      log(`[utils] [ERROR] batchMulticall failed: ${error.message}`);
      if (options.retryCount < config.alchemy.maxRetries) {
        await delay(config.alchemy.retryMaxDelayMs / config.alchemy.maxRetries);
        return batchMulticall(calls, batchSize, { retryCount: options.retryCount + 1 });
      }
      throw error;
    }
  }
  return results;
}

export async function getCache(key) {
  let data = cache.get(key);
  if (!data && redis && !redisDisableFlags[key.split('_')[0]]) {
    try {
      data = await redis.get(key);
      data = data ? JSON.parse(data) : null;
      if (data) cache.set(key, data);
    } catch (error) {
      log(`[utils] [ERROR] Redis get failed for key ${key}: ${error.message}`);
    }
  }
  if (DEBUG) {
    log(`[utils] [DEBUG] Cache get for ${key}: ${data ? 'hit' : 'miss'}`);
  }
  return data;
}

export async function setCache(key, value, ttl = config.cache.nodeCache.stdTTL) {
  cache.set(key, value, ttl);
  if (redis && !redisDisableFlags[key.split('_')[0]]) {
    try {
      await redis.set(key, JSON.stringify(value), { ex: ttl });
    } catch (error) {
      log(`[utils] [ERROR] Redis set failed for key ${key}: ${error.message}`);
    }
  }
  if (DEBUG) {
    log(`[utils] [DEBUG] Cache set for ${key}`);
  }
}

export async function loadCacheState(contractAddress) {
  const cacheKey = `state_${contractAddress}`;
  let state = cache.get(cacheKey);
  const collection = Object.keys(config.contractAddresses).find(col => config.contractAddresses[col].address === contractAddress);
  const disableRedis = collection ? redisDisableFlags[collection] : true;
  if (!state && redis && !disableRedis) {
    try {
      state = await redis.get(cacheKey);
      state = state ? JSON.parse(state) : null;
    } catch (error) {
      log(`[utils] [ERROR] Redis get failed for key ${cacheKey}: ${error.message}`);
    }
  }
  if (!state && process.env.NODE_ENV !== 'production') {
    try {
      state = JSON.parse(await fs.readFile(`./cache_state_${contractAddress}.json`, 'utf8'));
    } catch (_error) {
      log(`[utils] [ERROR] Failed to read cache state file for ${contractAddress}: ${_error.message}`);
      state = {
        isCachePopulating: false,
        holdersMapCache: null,
        totalOwners: 0,
        progressState: { step: 'idle', processedNfts: 0, totalNfts: 0 },
      };
    }
  }
  if (DEBUG) {
    log(`[utils] [DEBUG] Loaded cache state for ${cacheKey}: ${JSON.stringify(state)}`);
  }
  return state;
}

export async function saveCacheState(contractAddress, state) {
  const cacheKey = `state_${contractAddress}`;
  cache.set(cacheKey, state);
  const collection = Object.keys(config.contractAddresses).find(col => config.contractAddresses[col].address === contractAddress);
  const disableRedis = collection ? redisDisableFlags[collection] : true;
  if (redis && !disableRedis) {
    try {
      await redis.set(cacheKey, JSON.stringify(state));
    } catch (error) {
      log(`[utils] [ERROR] Redis set failed for key ${cacheKey}: ${error.message}`);
    }
  }
  if (process.env.NODE_ENV !== 'production') {
    try {
      await fs.writeFile(`./cache_state_${contractAddress}.json`, JSON.stringify(state, null, 2));
    } catch (_error) {
      log(`[utils] [ERROR] Failed to write cache state to file: ${_error.message}`);
    }
  }
  if (DEBUG) {
    log(`[utils] [DEBUG] Saved cache state for ${cacheKey}`);
  }
}

export async function getNftsForOwner(ownerAddress, contractAddress, abi) {
  try {
    const contract = {
      address: contractAddress,
      abi: parseAbi(abi),
    };
    const balance = await client.readContract({
      ...contract,
      functionName: 'balanceOf',
      args: [ownerAddress],
    });
    const tokenIds = [];
    for (let i = 0; i < Number(balance); i++) {
      const tokenId = await client.readContract({
        ...contract,
        functionName: 'tokenOfOwnerByIndex',
        args: [ownerAddress, BigInt(i)],
      });
      tokenIds.push(tokenId);
    }
    if (DEBUG) {
      log(`[utils] [DEBUG] Fetched ${tokenIds.length} NFTs for owner ${ownerAddress} at contract ${contractAddress}`);
    }
    return tokenIds.map(id => ({ tokenId: id.toString(), balance: 1 }));
  } catch (_error) {
    log(`[utils] [ERROR] Failed to fetch NFTs for owner ${ownerAddress}: ${_error.message}`);
    throw _error;
  }
}

export async function getOwnersForContract(contractAddress, _abi, _fromBlock) {
  try {
    const nfts = await alchemy.nft.getNftsForContract(contractAddress, {
      contractAddress,
      withMetadata: false,
    });
    const owners = nfts.nfts.map(nft => ({
      tokenId: nft.tokenId,
      ownerAddress: nft.owner || '0x0000000000000000000000000000000000000000',
    }));
    if (DEBUG) {
      log(`[utils] [DEBUG] Fetched ${owners.length} owners for contract ${contractAddress}`);
    }
    return owners;
  } catch (_error) {
    log(`[utils] [ERROR] Failed to fetch owners for contract ${contractAddress}: ${_error.message}`);
    throw _error;
  }
}

export async function getTransactionReceipt(transactionHash) {
  const cacheKey = `element280_receipt_${transactionHash}`;
  let receipt = await getCache(cacheKey);
  if (receipt) {
    if (DEBUG) {
      log(`[utils] [DEBUG] Cache hit for transaction receipt: ${transactionHash}`);
    }
    return receipt;
  }

  try {
    receipt = await retry(() => client.getTransactionReceipt({ hash: transactionHash }));
    if (!receipt) {
      throw new Error('Transaction receipt not found');
    }
    await setCache(cacheKey, receipt, config.cache.nodeCache.stdTTL);
    if (DEBUG) {
      log(`[utils] [DEBUG] Fetched and cached transaction receipt: ${transactionHash}`);
    }
    return receipt;
  } catch (_error) {
    log(`[utils] [ERROR] Failed to fetch transaction receipt for ${transactionHash}: ${_error.message}`);
    throw _error;
  }
}

export async function retry(fn, retries = config.alchemy.maxRetries, delayMs = config.alchemy.retryMaxDelayMs / config.alchemy.maxRetries) {
  let lastError;
  for (let i = 0; i < retries; i++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error;
      log(`[utils] [ERROR] Retry ${i + 1}/${retries} failed: ${error.message}`);
      if (i < retries - 1) {
        await delay(delayMs);
      }
    }
  }
  throw lastError;
}

export function safeSerialize(obj) {
  return JSON.parse(JSON.stringify(obj, (key, value) => {
    if (typeof value === 'bigint') {
      return value.toString();
    }
    return value;
  }));
}// app/api/holders/Ascendant/route.js
}// ./app/api/holders/Ascendant/progress/route.js
import { getCacheState } from '@/app/api/holders/Ascendant/route';
}// app/api/holders/E280/route.js
}// app/api/holders/E280/progress/route.js
}// app/api/holders/Element369/route.js
}// ./app/api/holders/Element369/progress/route.js
import { getCacheState } from '@/app/api/holders/Element369/route';
