// app/api/holders/Ascendant/route.js
import { NextResponse } from 'next/server';
import { createPublicClient, http, formatUnits, getAddress } from 'viem';
import { mainnet } from 'viem/chains';
import config from '@/config.js';
import { client, getCache, setCache, loadCacheState, saveCacheState, getOwnersForContract, getNftsForOwner, log, batchMulticall, retry, safeSerialize } from '@/app/api/utils';
import ascendant from '@/abi/ascendantNFT.json';



const CONTRACT_ADDRESS = config.contractAddresses.ascendant.address;
const CACHE_TTL = config.cache.nodeCache.stdTTL;
const PAGE_SIZE = config.contractDetails.ascendant.pageSize;
const TIERS = config.contractTiers.ascendant;
const COLLECTION = 'ascendant';

let cacheState = {
  isPopulating: false,
  totalOwners: 0,
  totalNfts: 0,
  processedNfts: 0,
  step: 'idle',
  debugId: `state-${Math.random().toString(36).slice(2)}`,
};

export async function getCacheState(_address) {
  return {
    isCachePopulating: cacheState.isPopulating,
    totalOwners: cacheState.totalOwners,
    progressState: {
      step: cacheState.step,
      totalNfts: cacheState.totalNfts,
      processedNfts: cacheState.processedNfts,
    },
    debugId: cacheState.debugId,
  };
}

async function getAllHolders(page = 0, pageSize = PAGE_SIZE, _requestId = '') {
  const cacheKey = `ascendant_holders_${CONTRACT_ADDRESS}-${page}-${pageSize}`;

  try {
    let cached;
    if (cacheState.isPopulating) {
      log(`[Ascendant] [INFO] Cache is populating for ${cacheKey}`);
      return { message: 'Cache is populating', ...await getCacheState(CONTRACT_ADDRESS) };
    }
    cached = await getCache(cacheKey, COLLECTION);
    if (cached) {
      log(`[Ascendant] [INFO] Cache hit: ${cacheKey}`);
      return cached;
    }
    log(`[Ascendant] [INFO] Cache miss: ${cacheKey}`);
  } catch (cacheError) {
    log(`[Ascendant] [ERROR] Cache read error for ${cacheKey}: ${cacheError.message}`);
  }

  if (!CONTRACT_ADDRESS || !TIERS) {
    log(`[Ascendant] [VALIDATION] Config error: contractAddress=${CONTRACT_ADDRESS}, tiers=${JSON.stringify(TIERS)}`);
    throw new Error('Missing contract address or tiers');
  }

  cacheState = { ...cacheState, isPopulating: true, step: 'fetching_owners', processedNfts: 0, totalNfts: 0, totalOwners: 0 };
  await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);

  const owners = await retry(() => getOwnersForContract(CONTRACT_ADDRESS, ascendant.abi));
  if (!Array.isArray(owners)) {
    log(`[Ascendant] [ERROR] getOwnersForContract returned non-array: ${JSON.stringify(owners)}`);
    cacheState = { ...cacheState, isPopulating: false, step: 'error' };
    await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);
    throw new Error('Invalid owners data');
  }

  cacheState = { ...cacheState, step: 'filtering_owners', totalNfts: owners.length, totalOwners: new Set(owners.map(o => o.ownerAddress.toLowerCase())).size };
  await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);

  const burnAddress = '0x0000000000000000000000000000000000000000';
  const filteredOwners = owners.filter(
    owner => owner.ownerAddress && owner.ownerAddress.toLowerCase() !== burnAddress
  );

  const tokenOwnerMap = new Map();
  let totalTokens = 0;

  filteredOwners.forEach(owner => {
    if (!owner.ownerAddress) return;
    let wallet;
    try {
      wallet = getAddress(owner.ownerAddress);
    } catch (error) {
      log(`[Ascendant] [ERROR] Invalid wallet address: ${owner.ownerAddress}, Error: ${error.message}`);
      return;
    }
    const tokenId = Number(owner.tokenId);
    tokenOwnerMap.set(tokenId, wallet);
    totalTokens++;
  });

  cacheState = { ...cacheState, step: 'building_token_map', totalNfts: totalTokens };
  await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);

  const allTokenIds = Array.from(tokenOwnerMap.keys());
  const start = page * pageSize;
  const end = Math.min(start + pageSize, allTokenIds.length);
  const paginatedTokenIds = allTokenIds.slice(start, end);

  if (paginatedTokenIds.length === 0) {
    const result = {
      holders: [],
      totalTokens,
      totalShares: 0,
      toDistributeDay8: 0,
      toDistributeDay28: 0,
      toDistributeDay90: 0,
      pendingRewards: 0,
      page,
      pageSize,
      totalPages: Math.ceil(totalTokens / pageSize),
    };
    await setCache(cacheKey, result, CACHE_TTL, COLLECTION);
    cacheState = { ...cacheState, isPopulating: false, step: 'completed' };
    await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);
    return result;
  }

  cacheState = { ...cacheState, step: 'fetching_tiers' };
  await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);

  const tierCalls = paginatedTokenIds.map(tokenId => ({
    address: CONTRACT_ADDRESS,
    abi: config.abis.ascendant.main,
    functionName: 'getNFTAttribute',
    args: [BigInt(tokenId)],
  }));
  const recordCalls = paginatedTokenIds.map(tokenId => ({
    address: CONTRACT_ADDRESS,
    abi: config.abis.ascendant.main,
    functionName: 'userRecords',
    args: [BigInt(tokenId)],
  }));

  const [tierResults, recordResults] = await Promise.all([
    retry(() => batchMulticall(tierCalls, config.alchemy.batchSize)),
    retry(() => batchMulticall(recordCalls, config.alchemy.batchSize)),
  ]);

  cacheState = { ...cacheState, step: 'fetching_shares' };
  await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);

  const totalSharesRaw = await retry(() =>
    client.readContract({
      address: CONTRACT_ADDRESS,
      abi: config.abis.ascendant.main,
      functionName: 'totalShares',
    })
  );
  const totalShares = parseFloat(formatUnits(totalSharesRaw.toString(), 18));

  const toDistributeDay8Raw = await retry(() =>
    client.readContract({
      address: CONTRACT_ADDRESS,
      abi: config.abis.ascendant.main,
      functionName: 'toDistribute',
      args: [0],
    })
  );
  const toDistributeDay8 = parseFloat(formatUnits(toDistributeDay8Raw.toString(), 18));

  const toDistributeDay28Raw = await retry(() =>
    client.readContract({
      address: CONTRACT_ADDRESS,
      abi: config.abis.ascendant.main,
      functionName: 'toDistribute',
      args: [1],
    })
  );
  const toDistributeDay28 = parseFloat(formatUnits(toDistributeDay28Raw.toString(), 18));

  const toDistributeDay90Raw = await retry(() =>
    client.readContract({
      address: CONTRACT_ADDRESS,
      abi: config.abis.ascendant.main,
      functionName: 'toDistribute',
      args: [2],
    })
  );
  const toDistributeDay90 = parseFloat(formatUnits(toDistributeDay90Raw.toString(), 18));

  const maxTier = Math.max(...Object.keys(TIERS).map(Number));
  const holdersMap = new Map();

  cacheState = { ...cacheState, step: 'processing_holders', processedNfts: paginatedTokenIds.length };
  await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);

  const walletTokenIds = new Map();
  paginatedTokenIds.forEach(tokenId => {
    const wallet = tokenOwnerMap.get(tokenId);
    if (!wallet) return;
    if (!walletTokenIds.has(wallet)) {
      walletTokenIds.set(wallet, []);
    }
    walletTokenIds.get(wallet).push(tokenId);
  });

  const claimableCalls = Array.from(walletTokenIds.entries()).map(([wallet, tokenIds]) => ({
    address: CONTRACT_ADDRESS,
    abi: config.abis.ascendant.main,
    functionName: 'batchClaimableAmount',
    args: [tokenIds.map(id => BigInt(id))],
  }));

  const claimableResults = await retry(() => batchMulticall(claimableCalls, config.alchemy.batchSize));

  paginatedTokenIds.forEach((tokenId, i) => {
    const wallet = tokenOwnerMap.get(tokenId);
    if (!wallet) return;
    if (!holdersMap.has(wallet)) {
      holdersMap.set(wallet, {
        wallet,
        total: 0,
        multiplierSum: 0,
        tiers: Array(maxTier + 1).fill(0),
        shares: 0,
        lockedAscendant: 0,
        pendingDay8: 0,
        pendingDay28: 0,
        pendingDay90: 0,
        claimableRewards: 0,
      });
    }
    const holder = holdersMap.get(wallet);

    const tierResult = tierResults[i];
    let tier;
    if (tierResult?.status === 'success') {
      if (Array.isArray(tierResult.result) && tierResult.result.length >= 2) {
        tier = Number(tierResult.result[1]);
      } else if (typeof tierResult.result === 'object' && tierResult.result.tier !== undefined) {
        tier = Number(tierResult.result.tier);
      } else {
        log(`[Ascendant] [ERROR] Unexpected tier result format for token ${tokenId}: ${JSON.stringify(tierResult)}`);
      }
    } else {
      log(`[Ascendant] [ERROR] Tier fetch failed for token ${tokenId}: ${tierResult?.error || 'Unknown'}`);
    }
    if (tier >= 1 && tier <= maxTier) {
      holder.tiers[tier] += 1;
      holder.total += 1;
      holder.multiplierSum += TIERS[tier]?.multiplier || 0;
    }

    const recordResult = recordResults[i];
    if (recordResult?.status === 'success' && Array.isArray(recordResult.result)) {
      const sharesRaw = recordResult.result[0] || '0';
      const lockedAscendantRaw = recordResult.result[1] || '0';
      const shares = parseFloat(formatUnits(sharesRaw, 18));
      const lockedAscendant = parseFloat(formatUnits(lockedAscendantRaw, 18));
      holder.shares += shares;
      holder.lockedAscendant += lockedAscendant;
    } else {
      log(`[Ascendant] [ERROR] Record fetch failed for token ${tokenId}: ${recordResult?.error || 'Unknown'}`);
    }
  });

  let claimableIndex = 0;
  for (const [wallet, _tokenIds] of walletTokenIds.entries()) {
    const holder = holdersMap.get(wallet);
    if (!holder) {
      claimableIndex++;
      continue;
    }
    if (claimableResults[claimableIndex]?.status === 'success') {
      const claimableRaw = claimableResults[claimableIndex].result || '0';
      holder.claimableRewards = parseFloat(formatUnits(claimableRaw, 18));
    } else {
      log(`[Ascendant] [ERROR] Claimable fetch failed for wallet ${wallet}: ${claimableResults[claimableIndex]?.error || 'Unknown'}`);
    }
    claimableIndex++;
  }

  const holders = Array.from(holdersMap.values());
  const totalMultiplierSum = holders.reduce((sum, h) => sum + h.multiplierSum, 0);
  const pendingRewardPerShareDay8 = totalShares > 0 ? toDistributeDay8 / totalShares : 0;
  const pendingRewardPerShareDay28 = totalShares > 0 ? toDistributeDay28 / totalShares : 0;
  const pendingRewardPerShareDay90 = totalShares > 0 ? toDistributeDay90 / totalShares : 0;

  holders.forEach(holder => {
    holder.pendingDay8 = holder.shares * pendingRewardPerShareDay8;
    holder.pendingDay28 = holder.shares * pendingRewardPerShareDay28;
    holder.pendingDay90 = holder.shares * pendingRewardPerShareDay90;
    holder.percentage = totalMultiplierSum > 0 ? (holder.multiplierSum / totalMultiplierSum) * 100 : 0;
    holder.rank = 0;
    holder.displayMultiplierSum = holder.multiplierSum;
  });

  holders.sort((a, b) => b.shares - a.shares || b.multiplierSum - a.shares || b.total - a.total);
  holders.forEach((holder, index) => (holder.rank = index + 1));

  const result = {
    holders,
    totalTokens,
    totalShares,
    toDistributeDay8,
    toDistributeDay28,
    toDistributeDay90,
    pendingRewards: toDistributeDay8 + toDistributeDay28 + toDistributeDay90,
    page,
    pageSize,
    totalPages: Math.ceil(totalTokens / pageSize),
  };

  await setCache(cacheKey, result, CACHE_TTL, COLLECTION);
  cacheState = { ...cacheState, isPopulating: false, step: 'completed', totalOwners: holders.length };
  await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);

  return result;
}

async function getHolderData(wallet, _requestId = '') {
  const cacheKey = `ascendant_holder_${CONTRACT_ADDRESS}-${wallet.toLowerCase()}`;

  try {
    let cached;
    if (cacheState.isPopulating) {
      log(`[Ascendant] [INFO] Cache is populating for ${cacheKey}`);
      return { message: 'Cache is populating', ...await getCacheState(CONTRACT_ADDRESS) };
    }
    cached = await getCache(cacheKey, COLLECTION);
    if (cached) {
      log(`[Ascendant] [INFO] Cache hit: ${cacheKey}`);
      return cached;
    }
    log(`[Ascendant] [INFO] Cache miss: ${cacheKey}`);
  } catch (cacheError) {
    log(`[Ascendant] [ERROR] Cache read error for ${cacheKey}: ${cacheError.message}`);
  }

  if (!CONTRACT_ADDRESS || !TIERS) {
    log(`[Ascendant] [VALIDATION] Config error: contractAddress=${CONTRACT_ADDRESS}, tiers=${JSON.stringify(TIERS)}`);
    throw new Error('Missing contract address or tiers');
  }

  cacheState = { ...cacheState, isPopulating: true, step: 'fetching_nfts', processedNfts: 0, totalNfts: 0, totalOwners: 0 };
  await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);

  const nfts = await retry(() => getNftsForOwner(wallet.toLowerCase(), CONTRACT_ADDRESS, ascendant.abi));
  if (nfts.length === 0) {
    cacheState = { ...cacheState, isPopulating: false, step: 'completed' };
    await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);
    return null;
  }

  cacheState = { ...cacheState, step: 'processing_nfts', totalNfts: nfts.length, totalOwners: 1 };
  await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);

  const maxTier = Math.max(...Object.keys(TIERS).map(Number));
  const holder = {
    wallet: wallet.toLowerCase(),
    total: 0,
    multiplierSum: 0,
    tiers: Array(maxTier + 1).fill(0),
    shares: 0,
    lockedAscendant: 0,
    pendingDay8: 0,
    pendingDay28: 0,
    pendingDay90: 0,
    claimableRewards: 0,
    percentage: 0,
    rank: 0,
    displayMultiplierSum: 0,
  };

  const tokenIds = nfts.map(nft => BigInt(nft.tokenId));
  const tierCalls = tokenIds.map(tokenId => ({
    address: CONTRACT_ADDRESS,
    abi: config.abis.ascendant.main,
    functionName: 'getNFTAttribute',
    args: [tokenId],
  }));
  const recordCalls = tokenIds.map(tokenId => ({
    address: CONTRACT_ADDRESS,
    abi: config.abis.ascendant.main,
    functionName: 'userRecords',
    args: [tokenId],
  }));
  const claimableCall = {
    address: CONTRACT_ADDRESS,
    abi: config.abis.ascendant.main,
    functionName: 'batchClaimableAmount',
    args: [tokenIds],
  };

  cacheState = { ...cacheState, step: 'fetching_attributes' };
  await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);

  const [tierResults, recordResults, claimableResults] = await Promise.all([
    retry(() => batchMulticall(tierCalls, config.alchemy.batchSize)),
    retry(() => batchMulticall(recordCalls, config.alchemy.batchSize)),
    retry(() => batchMulticall([claimableCall], config.alchemy.batchSize)),
  ]);

  tierResults.forEach((result, i) => {
    if (result?.status === 'success') {
      let tier;
      if (Array.isArray(result.result) && result.result.length >= 2) {
        tier = Number(result.result[1]);
      } else if (typeof result.result === 'object' && result.result.tier !== undefined) {
        tier = Number(result.result.tier);
      } else {
        log(`[Ascendant] [ERROR] Unexpected tier result format for token ${tokenIds[i]}: ${JSON.stringify(result)}`);
        return;
      }
      if (tier >= 1 && tier <= maxTier) {
        holder.tiers[tier] += 1;
        holder.total += 1;
        holder.multiplierSum += TIERS[tier]?.multiplier || 0;
      }
    } else {
      log(`[Ascendant] [ERROR] Tier fetch failed for token ${tokenIds[i]}: ${result?.error || 'Unknown'}`);
    }
  });

  let totalShares = 0;
  recordResults.forEach((result, i) => {
    if (result?.status === 'success' && Array.isArray(result.result)) {
      const sharesRaw = result.result[0] || '0';
      const lockedAscendantRaw = result.result[1] || '0';
      const shares = parseFloat(formatUnits(sharesRaw, 18));
      const lockedAscendant = parseFloat(formatUnits(lockedAscendantRaw, 18));
      holder.shares += shares;
      holder.lockedAscendant += lockedAscendant;
      totalShares += shares;
    } else {
      log(`[Ascendant] [ERROR] Record fetch failed for token ${tokenIds[i]}: ${result?.error || 'Unknown'}`);
    }
  });

  if (claimableResults[0]?.status === 'success') {
    const claimableRaw = claimableResults[0].result || '0';
    holder.claimableRewards = parseFloat(formatUnits(claimableRaw, 18));
  } else {
    log(`[Ascendant] [ERROR] Claimable fetch failed for wallet ${wallet}: ${claimableResults[0]?.error || 'Unknown'}`);
  }

  cacheState = { ...cacheState, step: 'fetching_shares' };
  await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);

  const totalSharesRaw = await retry(() =>
    client.readContract({
      address: CONTRACT_ADDRESS,
      abi: config.abis.ascendant.main,
      functionName: 'totalShares',
    })
  );
  const totalMultiplierSum = parseFloat(formatUnits(totalSharesRaw.toString(), 18));

  const toDistributeDay8Raw = await retry(() =>
    client.readContract({
      address: CONTRACT_ADDRESS,
      abi: config.abis.ascendant.main,
      functionName: 'toDistribute',
      args: [0],
    })
  );
  const toDistributeDay8 = parseFloat(formatUnits(toDistributeDay8Raw.toString(), 18));

  const toDistributeDay28Raw = await retry(() =>
    client.readContract({
      address: CONTRACT_ADDRESS,
      abi: config.abis.ascendant.main,
      functionName: 'toDistribute',
      args: [1],
    })
  );
  const toDistributeDay28 = parseFloat(formatUnits(toDistributeDay28Raw.toString(), 18));

  const toDistributeDay90Raw = await retry(() =>
    client.readContract({
      address: CONTRACT_ADDRESS,
      abi: config.abis.ascendant.main,
      functionName: 'toDistribute',
      args: [2],
    })
  );
  const toDistributeDay90 = parseFloat(formatUnits(toDistributeDay90Raw.toString(), 18));

  const pendingRewardPerShareDay8 = totalShares > 0 ? toDistributeDay8 / totalShares : 0;
  const pendingRewardPerShareDay28 = totalShares > 0 ? toDistributeDay28 / totalShares : 0;
  const pendingRewardPerShareDay90 = totalShares > 0 ? toDistributeDay90 / totalShares : 0;

  holder.pendingDay8 = holder.shares * pendingRewardPerShareDay8;
  holder.pendingDay28 = holder.shares * pendingRewardPerShareDay28;
  holder.pendingDay90 = holder.shares * pendingRewardPerShareDay90;
  holder.percentage = totalMultiplierSum > 0 ? (holder.multiplierSum / totalMultiplierSum) * 100 : 0;
  holder.displayMultiplierSum = holder.multiplierSum;

  await setCache(cacheKey, holder, CACHE_TTL, COLLECTION);
  cacheState = { ...cacheState, isPopulating: false, step: 'completed' };
  await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);

  return holder;
}

export async function GET(request) {
  const _requestId = crypto.randomUUID();
  const { searchParams, pathname } = new URL(request.url);
  const page = parseInt(searchParams.get('page') || '0', 10);
  const pageSize = parseInt(searchParams.get('pageSize') || PAGE_SIZE, 10);
  const wallet = searchParams.get('wallet');

  if (pathname.endsWith('/progress')) {
    const state = await getCacheState(CONTRACT_ADDRESS);
    const progressPercentage = state.progressState.totalNfts > 0
      ? ((state.progressState.processedNfts / state.progressState.totalNfts) * 100).toFixed(1)
      : '0.0';
    return NextResponse.json({
      isPopulating: state.isCachePopulating,
      totalLiveHolders: state.totalOwners,
      totalOwners: state.totalOwners,
      phase: state.progressState.step.charAt(0).toUpperCase() + state.progressState.step.slice(1),
      progressPercentage,
    });
  }

  try {
    if (wallet) {
      const holder = await getHolderData(wallet, _requestId);
      if (!holder) {
        log(`[Ascendant] [ERROR] No holder data found for wallet ${wallet}`);
        return NextResponse.json({ message: 'No holder data found for wallet' }, { status: 404 });
      }
      return NextResponse.json(safeSerialize(holder));
    }

    const data = await getAllHolders(page, pageSize, _requestId);
    if (!data.holders || !Array.isArray(data.holders)) {
      log(`[Ascendant] [ERROR] Invalid holders data returned: ${JSON.stringify(data)}`);
      return NextResponse.json({ error: 'Invalid holders data' }, { status: 500 });
    }
    return NextResponse.json(safeSerialize(data));
  } catch (error) {
    log(`[Ascendant] [ERROR] GET error: ${error.message}, stack: ${error.stack}`);
    let status = 500;
    let message = 'Failed to fetch Ascendant data';
    if (error.message.includes('Rate limit')) {
      status = 429;
      message = 'Rate limit exceeded';
    }
    return NextResponse.json({ error: message, details: error.message }, { status });
  }
}

export async function POST(_request) {
  try {
    cacheState = { ...cacheState, isPopulating: true, step: 'starting' };
    await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);
    const data = await getAllHolders(0, PAGE_SIZE);
    return NextResponse.json({ message: 'Cache population triggered', ...data });
  } catch (error) {
    log(`[Ascendant] [ERROR] POST error: ${error.message}, stack: ${error.stack}`);
    cacheState = { ...cacheState, isPopulating: false, step: 'error' };
    await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);
    return NextResponse.json({ error: 'Failed to populate cache', details: error.message }, { status: 500 });
  }
}// app/api/holders/Ascendant/progress/route.js
import { NextResponse } from 'next/server';
import { log } from '@/app/api/utils';
import { getCacheState } from '@/app/api/holders/Ascendant/route';
import config from '@/config';

export async function GET() {
  const address = config.contractAddresses.ascendant.address;
  if (!address) {
    log(`[Ascendant] [VALIDATION] Ascendant contract address not found`);
    return NextResponse.json({ error: 'Ascendant contract address not found' }, { status: 400 });
  }

  try {
    const state = await getCacheState(address);
    if (!state || !state.progressState) {
      log(`[Ascendant] [VALIDATION] Invalid cache state for ${address}`);
      return NextResponse.json({ error: 'Cache state not initialized' }, { status: 500 });
    }
    const progressPercentage = state.progressState.totalNfts > 0
      ? ((state.progressState.processedNfts / state.progressState.totalNfts) * 100).toFixed(1)
      : '0.0';

    return NextResponse.json({
      isPopulating: state.isCachePopulating,
      totalLiveHolders: state.totalOwners,
      totalOwners: state.totalOwners,
      phase: state.progressState.step.charAt(0).toUpperCase() + state.progressState.step.slice(1),
      progressPercentage,
    });
  } catch (error) {
    log(`[Ascendant] [ERROR] Progress endpoint error: ${error.message}, stack: ${error.stack}`);
    return NextResponse.json({ error: `Server error: ${error.message}` }, { status: 500 });
  }
}// app/api/holders/E280/route.js
import { NextResponse } from 'next/server';
import { log, getCache } from '@/app/api/utils';

const COLLECTION = 'e280';

export async function GET(request) {
  const { searchParams } = new URL(request.url);
  const page = parseInt(searchParams.get('page') || '0', 10);
  const pageSize = parseInt(searchParams.get('pageSize') || '1000', 10);
  const wallet = searchParams.get('wallet')?.toLowerCase();

  log(`[E280] [INFO] GET Request: page=${page}, pageSize=${pageSize}, wallet=${wallet}`);

  const cacheKey = `e280_holders_${page}_${pageSize}_${wallet || 'all'}`;
  try {
    const cachedData = await getCache(cacheKey, COLLECTION);
    if (cachedData) {
      log(`[E280] [INFO] Cache hit: ${cacheKey}`);
      return NextResponse.json(cachedData);
    }
    log(`[E280] [INFO] Cache miss: ${cacheKey}`);
  } catch (cacheError) {
    log(`[E280] [ERROR] Cache read error: ${cacheError.message}, stack: ${cacheError.stack}`);
  }

  log(`[E280] [VALIDATION] Contract not yet deployed`);
  return NextResponse.json({ error: 'E280 contract not yet deployed' }, { status: 400 });
}

export async function POST(_request) {
  log(`[E280] [VALIDATION] POST: Contract not yet deployed`);
  return NextResponse.json({ error: 'E280 contract not yet deployed' }, { status: 400 });
}// app/api/holders/E280/progress/route.js
import { NextResponse } from 'next/server';
import { log } from '@/app/api/utils';

export async function GET(_request) {
  try {
    log(`[E280] [INFO] Progress: Contract not yet deployed`);
    return NextResponse.json({
      isPopulating: false,
      totalLiveHolders: 0,
      totalOwners: 0,
      phase: 'Not Deployed',
      progressPercentage: '0.0',
    });
  } catch (error) {
    log(`[E280] [ERROR] Progress endpoint error: ${error.message}, stack: ${error.stack}`);
    return NextResponse.json({ error: 'Failed to fetch progress state', details: error.message }, { status: 500 });
  }
}// app/api/holders/Element369/route.js
import { NextResponse } from 'next/server';
import config from '@/config.js';
import { getOwnersForContract, log, batchMulticall, getCache, setCache, loadCacheState, saveCacheState, safeSerialize } from '@/app/api/utils';

const CONTRACT_ADDRESS = config.contractAddresses.element369.address;
const VAULT_ADDRESS = config.vaultAddresses.element369.address;
const TIERS_CONFIG = config.contractDetails.element369.tiers;
const PAGE_SIZE = config.contractDetails.element369.pageSize;
const ELEMENT369_ABI = config.abis.element369.main;
const ELEMENT369_VAULT_ABI = config.abis.element369.vault;
const CACHE_TTL = config.cache.nodeCache.stdTTL;
const COLLECTION = 'element369';

let cacheState = {
  isPopulating: false,
  totalOwners: 0,
  totalNfts: 0,
  processedNfts: 0,
  step: 'idle',
  debugId: `state-${Math.random().toString(36).slice(2)}`,
};

export async function getCacheState(_address) {
  return {
    isCachePopulating: cacheState.isPopulating,
    totalOwners: cacheState.totalOwners,
    progressState: {
      step: cacheState.step,
      totalNfts: cacheState.totalNfts,
      processedNfts: cacheState.processedNfts,
    },
    debugId: cacheState.debugId,
  };
}

export async function GET(request) {
  const { searchParams, pathname } = new URL(request.url);
  const page = parseInt(searchParams.get('page') || '0', 10);
  const pageSize = parseInt(searchParams.get('pageSize') || PAGE_SIZE, 10);
  const wallet = searchParams.get('wallet')?.toLowerCase();

  if (!CONTRACT_ADDRESS || !VAULT_ADDRESS || !TIERS_CONFIG || !PAGE_SIZE) {
    log(`[Element369] [VALIDATION] Config error: contractAddress=${CONTRACT_ADDRESS}, vaultAddress=${VAULT_ADDRESS}, tiersConfig=${TIERS_CONFIG}, pageSize=${PAGE_SIZE}`);
    return NextResponse.json({ error: 'Element369 contract, vault address, tiers config, or page size missing' }, { status: 400 });
  }

  if (pathname.endsWith('/progress')) {
    const state = await getCacheState(CONTRACT_ADDRESS);
    const progressPercentage = state.progressState.totalNfts > 0
      ? ((state.progressState.processedNfts / state.progressState.totalNfts) * 100).toFixed(1)
      : '0.0';
    return NextResponse.json({
      isPopulating: state.isCachePopulating,
      totalLiveHolders: state.totalOwners,
      totalOwners: state.totalOwners,
      phase: state.progressState.step.charAt(0).toUpperCase() + state.progressState.step.slice(1),
      progressPercentage,
    });
  }

  log(`[Element369] Request: page=${page}, pageSize=${pageSize}, wallet=${wallet}, contract=${CONTRACT_ADDRESS}`);

  try {
    const cacheKey = `element369_holders_${page}_${pageSize}_${wallet || 'all'}`;
    let cachedData;
    try {
      if (cacheState.isPopulating) {
        log(`[Element369] [INFO] Waiting for cache population to complete`);
        return NextResponse.json({ message: 'Cache is populating', ...await getCacheState(CONTRACT_ADDRESS) });
      }
      cachedData = await getCache(cacheKey, COLLECTION);
      if (cachedData) {
        log(`[Element369] [INFO] Cache hit: ${cacheKey}`);
        return NextResponse.json(safeSerialize(cachedData));
      }
      log(`[Element369] [INFO] Cache miss: ${cacheKey}`);
    } catch (cacheError) {
      log(`[Element369] [ERROR] Cache read error: ${cacheError.message}`);
    }

    cacheState = { ...cacheState, isPopulating: true, step: 'fetching_owners', processedNfts: 0, totalNfts: 0, totalOwners: 0 };
    await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);
    log(`[Element369] Fetching owners...`);

    const owners = await getOwnersForContract(CONTRACT_ADDRESS, ELEMENT369_ABI);
    if (!Array.isArray(owners)) {
      log(`[Element369] [ERROR] getOwnersForContract returned non-array: ${JSON.stringify(owners)}`);
      cacheState = { ...cacheState, isPopulating: false, step: 'error' };
      await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);
      throw new Error('Invalid owners data');
    }

    cacheState = { ...cacheState, step: 'filtering_owners', totalNfts: owners.length, totalOwners: new Set(owners.map(o => o.ownerAddress.toLowerCase())).size };
    await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);

    const burnAddress = '0x0000000000000000000000000000000000000000';
    const filteredOwners = wallet
      ? owners.filter(owner => owner.ownerAddress.toLowerCase() === wallet && owner.ownerAddress.toLowerCase() !== burnAddress)
      : owners.filter(owner => owner.ownerAddress.toLowerCase() !== burnAddress);
    log(`[Element369] Live owners: ${filteredOwners.length}`);

    cacheState = { ...cacheState, step: 'building_token_map' };
    await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);

    const tokenOwnerMap = new Map();
    const ownerTokens = new Map();
    let totalTokens = 0;
    filteredOwners.forEach(owner => {
      const walletAddr = owner.ownerAddress.toLowerCase();
      const tokenId = owner.tokenId;
      tokenOwnerMap.set(tokenId, walletAddr);
      totalTokens++;
      const tokens = ownerTokens.get(walletAddr) || [];
      tokens.push(tokenId);
      ownerTokens.set(walletAddr, tokens);
    });
    log(`[Element369] Total tokens: ${totalTokens}, tokenOwnerMap size: ${tokenOwnerMap.size}`);

    cacheState = { ...cacheState, step: 'fetching_tiers' };
    await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);

    const allTokenIds = Array.from(tokenOwnerMap.keys());
    const start = page * pageSize;
    const end = Math.min(start + pageSize, allTokenIds.length);
    const paginatedTokenIds = allTokenIds.slice(start, end);
    log(`[Element369] Paginated tokens: ${paginatedTokenIds.length}`);

    const tierCalls = paginatedTokenIds.map(tokenId => ({
      address: CONTRACT_ADDRESS,
      abi: ELEMENT369_ABI,
      functionName: 'getNftTier',
      args: [BigInt(tokenId)],
    }));
    const tierResults = await batchMulticall(tierCalls);
    log(`[Element369] Tiers fetched for ${tierResults.length} tokens`);

    cacheState = { ...cacheState, step: 'processing_holders', processedNfts: tierResults.length };
    await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);

    const maxTier = Math.max(...Object.keys(TIERS_CONFIG).filter(key => !isNaN(key)).map(Number));
    const holdersMap = new Map();

    tierResults.forEach((result, i) => {
      if (result?.status === 'success') {
        const tokenId = paginatedTokenIds[i];
        const walletAddr = tokenOwnerMap.get(tokenId);
        const tier = Number(result.result);

        if (walletAddr && walletAddr !== burnAddress) {
          if (!holdersMap.has(walletAddr)) {
            holdersMap.set(walletAddr, {
              wallet: walletAddr,
              total: 0,
              multiplierSum: 0,
              tiers: Array(maxTier).fill(0),
              infernoRewards: 0,
              fluxRewards: 0,
              e280Rewards: 0,
            });
          }
          const holder = holdersMap.get(walletAddr);
          holder.total += 1;
          if (tier >= 1 && tier <= maxTier) {
            holder.multiplierSum += TIERS_CONFIG[tier]?.multiplier || 0;
            holder.tiers[tier - 1] += 1;
          } else {
            log(`[Element369] [ERROR] Invalid tier ${tier} for token ${tokenId}`);
            holder.multiplierSum += TIERS_CONFIG[1]?.multiplier || 0;
          }
        }
      } else {
        log(`[Element369] [ERROR] Tier fetch failed for token ${paginatedTokenIds[i]}: ${result?.error || 'Unknown'}`);
      }
    });

    let holders = Array.from(holdersMap.values());
    cacheState = { ...cacheState, step: 'fetching_rewards' };
    await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);

    const rewardCalls = holders.map(holder => {
      const tokenIds = ownerTokens.get(holder.wallet) || [];
      return {
        address: VAULT_ADDRESS,
        abi: ELEMENT369_VAULT_ABI,
        functionName: 'getRewards',
        args: [tokenIds.map(id => BigInt(id)), holder.wallet, false],
      };
    });

    log(`[Element369] Fetching rewards for ${holders.length} holders`);
    const rewardsResults = await batchMulticall(rewardCalls);

    holders.forEach((holder, i) => {
      if (rewardsResults[i]?.status === 'success' && rewardsResults[i].result) {
        const [, , infernoPool, fluxPool, e280Pool] = rewardsResults[i].result;
        holder.infernoRewards = Number(infernoPool) / 1e18;
        holder.fluxRewards = Number(fluxPool) / 1e18;
        holder.e280Rewards = Number(e280Pool) / 1e18;
      } else {
        holder.infernoRewards = 0;
        holder.fluxRewards = 0;
        holder.e280Rewards = 0;
        log(`[Element369] [ERROR] Reward fetch failed for ${holder.wallet.slice(0, 6)}...: ${rewardsResults[i]?.error || 'Unknown'}`);
      }
      holder.displayMultiplierSum = holder.multiplierSum;
      holder.percentage = 0;
      holder.rank = 0;
    });

    const totalMultiplierSum = holders.reduce((sum, h) => sum + h.multiplierSum, 0);
    holders.forEach((holder, index) => {
      holder.percentage = totalMultiplierSum > 0 ? (holder.multiplierSum / totalMultiplierSum) * 100 : 0;
      holder.rank = index + 1;
      holder.displayMultiplierSum = holder.multiplierSum;
    });

    holders.sort((a, b) => b.multiplierSum - a.multiplierSum || b.total - a.total);

    const response = {
      holders,
      totalTokens,
      page,
      pageSize,
      totalPages: wallet ? 1 : Math.ceil(totalTokens / pageSize),
    };

    await setCache(cacheKey, response, CACHE_TTL, COLLECTION);
    log(`[Element369] [INFO] Cached response: ${cacheKey}`);

    cacheState = { ...cacheState, isPopulating: false, step: 'completed', totalOwners: holders.length };
    await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);
    log(`[Element369] Success: ${holders.length} holders`);

    return NextResponse.json(safeSerialize(response));
  } catch (error) {
    log(`[Element369] [ERROR] GET error: ${error.message}, stack: ${error.stack}`);
    cacheState = { ...cacheState, isPopulating: false, step: 'error' };
    await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);
    let status = 500;
    let message = 'Failed to fetch Element369 data';
    if (error.message.includes('Rate limit')) {
      status = 429;
      message = 'Rate limit exceeded';
    }
    return NextResponse.json({ error: message, details: error.message }, { status });
  }
}

export async function POST(_request) {
  try {
    cacheState = { ...cacheState, isPopulating: true, step: 'starting' };
    await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);
    const cacheKey = `element369_holders_0_${PAGE_SIZE}_all`;
    const data = await getCache(cacheKey, COLLECTION) || { message: 'Cache population triggered' };
    return NextResponse.json(data);
  } catch (error) {
    log(`[Element369] [ERROR] POST error: ${error.message}, stack: ${error.stack}`);
    cacheState = { ...cacheState, isPopulating: false, step: 'error' };
    await saveCacheState(CONTRACT_ADDRESS, cacheState, COLLECTION);
    return NextResponse.json({ error: 'Failed to populate cache', details: error.message }, { status: 500 });
  }
}// app/api/holders/Element369/progress/route.js
import { NextResponse } from 'next/server';
import { log } from '@/app/api/utils';
import { getCacheState } from '@/app/api/holders/Element369/route';
import config from '@/config';

export async function GET() {
  const address = config.contractAddresses.element369.address;
  if (!address) {
    log(`[Element369] [VALIDATION] Element369 contract address not found`);
    return NextResponse.json({ error: 'Element369 contract address not found' }, { status: 400 });
  }

  try {
    const state = await getCacheState(address);
    if (!state || !state.progressState) {
      log(`[Element369] [VALIDATION] Invalid cache state for ${address}`);
      return NextResponse.json({ error: 'Cache state not initialized' }, { status: 500 });
    }
    const progressPercentage = state.progressState.totalNfts > 0
      ? ((state.progressState.processedNfts / state.progressState.totalNfts) * 100).toFixed(1)
      : '0.0';

    return NextResponse.json({
      isPopulating: state.isCachePopulating,
      totalLiveHolders: state.totalOwners,
      totalOwners: state.totalOwners,
      phase: state.progressState.step.charAt(0).toUpperCase() + state.progressState.step.slice(1),
      progressPercentage,
    });
  } catch (error) {
    log(`[Element369] [ERROR] Progress endpoint error: ${error.message}, stack: ${error.stack}`);
    return NextResponse.json({ error: `Server error: ${error.message}` }, { status: 500 });
  }
}// File: app/store.js

'use client';
import { create } from 'zustand';

const CACHE_TTL = 30 * 60 * 1000; // 30 minutes

export const useNFTStore = create((set, get) => ({
  cache: {},
  setCache: (contractKey, data) => {
    const key = `nft:${contractKey}`;
    console.log(`[NFTStore] Setting cache for ${key}: ${data.holders?.length || 0} holders`);
    set((state) => ({
      cache: {
        ...state.cache,
        [key]: { data, timestamp: Date.now() },
      },
    }));
  },
  getCache: (contractKey) => {
    const key = `nft:${contractKey}`;
    console.log(`[NFTStore] Getting cache for ${key}`);
    const cachedEntry = get().cache[key];
    if (!cachedEntry) {
      console.log(`[NFTStore] Cache miss for ${key}`);
      return null;
    }
    const now = Date.now();
    if (now - cachedEntry.timestamp > CACHE_TTL) {
      console.log(`[NFTStore] Cache expired for ${key}`);
      set((state) => {
        const newCache = { ...state.cache };
        delete newCache[key];
        return { cache: newCache };
      });
      return null;
    }
    console.log(`[NFTStore] Cache hit for ${key}: ${cachedEntry.data.holders?.length || 0} holders`);
    return cachedEntry.data;
  },
}));// config.js
import element280NftStatus from './element280_nft_status.json' assert { type: 'json' };
import element280MainAbi from './abi/element280.json' assert { type: 'json' };
import element280VaultAbi from './abi/element280Vault.json' assert { type: 'json' };
import element369MainAbi from './abi/element369.json' assert { type: 'json' };
import element369VaultAbi from './abi/element369Vault.json' assert { type: 'json' };
import staxMainAbi from './abi/staxNFT.json' assert { type: 'json' };
import staxVaultAbi from './abi/staxVault.json' assert { type: 'json' };
import ascendantMainAbi from './abi/ascendantNFT.json' assert { type: 'json' };
// E280 ABI placeholder (not deployed)
const e280MainAbi = [];

const config = {
  // Supported blockchain networks
  supportedChains: ['ETH', 'BASE'],

  // ABIs for all collections
  abis: {
    element280: {
      main: element280MainAbi,
      vault: element280VaultAbi,
    },
    element369: {
      main: element369MainAbi,
      vault: element369VaultAbi,
    },
    stax: {
      main: staxMainAbi,
      vault: staxVaultAbi,
    },
    ascendant: {
      main: ascendantMainAbi,
      vault: [], // No vault ABI provided for Ascendant
    },
    e280: {
      main: e280MainAbi,
      vault: [],
    },
  },

  // NFT contract configurations
  nftContracts: {
    element280: {
      name: 'Element 280',
      symbol: 'ELMNT',
      chain: 'ETH',
      address: '0x7F090d101936008a26Bf1F0a22a5f92fC0Cf46c9',
      vaultAddress: '0x44c4ADAc7d88f85d3D33A7f856Ebc54E60C31E97',
      deploymentBlock: '20945304',
      tiers: {
        1: { name: 'Common', multiplier: 10, allocation: '100000000000000000000000000' },
        2: { name: 'Common Amped', multiplier: 12, allocation: '100000000000000000000000000' },
        3: { name: 'Rare', multiplier: 100, allocation: '1000000000000000000000000000' },
        4: { name: 'Rare Amped', multiplier: 120, allocation: '1000000000000000000000000000' },
        5: { name: 'Legendary', multiplier: 1000, allocation: '10000000000000000000000000000' },
        6: { name: 'Legendary Amped', multiplier: 1200, allocation: '10000000000000000000000000000' },
      },
      description:
        'Element 280 NFTs can be minted with TitanX or ETH during a presale and redeemed for Element 280 tokens after a cooldown period. Multipliers contribute to a pool used for reward calculations.',
      expectedTotalSupply: 8107,
      expectedBurned: 8776,
      maxTokensPerOwnerQuery: 100,
    },
    element369: {
      name: 'Element 369',
      symbol: 'E369',
      chain: 'ETH',
      address: '0x024D64E2F65747d8bB02dFb852702D588A062575',
      vaultAddress: '0x4e3DBD6333e649AF13C823DAAcDd14f8507ECBc5',
      deploymentBlock: '21224418',
      tiers: {
        1: { name: 'Common', multiplier: 1, price: '100000000000000000000000000' },
        2: { name: 'Rare', multiplier: 10, price: '1000000000000000000000000000' },
        3: { name: 'Legendary', multiplier: 100, price: '10000000000000000000000000000' },
      },
      description:
        'Element 369 NFTs are minted with TitanX or ETH during specific sale cycles. Burning NFTs updates a multiplier pool and tracks burn cycles for reward distribution in the Holder Vault.',
    },
    stax: {
      name: 'Stax',
      symbol: 'STAX',
      chain: 'ETH',
      address: '0x74270Ca3a274B4dbf26be319A55188690CACE6E1',
      vaultAddress: '0x5D27813C32dD705404d1A78c9444dAb523331717',
      deploymentBlock: '21452667',
      tiers: {
        1: { name: 'Common', multiplier: 1, price: '100000000000000000000000000' },
        2: { name: 'Common Amped', multiplier: 1.2, price: '100000000000000000000000000', amplifier: '10000000000000000000000000' },
        3: { name: 'Common Super', multiplier: 1.4, price: '100000000000000000000000000', amplifier: '20000000000000000000000000' },
        4: { name: 'Common LFG', multiplier: 2, price: '100000000000000000000000000', amplifier: '50000000000000000000000000' },
        5: { name: 'Rare', multiplier: 10, price: '1000000000000000000000000000' },
        6: { name: 'Rare Amped', multiplier: 12, price: '1000000000000000000000000000', amplifier: '100000000000000000000000000' },
        7: { name: 'Rare Super', multiplier: 14, price: '1000000000000000000000000000', amplifier: '200000000000000000000000000' },
        8: { name: 'Rare LFG', multiplier: 20, price: '1000000000000000000000000000', amplifier: '500000000000000000000000000' },
        9: { name: 'Legendary', multiplier: 100, price: '10000000000000000000000000000' },
        10: { name: 'Legendary Amped', multiplier: 120, price: '10000000000000000000000000000', amplifier: '1000000000000000000000000000' },
        11: { name: 'Legendary Super', multiplier: 140, price: '10000000000000000000000000000', amplifier: '2000000000000000000000000000' },
        12: { name: 'Legendary LFG', multiplier: 200, price: '10000000000000000000000000000', amplifier: '5000000000000000000000000000' },
      },
      description:
        'Stax NFTs are minted with TitanX or ETH during a presale. Burning NFTs after a cooldown period claims backing rewards, with multipliers contributing to a pool for cycle-based reward calculations.',
    },
    ascendant: {
      name: 'Ascendant',
      symbol: 'ASCNFT',
      chain: 'ETH',
      address: '0x9da95c32c5869c84ba2c020b5e87329ec0adc97f',
      deploymentBlock: '21112535',
      tiers: {
        1: { name: 'Tier 1', price: '7812500000000000000000', multiplier: 1.01 },
        2: { name: 'Tier 2', price: '15625000000000000000000', multiplier: 1.02 },
        3: { name: 'Tier 3', price: '31250000000000000000000', multiplier: 1.03 },
        4: { name: 'Tier 4', price: '62500000000000000000000', multiplier: 1.04 },
        5: { name: 'Tier 5', price: '125000000000000000000000', multiplier: 1.05 },
        6: { name: 'Tier 6', price: '250000000000000000000000', multiplier: 1.06 },
        7: { name: 'Tier 7', price: '500000000000000000000000', multiplier: 1.07 },
        8: { name: 'Tier 8', price: '1000000000000000000000000', multiplier: 1.08 },
      },
      description:
        'Ascendant NFTs are minted with ASCENDANT tokens and offer staking rewards from DragonX pools over 8, 28, and 90-day periods. Features fusion mechanics to combine same-tier NFTs into higher tiers.',
    },
    e280: {
      name: 'E280',
      symbol: 'E280',
      chain: 'BASE',
      address: null,
      deploymentBlock: null,
      tiers: {},
      description: 'E280 NFTs on BASE chain. Contract not yet deployed.',
      disabled: true,
    },
  },

  // Contract addresses
  contractAddresses: {
    element280: { chain: 'ETH', address: '0x7F090d101936008a26Bf1F0a22a5f92fC0Cf46c9' },
    element369: { chain: 'ETH', address: '0x024D64E2F65747d8bB02dFb852702D588A062575' },
    stax: { chain: 'ETH', address: '0x74270Ca3a274B4dbf26be319A55188690CACE6E1' },
    ascendant: { chain: 'ETH', address: '0x9da95c32c5869c84ba2c020b5e87329ec0adc97f' },
    e280: { chain: 'BASE', address: null },
  },

  // Vault addresses
  vaultAddresses: {
    element280: { chain: 'ETH', address: '0x44c4ADAc7d88f85d3D33A7f856Ebc54E60C31E97' },
    element369: { chain: 'ETH', address: '0x4e3DBD6333e649AF13C823DAAcDd14f8507ECBc5' },
    stax: { chain: 'ETH', address: '0x5D27813C32dD705404d1A78c9444dAb523331717' },
    e280: { chain: 'BASE', address: null },
  },

  // Deployment blocks
  deploymentBlocks: {
    element280: { chain: 'ETH', block: '20945304' },
    element369: { chain: 'ETH', block: '21224418' },
    stax: { chain: 'ETH', block: '21452667' },
    ascendant: { chain: 'ETH', block: '21112535' },
    e280: { chain: 'BASE', block: null },
  },

  // Contract tiers
  contractTiers: {
    element280: {
      1: { name: 'Common', multiplier: 10 },
      2: { name: 'Common Amped', multiplier: 12 },
      3: { name: 'Rare', multiplier: 100 },
      4: { name: 'Rare Amped', multiplier: 120 },
      5: { name: 'Legendary', multiplier: 1000 },
      6: { name: 'Legendary Amped', multiplier: 1200 },
    },
    element369: {
      1: { name: 'Common', multiplier: 1 },
      2: { name: 'Rare', multiplier: 10 },
      3: { name: 'Legendary', multiplier: 100 },
      tierOrder: [
        { tierId: '3', name: 'Legendary' },
        { tierId: '2', name: 'Rare' },
        { tierId: '1', name: 'Common' },
      ],
    },
    stax: {
      1: { name: 'Common', multiplier: 1 },
      2: { name: 'Common Amped', multiplier: 1.2 },
      3: { name: 'Common Super', multiplier: 1.4 },
      4: { name: 'Common LFG', multiplier: 2 },
      5: { name: 'Rare', multiplier: 10 },
      6: { name: 'Rare Amped', multiplier: 12 },
      7: { name: 'Rare Super', multiplier: 14 },
      8: { name: 'Rare LFG', multiplier: 20 },
      9: { name: 'Legendary', multiplier: 100 },
      10: { name: 'Legendary Amped', multiplier: 120 },
      11: { name: 'Legendary Super', multiplier: 140 },
      12: { name: 'Legendary LFG', multiplier: 200 },
    },
    ascendant: {
      1: { name: 'Tier 1', multiplier: 1.01 },
      2: { name: 'Tier 2', multiplier: 1.02 },
      3: { name: 'Tier 3', multiplier: 1.03 },
      4: { name: 'Tier 4', multiplier: 1.04 },
      5: { name: 'Tier 5', multiplier: 1.05 },
      6: { name: 'Tier 6', multiplier: 1.06 },
      7: { name: 'Tier 7', multiplier: 1.07 },
      8: { name: 'Tier 8', multiplier: 1.08 },
    },
    e280: {},
  },

  // Contract details
  contractDetails: {
    element280: {
      name: 'Element 280',
      chain: 'ETH',
      pageSize: 100,
      apiEndpoint: '/api/holders/Element280',
      rewardToken: 'ELMNT',
    },
    element369: {
      name: 'Element 369',
      chain: 'ETH',
      pageSize: 1000,
      apiEndpoint: '/api/holders/Element369',
      rewardToken: 'INFERNO/FLUX/E280',
    },
    stax: {
      name: 'Stax',
      chain: 'ETH',
      pageSize: 1000,
      apiEndpoint: '/api/holders/Stax',
      rewardToken: 'X28',
    },
    ascendant: {
      name: 'Ascendant',
      chain: 'ETH',
      pageSize: 1000,
      apiEndpoint: '/api/holders/Ascendant',
      rewardToken: 'DRAGONX',
    },
    e280: {
      name: 'E280',
      chain: 'BASE',
      pageSize: 1000,
      apiEndpoint: '/api/holders/E280',
      rewardToken: 'E280',
      disabled: true,
    },
  },

  // Utility function to get contract details by name
  getContractDetails: (contractName) => {
    return config.nftContracts[contractName] || null;
  },

  alchemy: {
    apiKey: process.env.ALCHEMY_API_KEY || process.env.NEXT_PUBLIC_ALCHEMY_API_KEY,
    network: 'eth-mainnet',
    batchSize: 10,
    batchDelayMs: 1000,
    retryMaxDelayMs: 30000,
    maxRetries: 3,
    timeoutMs: 30000,
  },
  
  // Cache settings
  cache: {
    redis: {
      disableElement280: process.env.DISABLE_ELEMENT280_REDIS === 'true',
      disableElement369: process.env.DISABLE_ELEMENT369_REDIS === 'true',
      disableStax: process.env.DISABLE_STAX_REDIS === 'true',
      disableAscendant: process.env.DISABLE_ASCENDANT_REDIS === 'true',
      disableE280: process.env.DISABLE_E280_REDIS === 'true' || true,
    },
    nodeCache: {
      stdTTL: 3600,
      checkperiod: 120,
    },
  },

  // Debug settings
  debug: {
    enabled: process.env.DEBUG === 'true',
    logLevel: 'debug',
  },

  // Fallback data (optional, for testing)
  fallbackData: {
    element280: process.env.USE_FALLBACK_DATA === 'true' ? element280NftStatus : null,
  },
};

export default config;import NodeCache from 'node-cache';
import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';
import pino from 'pino';
import { Redis } from '@upstash/redis';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const logger = pino({
  level: process.env.LOG_LEVEL || 'info',
  transport: { target: 'pino-pretty' },
});

const cache = new NodeCache({
  stdTTL: 0, // No expiration
  checkperiod: 120,
});

const cacheDir = path.join(__dirname, '../../cache');
const redisEnabled = process.env.DISABLE_STAX_REDIS !== 'true' && process.env.UPSTASH_REDIS_REST_URL && process.env.UPSTASH_REDIS_REST_TOKEN;
let redis = null;

if (redisEnabled) {
  try {
    redis = new Redis({
      url: process.env.UPSTASH_REDIS_REST_URL,
      token: process.env.UPSTASH_REDIS_REST_TOKEN,
    });
    logger.info('utils', 'Upstash Redis initialized');
  } catch (error) {
    logger.error('utils', `Failed to initialize Upstash Redis: ${error.message}`, { stack: error.stack });
    redis = null;
  }
}

// Ensure cache directory exists
async function ensureCacheDir() {
  try {
    await fs.mkdir(cacheDir, { recursive: true });
    await fs.chmod(cacheDir, 0o755);
    logger.debug('utils', `Ensured cache directory: ${cacheDir}`);
  } catch (error) {
    logger.error('utils', `Failed to create/chmod cache directory ${cacheDir}: ${error.message}`, { stack: error.stack });
    throw error;
  }
}

// Initialize cache from disk or Redis
async function initializeCache() {
  try {
    if (redisEnabled && redis) {
      try {
        const data = await redis.get('stax_stax_holders');
        if (data) {
          const parsed = typeof data === 'string' ? JSON.parse(data) : data;
          if (parsed && Array.isArray(parsed.holders) && Number.isInteger(parsed.totalBurned)) {
            const success = cache.set('stax_stax_holders', parsed);
            logger.info('utils', `Initialized cache from Redis: stax_holders, success: ${success}, holders: ${parsed.holders.length}`);
            return true;
          } else {
            logger.warn('utils', 'Invalid cache data in Redis for stax_holders');
          }
        }
      } catch (error) {
        logger.error('utils', `Failed to initialize cache from Redis: ${error.message}`, { stack: error.stack });
      }
    }
    
    await ensureCacheDir();
    const cacheFile = path.join(cacheDir, 'stax_holders.json');
    try {
      const data = await fs.readFile(cacheFile, 'utf8');
      const parsed = JSON.parse(data);
      if (parsed && Array.isArray(parsed.holders) && Number.isInteger(parsed.totalBurned)) {
        const success = cache.set('stax_stax_holders', parsed);
        logger.info('utils', `Initialized cache from disk: stax_holders, success: ${success}, holders: ${parsed.holders.length}`);
        return true;
      } else {
        logger.warn('utils', `Invalid cache data in ${cacheFile}`);
      }
    } catch (error) {
      if (error.code !== 'ENOENT') {
        logger.error('utils', `Failed to read cache from ${cacheFile}: ${error.message}`, { stack: error.stack });
      } else {
        logger.debug('utils', `No cache file at ${cacheFile}`);
      }
    }
    return false;
  } catch (error) {
    logger.error('utils', `Cache initialization error: ${error.message}`, { stack: error.stack });
    return false;
  }
}

// Initialize cache on module load
initializeCache().catch(error => {
  logger.error('utils', `Cache initialization failed: ${error.message}`, { stack: error.stack });
});

async function setCache(key, value, ttl, prefix) {
  try {
    const cacheKey = `${prefix}_${key}`;
    const success = cache.set(cacheKey, value); // No TTL
    logger.debug('utils', `Set cache: ${cacheKey}, success: ${success}, holders: ${value.holders?.length || 'unknown'}`);
    
    if (key === 'stax_holders' && prefix === 'stax') {
      if (redisEnabled && redis) {
        try {
          await redis.set('stax_stax_holders', JSON.stringify(value));
          logger.info('utils', `Persisted stax_holders to Redis, holders: ${value.holders.length}`);
        } catch (error) {
          logger.error('utils', `Failed to persist stax_holders to Redis: ${error.message}`, { stack: error.stack });
        }
      } else {
        const cacheFile = path.join(cacheDir, 'stax_holders.json');
        await ensureCacheDir();
        await fs.writeFile(cacheFile, JSON.stringify(value));
        await fs.chmod(cacheFile, 0o644);
        logger.info('utils', `Persisted stax_holders to ${cacheFile}, holders: ${value.holders.length}`);
      }
    }
    
    return success;
  } catch (error) {
    logger.error('utils', `Failed to set cache ${prefix}_${key}: ${error.message}`, { stack: error.stack });
    return false;
  }
}

async function getCache(key, prefix) {
  try {
    const cacheKey = `${prefix}_${key}`;
    let data = cache.get(cacheKey);
    if (data !== undefined) {
      logger.debug('utils', `Cache hit: ${cacheKey}, holders: ${data.holders?.length || 'unknown'}`);
      return data;
    }

    if (key === 'stax_holders' && prefix === 'stax') {
      if (redisEnabled && redis) {
        try {
          const redisData = await redis.get('stax_stax_holders');
          if (redisData) {
            const parsed = typeof redisData === 'string' ? JSON.parse(redisData) : redisData;
            if (parsed && Array.isArray(parsed.holders) && Number.isInteger(parsed.totalBurned)) {
              const success = cache.set(cacheKey, parsed);
              logger.info('utils', `Loaded stax_holders from Redis, cached: ${success}, holders: ${parsed.holders.length}`);
              return parsed;
            } else {
              logger.warn('utils', `Invalid data in Redis for ${cacheKey}`);
            }
          }
        } catch (error) {
          logger.error('utils', `Failed to load cache from Redis: ${error.message}`, { stack: error.stack });
        }
      }
      
      const cacheFile = path.join(cacheDir, 'stax_holders.json');
      try {
        const fileData = await fs.readFile(cacheFile, 'utf8');
        const parsed = JSON.parse(fileData);
        if (parsed && Array.isArray(parsed.holders) && Number.isInteger(parsed.totalBurned)) {
          const success = cache.set(cacheKey, parsed);
          logger.info('utils', `Loaded stax_holders from ${cacheFile}, cached: ${success}, holders: ${parsed.holders.length}`);
          return parsed;
        } else {
          logger.warn('utils', `Invalid data in ${cacheFile}`);
        }
      } catch (error) {
        if (error.code !== 'ENOENT') {
          logger.error('utils', `Failed to load cache from ${cacheFile}: ${error.message}`, { stack: error.stack });
        } else {
          logger.debug('utils', `No cache file at ${cacheFile}`);
        }
      }
    }

    logger.info('utils', `Cache miss: ${cacheKey}`);
    return null;
  } catch (error) {
    logger.error('utils', `Failed to get cache ${prefix}_${key}: ${error.message}`, { stack: error.stack });
    return null;
  }
}

async function saveCacheState(collection, state, prefix) {
  try {
    const cacheFile = path.join(cacheDir, `cache_state_${prefix.toLowerCase()}.json`);
    await ensureCacheDir();
    await fs.writeFile(cacheFile, JSON.stringify(state));
    await fs.chmod(cacheFile, 0o644);
    logger.debug('utils', `Saved cache state for ${prefix}: ${cacheFile}`);
  } catch (error) {
    logger.error('utils', `Failed to save cache state for ${prefix}: ${error.message}`, { stack: error.stack });
  }
}

async function loadCacheState(collection, prefix) {
  try {
    const cacheFile = path.join(cacheDir, `cache_state_${prefix.toLowerCase()}.json`);
    const data = await fs.readFile(cacheFile, 'utf8');
    const parsed = JSON.parse(data);
    logger.debug('utils', `Loaded cache state for ${prefix}: ${cacheFile}`);
    return parsed;
  } catch (error) {
    if (error.code === 'ENOENT') {
      logger.debug('utils', `No cache state found for ${prefix}`);
      return null;
    }
    logger.error('utils', `Failed to load cache state for ${prefix}: ${error.message}`, { stack: error.stack });
    return null;
  }
}

// Mock client and retry
const client = {
  getBlockNumber: async () => BigInt(22382116),
  getLogs: async () => [],
  readContract: async () => null,
};

function retry(operation, { retries }) {
  return operation();
}

export { client, retry, logger, getCache, setCache, saveCacheState, loadCacheState };