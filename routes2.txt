// app/api/holders/Element280/route.js
import { NextResponse } from 'next/server';
import { log, saveCacheState, getCache, setCache, loadCacheState, batchMulticall, safeSerialize, getOwnersForContract, getNftsForOwner } from '@/app/api/utils';
import config from '@/config';
import { client } from '@/app/api/utils';
import pLimit from 'p-limit';
import { parseAbiItem } from 'viem';
import element280 from '@/abi/element280.json';

const CONTRACT_ADDRESS = config.contractAddresses.element280.address;
const CACHE_TTL = config.cache.nodeCache.stdTTL;
const HOLDERS_CACHE_KEY = 'element280_holders_map';
const TOKEN_CACHE_KEY = 'element280_token_cache';
const BURNED_EVENTS_CACHE_KEY = 'element280_burned_events';
const COLLECTION = 'element280';

export async function getCacheState(contractAddress) {
  try {
    const state = await loadCacheState(contractAddress, COLLECTION);
    return state || {
      isCachePopulating: false,
      holdersMapCache: null,
      totalOwners: 0,
      progressState: { step: 'idle', processedNfts: 0, totalNfts: 0 },
    };
  } catch (error) {
    log(`[Element280] [ERROR] Error fetching cache state for ${contractAddress}: ${error.message}`);
    return {
      isCachePopulating: false,
      holdersMapCache: null,
      totalOwners: 0,
      progressState: { step: 'error', processedNfts: 0, totalNfts: 0 },
    };
  }
}

async function getBurnedCountFromEvents(contractAddress, errorLog) {
  const burnAddress = '0x0000000000000000000000000000000000000000';
  const cacheKey = `${BURNED_EVENTS_CACHE_KEY}_${contractAddress}`;
  let cachedBurned = await getCache(cacheKey, COLLECTION);

  if (cachedBurned) {
    log(`[Element280] [INFO] Cache hit for burned events: ${cacheKey}`);
    return cachedBurned.count;
  }

  let burnedCount = 0;
  const endBlock = await client.getBlockNumber();
  const limit = pLimit(2);
  const ranges = [];
  for (let fromBlock = BigInt(config.deploymentBlocks.element280.block); fromBlock <= endBlock; fromBlock += BigInt(config.nftContracts.element280.maxTokensPerOwnerQuery)) {
    const toBlock = BigInt(Math.min(Number(fromBlock) + config.nftContracts.element280.maxTokensPerOwnerQuery - 1, Number(endBlock)));
    ranges.push({ fromBlock, toBlock });
  }

  try {
    await Promise.all(
      ranges.map(({ fromBlock, toBlock }) =>
        limit(async () => {
          const logs = await client.getLogs({
            address: contractAddress,
            event: parseAbiItem('event Transfer(address indexed from, address indexed to, uint256 indexed tokenId)'),
            fromBlock,
            toBlock,
          });
          const burns = logs.filter(log => log.args.to.toLowerCase() === burnAddress);
          burnedCount += burns.length;
        })
      )
    );

    const cacheData = { count: burnedCount, timestamp: Date.now() };
    await setCache(cacheKey, cacheData, CACHE_TTL, COLLECTION);
    log(`[Element280] [INFO] Cached burned events: ${cacheKey}, count: ${burnedCount}`);
    return burnedCount;
  } catch (error) {
    log(`[Element280] [ERROR] Failed to fetch burned events for ${contractAddress}: ${error.message}`);
    errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_burned_events', error: error.message });
    throw error;
  }
}

async function getTotalSupply(contractAddress, errorLog) {
  const cacheKey = `element280_total_supply_${contractAddress}`;
  const cached = await getCache(cacheKey, COLLECTION);
  if (cached) {
    log(`[Element280] [INFO] Cache hit for total supply: ${cacheKey}`);
    return { totalSupply: cached.totalSupply, totalBurned: cached.totalBurned };
  }

  try {
    const results = await batchMulticall([
      { address: contractAddress, abi: config.abis.element280.main, functionName: 'totalSupply' },
    ]);
    const totalSupply = results[0].status === 'success' ? Number(results[0].result) : 0;
    if (isNaN(totalSupply)) {
      const errorMsg = `Invalid totalSupply=${totalSupply}`;
      log(`[Element280] [ERROR] ${errorMsg}`);
      errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_total_supply', error: errorMsg });
      throw new Error(errorMsg);
    }

    const totalBurned = await getBurnedCountFromEvents(contractAddress, errorLog);
    if (totalBurned < 0) {
      const errorMsg = `Invalid totalBurned=${totalBurned} from events`;
      log(`[Element280] [ERROR] ${errorMsg}`);
      errorLog.push({ timestamp: new Date().toISOString(), phase: 'validate_burned', error: errorMsg });
      throw new Error(errorMsg);
    }

    if (totalSupply + totalBurned > config.nftContracts.element280.expectedTotalSupply + config.nftContracts.element280.expectedBurned) {
      const errorMsg = `Invalid data: totalSupply (${totalSupply}) + totalBurned (${totalBurned}) exceeds totalMinted (${config.nftContracts.element280.expectedTotalSupply + config.nftContracts.element280.expectedBurned})`;
      log(`[Element280] [ERROR] ${errorMsg}`);
      errorLog.push({ timestamp: new Date().toISOString(), phase: 'validate_supply', error: errorMsg });
      throw new Error(errorMsg);
    }

    const expectedBurned = config.nftContracts.element280.expectedBurned;
    if (Math.abs(totalBurned - expectedBurned) > 100) {
      log(`[Element280] [VALIDATION] Event-based totalBurned=${totalBurned} deviates from expected=${expectedBurned}.`);
    }

    const cacheData = { totalSupply, totalBurned };
    await setCache(cacheKey, cacheData, CACHE_TTL, COLLECTION);
    log(`[Element280] [INFO] Cached total supply: ${cacheKey}, supply: ${totalSupply}, burned: ${totalBurned}`);
    return cacheData;
  } catch (error) {
    log(`[Element280] [ERROR] Failed to fetch total supply for ${contractAddress}: ${error.message}`);
    errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_total_supply', error: error.message });
    throw error;
  }
}

async function fetchAllNftOwnership(contractAddress, errorLog, timings) {
  const ownershipByToken = new Map();
  const ownershipByWallet = new Map();
  const burnAddress = '0x0000000000000000000000000000000000000000';
  const failedTokens = new Set();

  if (!contractAddress || !/^0x[a-fA-F0-9]{40}$/.test(contractAddress)) {
    const errorMsg = `Invalid contract address: ${contractAddress}`;
    log(`[Element280] [ERROR] ${errorMsg}`);
    errorLog.push({ timestamp: new Date().toISOString(), phase: 'validate_contract', error: errorMsg });
    throw new Error(errorMsg);
  }

  const tokenIdStart = Date.now();
  const owners = await getOwnersForContract(contractAddress, element280.abi);
  if (!Array.isArray(owners)) {
    const errorMsg = `getOwnersForContract returned non-array: ${JSON.stringify(owners)}`;
    log(`[Element280] [ERROR] ${errorMsg}`);
    errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_token_ids', error: errorMsg });
    throw new Error(errorMsg);
  }
  timings.tokenIdFetch = Date.now() - tokenIdStart;

  const ownerFetchStart = Date.now();
  const ownerCalls = owners.map(owner => ({
    address: contractAddress,
    abi: config.abis.element280.main,
    functionName: 'ownerOf',
    args: [BigInt(owner.tokenId)],
  }));
  const ownerResults = await batchMulticall(ownerCalls);
  owners.forEach((owner, index) => {
    const tokenId = owner.tokenId;
    const ownerAddr = owner.ownerAddress.toLowerCase();
    const verifiedOwner = ownerResults[index]?.status === 'success' ? ownerResults[index].result.toLowerCase() : null;
    if (verifiedOwner && verifiedOwner === ownerAddr && ownerAddr !== burnAddress) {
      ownershipByToken.set(tokenId, ownerAddr);
      const walletTokens = ownershipByWallet.get(ownerAddr) || [];
      walletTokens.push(tokenId);
      ownershipByWallet.set(ownerAddr, walletTokens);
    } else {
      failedTokens.add(tokenId);
      if (!verifiedOwner) {
        log(`[Element280] [VALIDATION] Failed to verify owner for token ${tokenId}`);
      } else if (verifiedOwner !== ownerAddr) {
        log(`[Element280] [VALIDATION] Owner mismatch for token ${tokenId}: event=${ownerAddr}, ownerOf=${verifiedOwner}`);
      }
    }
  });
  timings.ownerFetch = Date.now() - ownerFetchStart;
  timings.ownerProcess = timings.ownerFetch;

  const { totalSupply } = await getTotalSupply(contractAddress, errorLog);
  if (ownershipByToken.size > totalSupply) {
    const errorMsg = `Found ${ownershipByToken.size} live NFTs, more than totalSupply ${totalSupply}`;
    log(`[Element280] [ERROR] ${errorMsg}`);
    errorLog.push({ timestamp: new Date().toISOString(), phase: 'validate_ownership', error: errorMsg });
    throw new Error(errorMsg);
  }
  if (ownershipByToken.size === 0 && totalSupply > 0) {
    const errorMsg = `No valid NFTs with owners found for contract ${contractAddress}, expected up to ${totalSupply}`;
    log(`[Element280] [ERROR] ${errorMsg}`);
    errorLog.push({ timestamp: new Date().toISOString(), phase: 'validate_ownership', error: errorMsg });
    throw new Error(errorMsg);
  }

  return { ownershipByToken, ownershipByWallet, totalSupply };
}

async function populateHoldersMapCache(contractAddress) {
  let state = await getCacheState(contractAddress);
  if (state.isCachePopulating) {
    log(`[Element280] [INFO] Cache population already in progress for ${contractAddress}`);
    return;
  }

  state.isCachePopulating = true;
  state.progressState = { step: 'fetching_supply', processedNfts: 0, totalNfts: 0 };
  await saveCacheState(contractAddress, state, COLLECTION);

  const timings = {
    totalSupply: 0,
    tokenIdFetch: 0,
    ownerFetch: 0,
    ownerProcess: 0,
    holderInit: 0,
    tierFetch: 0,
    rewardFetch: 0,
    metricsCalc: 0,
    total: 0,
  };
  const errorLog = [];
  const totalStart = Date.now();
  let holdersMap = new Map();

  try {
    const supplyStart = Date.now();
    const { ownershipByToken, ownershipByWallet, totalSupply } = await fetchAllNftOwnership(contractAddress, errorLog, timings);
    timings.totalSupply = Date.now() - supplyStart;
    state.progressState = { step: 'fetching_ownership', processedNfts: 0, totalNfts: totalSupply };
    await saveCacheState(contractAddress, state, COLLECTION);

    const holderInitStart = Date.now();
    ownershipByWallet.forEach((tokenIds, wallet) => {
      const holder = {
        wallet,
        total: tokenIds.length,
        totalLive: tokenIds.length,
        multiplierSum: 0,
        displayMultiplierSum: 0,
        tiers: Array(6).fill(0),
        tokenIds: tokenIds.map(id => BigInt(id)),
        claimableRewards: 0,
        percentage: 0,
        rank: 0,
      };
      holdersMap.set(wallet, holder);
      setCache(`${TOKEN_CACHE_KEY}_${contractAddress}-${wallet}-nfts`, tokenIds.map(id => ({ tokenId: id, tier: 0 })), CACHE_TTL, COLLECTION);
    });
    timings.holderInit = Date.now() - holderInitStart;
    state.totalOwners = holdersMap.size;
    state.progressState = { step: 'fetching_tiers', processedNfts: ownershipByToken.size, totalNfts: totalSupply };
    await saveCacheState(contractAddress, state, COLLECTION);

    const tierFetchStart = Date.now();
    const allTokenIds = Array.from(ownershipByToken.keys()).map(id => BigInt(id));
    const tierCalls = allTokenIds.map(tokenId => ({
      address: contractAddress,
      abi: config.abis.element280.main,
      functionName: 'getNftTier',
      args: [tokenId],
    }));
    if (tierCalls.length > 0) {
      const limit = pLimit(config.alchemy.batchSize);
      const chunkSize = config.nftContracts.element280.maxTokensPerOwnerQuery;
      const tierResults = [];
      for (let i = 0; i < tierCalls.length; i += chunkSize) {
        const chunk = tierCalls.slice(i, i + chunkSize);
        const results = await limit(() => batchMulticall(chunk));
        tierResults.push(...results);
        state.progressState = {
          step: 'fetching_tiers',
          processedNfts: Math.min(ownershipByToken.size, i + chunkSize),
          totalNfts: totalSupply,
        };
        await saveCacheState(contractAddress, state, COLLECTION);
      }
      tierResults.forEach((result, index) => {
        const tokenId = allTokenIds[index].toString();
        if (result.status === 'success') {
          const tier = Number(result.result);
          if (tier >= 1 && tier <= 6) {
            const owner = ownershipByToken.get(tokenId);
            const holder = holdersMap.get(owner);
            if (holder) {
              holder.tiers[tier - 1]++;
              setCache(`${TOKEN_CACHE_KEY}_${contractAddress}-${tokenId}-tier`, tier, CACHE_TTL, COLLECTION);
            }
          }
        } else {
          log(`[Element280] [ERROR] Failed to fetch tier for token ${tokenId}: ${result.error || 'unknown error'}`);
        }
      });
    }
    timings.tierFetch = Date.now() - tierFetchStart;
    state.progressState = { step: 'fetching_rewards', processedNfts: ownershipByToken.size, totalNfts: totalSupply };
    await saveCacheState(contractAddress, state, COLLECTION);

    const rewardFetchStart = Date.now();
    const rewardCalls = [];
    ownershipByWallet.forEach((tokenIds, wallet) => {
      tokenIds.forEach(tokenId => {
        rewardCalls.push({
          address: config.vaultAddresses.element280.address,
          abi: config.abis.element280.vault,
          functionName: 'getRewards',
          args: [[BigInt(tokenId)], wallet],
        });
      });
    });
    if (rewardCalls.length > 0) {
      const limit = pLimit(config.alchemy.batchSize);
      const chunkSize = config.nftContracts.element280.maxTokensPerOwnerQuery;
      const rewardResults = [];
      for (let i = 0; i < rewardCalls.length; i += chunkSize) {
        const chunk = rewardCalls.slice(i, i + chunkSize);
        const results = await limit(() => batchMulticall(chunk));
        rewardResults.push(...results);
        state.progressState = {
          step: 'fetching_rewards',
          processedNfts: Math.min(ownershipByToken.size, i + chunkSize),
          totalNfts: totalSupply,
        };
        await saveCacheState(contractAddress, state, COLLECTION);
      }
      let resultIndex = 0;
      ownershipByWallet.forEach((tokenIds, wallet) => {
        let totalRewards = 0n;
        tokenIds.forEach(() => {
          const result = rewardResults[resultIndex++];
          if (result.status === 'success') {
            const rewardValue = BigInt(result.result[1] || 0);
            totalRewards += rewardValue;
          }
        });
        const holder = holdersMap.get(wallet);
        if (holder) {
          holder.claimableRewards = Number(totalRewards) / 1e18;
          if (isNaN(holder.claimableRewards)) {
            holder.claimableRewards = 0;
          }
          setCache(`${TOKEN_CACHE_KEY}_element280-${wallet}-reward`, holder.claimableRewards, CACHE_TTL, COLLECTION);
        }
      });
    }
    timings.rewardFetch = Date.now() - rewardFetchStart;
    state.progressState = { step: 'calculating_metrics', processedNfts: ownershipByToken.size, totalNfts: totalSupply };
    await saveCacheState(contractAddress, state, COLLECTION);

    const metricsStart = Date.now();
    const multipliers = Object.values(config.contractTiers.element280).map(t => t.multiplier);
    const totalMultiplierSum = Array.from(holdersMap.values()).reduce((sum, holder) => {
      holder.multiplierSum = holder.tiers.reduce(
        (sum, count, index) => sum + count * (multipliers[index] || 0),
        0
      );
      holder.displayMultiplierSum = holder.multiplierSum / 10;
      return sum + holder.multiplierSum;
    }, 0);
    const holders = Array.from(holdersMap.values());
    holders.forEach(holder => {
      holder.percentage = totalMultiplierSum > 0 ? (holder.multiplierSum / totalMultiplierSum) * 100 : 0;
    });
    holders.sort((a, b) => b.multiplierSum - a.multiplierSum || b.total - a.total);
    holders.forEach((holder, index) => {
      holder.rank = index + 1;
      holdersMap.set(holder.wallet, holder);
    });
    await setCache(`${HOLDERS_CACHE_KEY}_${contractAddress}`, Array.from(holdersMap.entries()), CACHE_TTL, COLLECTION);
    timings.metricsCalc = Date.now() - metricsStart;

    timings.total = Date.now() - totalStart;
    state.progressState = { step: 'completed', processedNfts: ownershipByToken.size, totalNfts: totalSupply };
  } catch (error) {
    log(`[Element280] [ERROR] Failed to populate holdersMapCache for ${contractAddress}: ${error.message}, stack: ${error.stack}`);
    errorLog.push({ timestamp: new Date().toISOString(), phase: 'populate_cache', error: error.message });
    state.holdersMapCache = null;
    state.progressState = { step: 'error', processedNfts: 0, totalNfts: 0 };
  } finally {
    state.isCachePopulating = false;
    state.totalOwners = holdersMap.size;
    await saveCacheState(contractAddress, state, COLLECTION);
  }
}

async function getHolderData(contractAddress, wallet) {
  const cacheKey = `element280_holder_${contractAddress}-${wallet.toLowerCase()}`;
  const cached = await getCache(cacheKey, COLLECTION);
  if (cached) {
    log(`[Element280] [INFO] Cache hit for holder: ${cacheKey}`);
    return cached;
  }

  let state = await getCacheState(contractAddress);
  while (state.isCachePopulating) {
    log(`[Element280] [INFO] Waiting for cache population: ${cacheKey}`);
    await new Promise(resolve => setTimeout(resolve, config.alchemy.batchDelayMs));
    state = await getCacheState(contractAddress);
  }

  let holdersMap;
  try {
    const holdersEntries = await getCache(`${HOLDERS_CACHE_KEY}_${contractAddress}`, COLLECTION);
    holdersMap = holdersEntries ? new Map(holdersEntries) : new Map();
  } catch (cacheError) {
    log(`[Element280] [ERROR] Cache read error for holders map: ${cacheError.message}`);
    holdersMap = new Map();
  }

  const walletLower = wallet.toLowerCase();
  if (holdersMap.has(walletLower)) {
    const holder = holdersMap.get(walletLower);
    await setCache(cacheKey, safeSerialize(holder), CACHE_TTL, COLLECTION);
    return safeSerialize(holder);
  }

  const nfts = await getNftsForOwner(walletLower, contractAddress, element280.abi);
  const holder = {
    wallet: walletLower,
    total: nfts.length,
    totalLive: nfts.length,
    multiplierSum: 0,
    displayMultiplierSum: 0,
    tiers: Array(6).fill(0),
    tokenIds: nfts.map(nft => BigInt(nft.tokenId)),
    claimableRewards: 0,
    percentage: 0,
    rank: 0,
  };

  if (nfts.length === 0) {
    return null;
  }

  const calls = [];
  holder.tokenIds.forEach(tokenId => {
    calls.push({
      address: contractAddress,
      abi: config.abis.element280.main,
      functionName: 'getNftTier',
      args: [tokenId],
    });
    calls.push({
      address: config.vaultAddresses.element280.address,
      abi: config.abis.element280.vault,
      functionName: 'getRewards',
      args: [[tokenId], walletLower],
    });
  });

  const results = await batchMulticall(calls);
  const finalTokenIds = [];
  let totalRewards = 0n;
  nfts.forEach((nft, index) => {
    const tierResult = results[index * 2];
    const rewardResult = results[index * 2 + 1];
    if (tierResult.status === 'success') {
      const tier = Number(tierResult.result);
      if (tier >= 1 && tier <= 6) {
        holder.tiers[tier - 1]++;
        finalTokenIds.push(BigInt(nft.tokenId));
        setCache(`${TOKEN_CACHE_KEY}_${contractAddress}-${nft.tokenId}-tier`, tier, CACHE_TTL, COLLECTION);
      }
    }
    if (rewardResult.status === 'success') {
      const rewardValue = BigInt(rewardResult.result[1] || 0);
      totalRewards += rewardValue;
    }
  });

  holder.tokenIds = finalTokenIds;
  holder.total = finalTokenIds.length;
  holder.totalLive = finalTokenIds.length;
  holder.claimableRewards = Number(totalRewards) / 1e18;
  if (isNaN(holder.claimableRewards)) {
    holder.claimableRewards = 0;
  }
  setCache(`${TOKEN_CACHE_KEY}_element280-${walletLower}-reward`, holder.claimableRewards, CACHE_TTL, COLLECTION);

  const multipliers = Object.values(config.contractTiers.element280).map(t => t.multiplier);
  holder.multiplierSum = holder.tiers.reduce(
    (sum, count, index) => sum + count * (multipliers[index] || 0),
    0
  );
  holder.displayMultiplierSum = holder.multiplierSum / 10;

  await setCache(cacheKey, safeSerialize(holder), CACHE_TTL, COLLECTION);
  return safeSerialize(holder);
}

async function getAllHolders(contractAddress, page = 0, pageSize = config.contractDetails.element280.pageSize) {
  let state = await getCacheState(contractAddress);

  if (state.progressState.step === 'completed') {
    const persistedHolders = await loadCacheState(`holders_${contractAddress}`, COLLECTION);
    if (persistedHolders) {
      await setCache(`${HOLDERS_CACHE_KEY}_${contractAddress}`, persistedHolders, CACHE_TTL, COLLECTION);
    } else {
      await populateHoldersMapCache(contractAddress);
      state = await getCacheState(contractAddress);
    }
  }

  while (state.isCachePopulating) {
    log(`[Element280] [INFO] Waiting for cache population to complete`);
    await new Promise(resolve => setTimeout(resolve, config.alchemy.batchDelayMs));
    state = await getCacheState(contractAddress);
  }

  let holdersMap;
  try {
    const holdersEntries = await getCache(`${HOLDERS_CACHE_KEY}_${contractAddress}`, COLLECTION);
    holdersMap = holdersEntries ? new Map(holdersEntries) : new Map();
  } catch (cacheError) {
    log(`[Element280] [ERROR] Cache read error for holders map: ${cacheError.message}`);
    holdersMap = new Map();
  }

  if (holdersMap.size === 0 && state.progressState.step !== 'completed') {
    log(`[Element280] [INFO] No cached holders, populating cache`);
    await populateHoldersMapCache(contractAddress);
    state = await getCacheState(contractAddress);
    holdersMap = new Map(await getCache(`${HOLDERS_CACHE_KEY}_${contractAddress}`, COLLECTION) || []);
  }

  let tierDistribution = [0, 0, 0, 0, 0, 0];
  let multiplierPool = 0;
  try {
    const results = await batchMulticall([
      { address: contractAddress, abi: config.abis.element280.main, functionName: 'getTotalNftsPerTiers' },
      { address: contractAddress, abi: config.abis.element280.main, functionName: 'multiplierPool' },
    ]);
    if (results[0].status === 'success' && results[0].result) {
      tierDistribution = results[0].result.map(Number);
    }
    if (results[1].status === 'success' && results[1].result) {
      multiplierPool = Number(results[1].result);
    }
  } catch (error) {
    log(`[Element280] [ERROR] Failed to fetch tierDistribution or multiplierPool: ${error.message}`);
    const allTokenIds = Array.from(holdersMap.values()).flatMap(h => h.tokenIds);
    if (allTokenIds.length > 0) {
      try {
        const tierCalls = allTokenIds.map(tokenId => ({
          address: contractAddress,
          abi: config.abis.element280.main,
          functionName: 'getNftTier',
          args: [tokenId],
        }));
        const tierResults = await batchMulticall(tierCalls, config.alchemy.batchSize);
        tierResults.forEach(result => {
          if (result.status === 'success') {
            const tier = Number(result.result);
            if (tier >= 1 && tier <= 6) {
              tierDistribution[tier - 1]++;
            }
          }
        });
        const multipliers = Object.values(config.contractTiers.element280).map(t => t.multiplier);
        multiplierPool = tierDistribution.reduce(
          (sum, count, index) => sum + count * (multipliers[index] || 0),
          0
        );
        await setCache(`element280_tier_distribution_${contractAddress}`, { tierDistribution, multiplierPool }, CACHE_TTL, COLLECTION);
      } catch (computeError) {
        log(`[Element280] [ERROR] Failed to compute tierDistribution: ${computeError.message}`);
      }
    }
  }

  const totalTokens = Array.from(holdersMap.values()).reduce((sum, h) => sum + h.totalLive, 0);
  const holders = Array.from(holdersMap.values());
  const totalPages = Math.ceil(holders.length / pageSize);
  const startIndex = page * pageSize;
  const paginatedHolders = holders.slice(startIndex, startIndex + pageSize);
  const response = {
    holders: safeSerialize(paginatedHolders),
    totalPages,
    totalTokens,
    totalShares: multiplierPool,
    totalClaimableRewards: paginatedHolders.reduce((sum, h) => sum + h.claimableRewards, 0),
    summary: {
      totalLive: totalTokens,
      totalBurned: await getBurnedCountFromEvents(contractAddress, []),
      totalMinted: config.nftContracts.element280.expectedTotalSupply + config.nftContracts.element280.expectedBurned,
      tierDistribution,
      multiplierPool,
      totalRewardPool: 0,
    },
  };
  return response;
}

export async function GET(request) {
  const address = CONTRACT_ADDRESS;
  if (!address) {
    log(`[Element280] [VALIDATION] Element280 contract address not found`);
    return NextResponse.json({ error: 'Element280 contract address not found' }, { status: 400 });
  }

  const { searchParams } = new URL(request.url);
  const page = parseInt(searchParams.get('page') || '0', 10);
  const pageSize = parseInt(searchParams.get('pageSize') || config.contractDetails.element280.pageSize, 10);
  const wallet = searchParams.get('wallet');

  try {
    if (wallet) {
      const holder = await getHolderData(address, wallet);
      if (!holder) {
        log(`[Element280] [INFO] No holder data found for wallet ${wallet}`);
        return NextResponse.json({ message: 'No holder data found for wallet' }, { status: 404 });
      }
      return NextResponse.json(safeSerialize(holder));
    }

    const data = await getAllHolders(address, page, pageSize);
    if (!data.holders || !Array.isArray(data.holders)) {
      log(`[Element280] [ERROR] Invalid holders data returned: ${JSON.stringify(data)}`);
      return NextResponse.json({ error: 'Invalid holders data' }, { status: 500 });
    }
    return NextResponse.json(data);
  } catch (error) {
    log(`[Element280] [ERROR] GET error: ${error.message}, stack: ${error.stack}`);
    return NextResponse.json({ error: `Server error: ${error.message}` }, { status: 500 });
  }
}

export async function POST() {
  const address = CONTRACT_ADDRESS;
  if (!address) {
    log(`[Element280] [VALIDATION] Element280 contract address not found`);
    return NextResponse.json({ error: 'Element280 contract address not found' }, { status: 400 });
  }

  try {
    populateHoldersMapCache(address).catch((error) => {
      log(`[Element280] [ERROR] Async cache population failed: ${error.message}, stack: ${error.stack}`);
    });
    return NextResponse.json({ message: 'Cache population started' });
  } catch (error) {
    log(`[Element280] [ERROR] POST error: ${error.message}, stack: ${error.stack}`);
    return NextResponse.json({ error: `Server error: ${error.message}` }, { status: 500 });
  }
}// app/api/holders/Element280/progress/route.js
import { NextResponse } from 'next/server';
import { log } from '@/app/api/utils';
import { getCacheState } from '@/app/api/holders/Element280/route';
import config from '@/config';

export async function GET() {
  const address = config.contractAddresses.element280.address;
  if (!address) {
    log(`[Element280] [VALIDATION] Element280 contract address not found`);
    return NextResponse.json({ error: 'Element280 contract address not found' }, { status: 400 });
  }

  try {
    const state = await getCacheState(address);
    if (!state || !state.progressState) {
      log(`[Element280] [VALIDATION] Invalid cache state for ${address}`);
      return NextResponse.json({ error: 'Cache state not initialized' }, { status: 500 });
    }
    const progressPercentage = state.progressState.totalNfts > 0
      ? ((state.progressState.processedNfts / state.progressState.totalNfts) * 100).toFixed(1)
      : '0.0';

    return NextResponse.json({
      isPopulating: state.isCachePopulating,
      totalLiveHolders: state.totalOwners,
      totalOwners: state.totalOwners,
      phase: state.progressState.step.charAt(0).toUpperCase() + state.progressState.step.slice(1),
      progressPercentage,
    });
  } catch (error) {
    log(`[Element280] [ERROR] Progress endpoint error: ${error.message}, stack: ${error.stack}`);
    return NextResponse.json({ error: `Server error: ${error.message}` }, { status: 500 });
  }
}// app/api/holders/Element280/validate-burned/route.js
import { NextResponse } from 'next/server';
import config from '@/config';
import { getTransactionReceipt, log, client, getCache, setCache } from '@/app/api/utils.js';
import { parseAbiItem } from 'viem';

export async function POST(request) {
  if (process.env.DEBUG === 'true') {
    log(`[Element280-Validate-Burned] [DEBUG] Processing POST request for validate-burned`);
  }

  try {
    const { transactionHash } = await request.json();
    if (!transactionHash || typeof transactionHash !== 'string' || !transactionHash.match(/^0x[a-fA-F0-9]{64}$/)) {
      log(`[Element280-Validate-Burned] [VALIDATION] Invalid transaction hash: ${transactionHash || 'undefined'}`);
      return NextResponse.json({ error: 'Invalid transaction hash' }, { status: 400 });
    }

    const contractAddress = config.contractAddresses?.element280?.address;
    if (!contractAddress) {
      log(`[Element280-Validate-Burned] [VALIDATION] Element280 contract address not configured in config.js`);
      return NextResponse.json({ error: 'Contract address not configured' }, { status: 500 });
    }

    const cacheKey = `element280_burn_validation_${transactionHash}`;
    const cachedResult = await getCache(cacheKey, 'element280');
    if (cachedResult) {
      if (process.env.DEBUG === 'true') {
        log(`[Element280-Validate-Burned] [DEBUG] Cache hit for burn validation: ${transactionHash}`);
      }
      return NextResponse.json(cachedResult);
    }

    if (process.env.DEBUG === 'true') {
      log(`[Element280-Validate-Burned] [DEBUG] Fetching transaction receipt for hash: ${transactionHash}`);
    }
    const receipt = await getTransactionReceipt(transactionHash);
    if (!receipt) {
      log(`[Element280-Validate-Burned] [VALIDATION] Transaction receipt not found for hash: ${transactionHash}`);
      return NextResponse.json({ error: 'Transaction not found' }, { status: 404 });
    }

    const burnAddress = '0x0000000000000000000000000000000000000000';
    const transferEvent = parseAbiItem('event Transfer(address indexed from, address indexed to, uint256 indexed tokenId)');
    const burnedTokenIds = [];

    for (const logEntry of receipt.logs) {
      if (
        logEntry.address.toLowerCase() === contractAddress.toLowerCase() &&
        logEntry.topics[0] === transferEvent.topics[0]
      ) {
        try {
          const decodedLog = client.decodeEventLog({
            abi: [transferEvent],
            data: logEntry.data,
            topics: logEntry.topics,
          });
          if (decodedLog.args.to.toLowerCase() === burnAddress) {
            burnedTokenIds.push(decodedLog.args.tokenId.toString());
          }
        } catch (_decodeError) {
          log(`[Element280-Validate-Burned] [ERROR] Failed to decode log entry for transaction ${transactionHash}: ${_decodeError.message}`);
        }
      }
    }

    if (burnedTokenIds.length === 0) {
      log(`[Element280-Validate-Burned] [VALIDATION] No burn events found in transaction: ${transactionHash}`);
      return NextResponse.json({ error: 'No burn events found in transaction' }, { status: 400 });
    }

    const result = {
      transactionHash,
      burnedTokenIds,
      blockNumber: receipt.blockNumber.toString(),
    };

    await setCache(cacheKey, result, config.cache.nodeCache.stdTTL, 'element280');
    if (process.env.DEBUG === 'true') {
      log(`[Element280-Validate-Burned] [DEBUG] Found ${burnedTokenIds.length} burned tokens in transaction: ${transactionHash}`);
    }
    return NextResponse.json(result);
  } catch (error) {
    log(`[Element280-Validate-Burned] [ERROR] Error processing transaction: ${error.message}, stack: ${error.stack}`);
    return NextResponse.json({ error: 'Failed to validate transaction', details: error.message }, { status: 500 });
  }
}import { NextResponse } from 'next/server';
import { client, retry, logger, getCache, setCache, saveCacheState, loadCacheState } from '@/app/api/utils';
import { parseAbiItem } from 'viem';
import pLimit from 'p-limit';
import config from '@/config.js';

const limit = pLimit(5);
let cacheState = {
  isPopulating: false,
  totalOwners: 0,
  progressState: { step: 'idle', processedNfts: 0, totalNfts: 0 },
  lastUpdated: null,
  lastProcessedBlock: null
};

async function saveStaxCacheState() {
  try {
    await saveCacheState('Stax', cacheState, 'stax');
    logger.debug('Stax', `Saved cache state: totalOwners=${cacheState.totalOwners}, step=${cacheState.progressState.step}, lastProcessedBlock=${cacheState.lastProcessedBlock}`);
  } catch (error) {
    logger.error('Stax', `Failed to save cache state: ${error.message}`, { stack: error.stack });
  }
}

async function loadStaxCacheState() {
  try {
    const savedState = await loadCacheState('Stax', 'stax');
    if (savedState && typeof savedState === 'object') {
      cacheState = {
        isPopulating: savedState.isPopulating ?? savedState.isCachePopulating ?? false,
        totalOwners: savedState.totalOwners ?? 0,
        progressState: {
          step: savedState.progressState?.step ?? 'idle',
          processedNfts: savedState.progressState?.processedNfts ?? 0,
          totalNfts: savedState.progressState?.totalNfts ?? 0,
          error: savedState.progressState?.error ?? null
        },
        lastUpdated: savedState.lastUpdated ?? null,
        lastProcessedBlock: savedState.lastProcessedBlock ?? null
      };
      logger.debug('Stax', `Loaded cache state: totalOwners=${cacheState.totalOwners}, step=${cacheState.progressState.step}, lastProcessedBlock=${cacheState.lastProcessedBlock}`);
    } else {
      logger.info('Stax', 'No valid saved cache state, using defaults');
    }
  } catch (error) {
    logger.error('Stax', `Failed to load cache state: ${error.message}`, { stack: error.stack });
    cacheState = {
      isPopulating: false,
      totalOwners: 0,
      progressState: { step: 'idle', processedNfts: 0, totalNfts: 0 },
      lastUpdated: null,
      lastProcessedBlock: null
    };
  }
}

export async function getCacheState() {
  await loadStaxCacheState();
  return {
    cached: !!await getCache('stax_holders', 'stax'),
    holderCount: cacheState.totalOwners,
    lastUpdated: cacheState.lastUpdated,
    isPopulating: cacheState.isPopulating,
    progressState: cacheState.progressState,
    lastProcessedBlock: cacheState.lastProcessedBlock
  };
}

async function getNewBurnEvents(contractAddress, fromBlock, _errorLog) {
  const burnAddress = config.burnAddress || '0x0000000000000000000000000000000000000000';
  const cacheKey = `stax_burned_events_${contractAddress}_${fromBlock}`;
  let cachedBurned = await getCache(cacheKey, 'stax');

  if (cachedBurned) {
    logger.info('Stax', `Burn events cache hit: ${cacheKey}, count: ${cachedBurned.burnedTokenIds.length}`);
    return cachedBurned;
  }

  logger.info('Stax', `Checking burn events from block ${fromBlock} (from cache_state_stax.json)`);
  let burnedTokenIds = [];
  const endBlock = await client.getBlockNumber();
  if (fromBlock >= endBlock) {
    logger.info('Stax', `No new blocks: fromBlock ${fromBlock} >= endBlock ${endBlock}`);
    return { burnedTokenIds, lastBlock: Number(endBlock) };
  }

  try {
    const logs = await client.getLogs({
      address: contractAddress,
      event: parseAbiItem('event Transfer(address indexed from, address indexed to, uint256 indexed tokenId)'),
      fromBlock: BigInt(fromBlock),
      toBlock: endBlock,
    });
    burnedTokenIds = logs
      .filter(log => log.args.to.toLowerCase() === burnAddress)
      .map(log => Number(log.args.tokenId));
    const cacheData = { burnedTokenIds, lastBlock: Number(endBlock), timestamp: Date.now() };
    await setCache(cacheKey, cacheData, config.cache.nodeCache.stdTTL || 3600, 'stax');
    logger.info('Stax', `Cached burn events: ${cacheKey}, count: ${burnedTokenIds.length}, endBlock: ${endBlock}`);
    return cacheData;
  } catch (error) {
    logger.error('Stax', `Failed to fetch burn events from ${fromBlock} to ${endBlock}: ${error.message}`, { stack: error.stack });
    return { burnedTokenIds, lastBlock: Number(endBlock) };
  }
}

async function getHoldersMap() {
  const contractAddress = config.contractAddresses?.stax?.address;
  if (!contractAddress) {
    logger.error('Stax', 'Contract address missing');
    throw new Error('Contract address missing');
  }
  if (!config.abis?.stax?.main) {
    logger.error('Stax', 'Stax ABI missing');
    throw new Error('Stax ABI missing');
  }

  logger.info('Stax', `Fetching holders for contract: ${contractAddress}`);
  const requiredFunctions = ['totalSupply', 'totalBurned', 'ownerOf', 'getNftTier'];
  const missingFunctions = requiredFunctions.filter(fn => !config.abis.stax.main.some(abi => abi.name === fn));
  if (missingFunctions.length > 0) {
    logger.error('Stax', `Missing ABI functions: ${missingFunctions.join(', ')}`);
    throw new Error(`Missing ABI functions: ${missingFunctions.join(', ')}`);
  }

  const burnAddress = config.burnAddress || '0x0000000000000000000000000000000000000000';
  const holdersMap = new Map();
  let totalBurned = 0;
  let nonExistentCount = 0;
  let burnedCount = 0;

  try {
    cacheState.progressState.step = 'fetching_supply';
    await saveStaxCacheState();
    logger.info('Stax', 'Fetching supply and burned count');

    const currentBlock = await client.getBlockNumber();
    const totalSupply = await retry(
      async () => {
        const result = await client.readContract({
          address: contractAddress,
          abi: config.abis.stax.main,
          functionName: 'totalSupply',
        });
        logger.debug('Stax', `Fetched totalSupply: ${result}`);
        return result;
      },
      { retries: config.alchemy.maxRetries }
    );

    const burnedCountContract = await retry(
      async () => {
        const result = await client.readContract({
          address: contractAddress,
          abi: config.abis.stax.main,
          functionName: 'totalBurned',
        });
        logger.debug('Stax', `Fetched totalBurned: ${result}`);
        return result;
      },
      { retries: config.alchemy.maxRetries }
    );

    totalBurned = Number(burnedCountContract || 0);
    cacheState.progressState.totalNfts = Number(totalSupply || 0);
    cacheState.lastProcessedBlock = Number(currentBlock);
    await saveStaxCacheState();

    logger.info('Stax', `Total supply: ${totalSupply}, burned: ${totalBurned}, block: ${currentBlock}`);
    cacheState.progressState.step = 'fetching_owners';
    await saveStaxCacheState();

    const batchSize = config.alchemy.batchSize;
    const tokenIds = Array.from({ length: Number(totalSupply) - 1 }, (_, i) => i + 1);
    for (let i = 0; i < tokenIds.length; i += batchSize) {
      const batchTokenIds = tokenIds.slice(i, i + batchSize);
      const ownerPromises = batchTokenIds.map(tokenId =>
        limit(() =>
          retry(
            async () => {
              logger.trace('Stax', `Calling ownerOf for token ${tokenId}`);
              try {
                const result = await client.readContract({
                  address: contractAddress,
                  abi: config.abis.stax.main,
                  functionName: 'ownerOf',
                  args: [tokenId],
                });
                logger.trace('Stax', `Fetched owner for token ${tokenId}: ${result}`);
                return result;
              } catch (error) {
                if (error.message.includes('OwnerQueryForNonexistentToken')) {
                  logger.debug('Stax', `Token ${tokenId} does not exist`);
                  nonExistentCount++;
                  return null;
                }
                logger.error('Stax', `Failed to fetch ownerOf for token ${tokenId}: ${error.message}`, { stack: error.stack });
                throw error;
              }
            },
            { retries: config.alchemy.maxRetries }
          )
        )
      );
      const owners = await Promise.all(ownerPromises);
      owners.forEach((owner, index) => {
        if (owner === burnAddress) {
          logger.debug('Stax', `Token ${batchTokenIds[index]} burned`);
          burnedCount++;
        } else if (owner && owner !== '0x0000000000000000000000000000000000000000') {
          const current = holdersMap.get(owner) || { wallet: owner, tokenIds: [], tiers: Array(12).fill(0), total: 0, multiplierSum: 0 };
          current.tokenIds.push(batchTokenIds[index]);
          current.total += 1;
          holdersMap.set(owner, current);
        }
      });
      cacheState.progressState.processedNfts = i + batchSize;
      await saveStaxCacheState();
      logger.info('Stax', `Processed batch ${i / batchSize + 1}: ${holdersMap.size} holders, ${nonExistentCount} non-existent, ${burnedCount} burned`);
    }

    logger.info('Stax', 'Fetching tiers');
    cacheState.progressState.step = 'fetching_tiers';
    await saveStaxCacheState();

    for (const holder of holdersMap.values()) {
      const tierPromises = holder.tokenIds.map(tokenId =>
        limit(() =>
          retry(
            async () => {
              logger.trace('Stax', `Calling getNftTier for token ${tokenId}`);
              try {
                const result = await client.readContract({
                  address: contractAddress,
                  abi: config.abis.stax.main,
                  functionName: 'getNftTier',
                  args: [tokenId],
                });
                logger.trace('Stax', `Fetched tier for token ${tokenId}: ${result}`);
                return Number(result);
              } catch (error) {
                if (error.message.includes('OwnerQueryForNonexistentToken')) {
                  logger.debug('Stax', `Token ${tokenId} does not exist`);
                  return null;
                }
                logger.error('Stax', `Failed to fetch getNftTier for token ${tokenId}: ${error.message}`, { stack: error.stack });
                throw error;
              }
            },
            { retries: config.alchemy.maxRetries }
          )
        )
      );
      const tiers = await Promise.all(tierPromises);
      tiers.forEach((tier, index) => {
        if (tier !== null && tier >= 1 && tier <= 12) {
          holder.tiers[tier - 1]++;
          if (!config.contractTiers?.stax?.[tier]) {
            logger.error('Stax', `Missing multiplier for tier ${tier}`);
            throw new Error(`Missing multiplier for tier ${tier}`);
          }
          holder.multiplierSum += config.contractTiers.stax[tier].multiplier;
        } else {
          logger.warn('Stax', `Invalid tier ${tier} for token ${holder.tokenIds[index]}`);
        }
      });
      cacheState.progressState.processedNfts += holder.tokenIds.length;
      await saveStaxCacheState();
    }

    cacheState.totalOwners = holdersMap.size;
    cacheState.progressState.step = 'completed';
    cacheState.progressState.processedNfts = cacheState.progressState.totalNfts;
    delete cacheState.progressState.error;
    await saveStaxCacheState();
    logger.info('Stax', `Fetched ${holdersMap.size} holders, ${nonExistentCount} non-existent, ${burnedCount} burned, block ${currentBlock}`);
    return { holdersMap, totalBurned, lastBlock: Number(currentBlock) };
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : `Unknown error: ${JSON.stringify(error)}`;
    logger.error('Stax', `Error fetching holders: ${errorMessage}`, { stack: error.stack });
    cacheState.progressState.step = 'error';
    cacheState.progressState.error = errorMessage;
    await saveStaxCacheState();
    throw new Error(errorMessage);
  }
}

async function populateHoldersMapCache(forceUpdate = false) {
  await loadStaxCacheState();
  if (cacheState.isPopulating) {
    logger.info('Stax', 'Cache population in progress');
    return;
  }

  cacheState.isPopulating = true;
  cacheState.progressState.step = 'starting';
  cacheState.progressState.error = null;
  await saveStaxCacheState();

  const contractAddress = config.contractAddresses?.stax?.address;
  const errorLog = [];

  try {
    const cachedData = await getCache('stax_holders', 'stax');
    const isCacheValid = cachedData && Array.isArray(cachedData.holders) && Number.isInteger(cachedData.totalBurned) && !forceUpdate;
    logger.info('Stax', `Cache check: ${isCacheValid ? 'hit' : 'miss'}, forceUpdate: ${forceUpdate}, holders: ${cachedData?.holders?.length || 'none'}`);

    if (isCacheValid) {
      logger.info('Stax', `Cache hit: ${cachedData.holders.length} holders, totalBurned: ${cachedData.totalBurned}`);
      const fromBlock = cacheState.lastProcessedBlock || config.deploymentBlocks.stax.block;
      logger.info('Stax', `Checking burn events from block ${fromBlock} (cache_state_stax.json, fallback: ${config.deploymentBlocks.stax.block})`);
      const { burnedTokenIds, lastBlock } = await getNewBurnEvents(contractAddress, fromBlock, errorLog);

      if (burnedTokenIds.length > 0) {
        logger.info('Stax', `Found ${burnedTokenIds.length} burn events since block ${fromBlock}`);
        const holdersMap = new Map();
        for (const holder of cachedData.holders) {
          const updatedTokenIds = holder.tokenIds.filter(id => !burnedTokenIds.includes(id));
          if (updatedTokenIds.length > 0) {
            const updatedHolder = {
              ...holder,
              tokenIds: updatedTokenIds,
              total: updatedTokenIds.length,
              tiers: Array(12).fill(0),
              multiplierSum: 0
            };
            const tierPromises = updatedTokenIds.map(tokenId =>
              limit(() =>
                retry(
                  async () => {
                    logger.trace('Stax', `Calling getNftTier for token ${tokenId}`);
                    try {
                      const result = await client.readContract({
                        address: contractAddress,
                        abi: config.abis.stax.main,
                        functionName: 'getNftTier',
                        args: [tokenId],
                      });
                      logger.trace('Stax', `Fetched tier for token ${tokenId}: ${result}`);
                      return Number(result);
                    } catch (error) {
                      if (error.message.includes('OwnerQueryForNonexistentToken')) {
                        logger.debug('Stax', `Token ${tokenId} does not exist`);
                        return null;
                      }
                      logger.error('Stax', `Failed to fetch getNftTier for token ${tokenId}: ${error.message}`, { stack: error.stack });
                      throw error;
                    }
                  },
                  { retries: config.alchemy.maxRetries }
                )
              )
            );
            const tiers = await Promise.all(tierPromises);
            tiers.forEach((tier, _index) => {
              if (tier !== null && tier >= 1 && tier <= 12) {
                updatedHolder.tiers[tier - 1]++;
                updatedHolder.multiplierSum += config.contractTiers.stax[tier].multiplier;
              }
            });
            holdersMap.set(holder.wallet, updatedHolder);
          }
        }

        const burnedCountContract = await retry(
          async () => {
            const result = await client.readContract({
              address: contractAddress,
              abi: config.abis.stax.main,
              functionName: 'totalBurned',
            });
            logger.debug('Stax', `Fetched totalBurned: ${result}`);
            return result;
          },
          { retries: config.alchemy.maxRetries }
        );
        const totalBurned = Number(burnedCountContract || 0);

        const holders = Array.from(holdersMap.values());
        const cacheData = { holders, totalBurned, timestamp: Date.now() };
        await setCache('stax_holders', cacheData, 0, 'stax');
        logger.info('Stax', `Updated cache: ${holders.length} holders, totalBurned: ${totalBurned}`);
        cacheState.lastUpdated = Date.now();
        cacheState.totalOwners = holders.length;
        cacheState.lastProcessedBlock = lastBlock;
        cacheState.progressState = {
          step: 'completed',
          processedNfts: cacheState.progressState.totalNfts,
          totalNfts: cacheState.progressState.totalNfts
        };
        await saveStaxCacheState();
        logger.info('Stax', `Cache updated: ${holders.length} holders, ${totalBurned} burned, block ${lastBlock}`);
        return holders;
      } else {
        cacheState.isPopulating = false;
        cacheState.progressState.step = 'completed';
        cacheState.lastProcessedBlock = Number(await client.getBlockNumber());
        await saveStaxCacheState();
        logger.info('Stax', `No new burn events, using cache: ${cachedData.holders.length} holders`);
        return cachedData.holders;
      }
    }

    logger.info('Stax', `Cache miss, fetching holders`);
    const { holdersMap, totalBurned, lastBlock } = await getHoldersMap();
    const holders = Array.from(holdersMap.values());
    const cacheData = { holders, totalBurned, timestamp: Date.now() };
    await setCache('stax_holders', cacheData, 0, 'stax');
    logger.info('Stax', `Set cache: ${holders.length} holders, totalBurned: ${totalBurned}`);
    cacheState.lastUpdated = Date.now();
    cacheState.totalOwners = holders.length;
    cacheState.lastProcessedBlock = lastBlock;
    cacheState.progressState = {
      step: 'completed',
      processedNfts: cacheState.progressState.totalNfts,
      totalNfts: cacheState.progressState.totalNfts
    };
    await saveStaxCacheState();
    logger.info('Stax', `Cached: ${holders.length} holders, ${totalBurned} burned, block ${lastBlock}`);
    return holders;
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : `Unknown error: ${JSON.stringify(error)}`;
    logger.error('Stax', `Cache population failed: ${errorMessage}`, { stack: error.stack });
    cacheState.progressState.step = 'error';
    cacheState.progressState.error = errorMessage;
    await saveStaxCacheState();
    throw new Error(errorMessage);
  } finally {
    cacheState.isPopulating = false;
    cacheState.progressState.step = cacheState.progressState.step === 'error' ? 'error' : 'completed';
    await saveStaxCacheState();
  }
}

export async function GET(_request) {
  try {
    await loadStaxCacheState();
    if (cacheState.isPopulating) {
      logger.info('Stax', 'Cache populating');
      return NextResponse.json({
        message: 'Cache is populating',
        isCachePopulating: true,
        totalOwners: cacheState.totalOwners,
        progressState: cacheState.progressState,
        lastProcessedBlock: cacheState.lastProcessedBlock,
        debugId: `state-${Math.random().toString(36).slice(2)}`,
      }, { status: 202 });
    }

    const cachedData = await getCache('stax_holders', 'stax');
    if (cachedData) {
      logger.info('Stax', `Cache hit: ${cachedData.holders.length} holders`);
      return NextResponse.json({
        holders: cachedData.holders,
        totalTokens: cachedData.holders.reduce((sum, h) => sum + h.total, 0),
        totalBurned: cachedData.totalBurned,
      });
    }

    logger.info('Stax', 'Cache miss, populating');
    const holders = await populateHoldersMapCache();
    return NextResponse.json({
      holders,
      totalTokens: holders.reduce((sum, h) => sum + h.total, 0),
      totalBurned: (await getCache('stax_holders', 'stax'))?.totalBurned || 0,
    });
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : `Unknown error: ${JSON.stringify(error)}`;
    logger.error('Stax', `GET error: ${errorMessage}`, { stack: error.stack });
    return NextResponse.json({ error: 'Failed to fetch Stax holders', details: errorMessage }, { status: 500 });
  }
}

export async function POST(request) {
  try {
    const { forceUpdate } = await request.json().catch(() => ({}));
    logger.info('Stax', `POST received, forceUpdate: ${forceUpdate}`);
    await populateHoldersMapCache(forceUpdate === true);
    return NextResponse.json({ message: 'Stax cache population triggered' });
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : `Unknown error: ${JSON.stringify(error)}`;
    logger.error('Stax', `POST error: ${errorMessage}`, { stack: error.stack });
    return NextResponse.json({ error: 'Failed to populate Stax cache', details: errorMessage }, { status: 500 });
  }
}// File: app/store.js

'use client';
import { create } from 'zustand';

const CACHE_TTL = 30 * 60 * 1000; // 30 minutes

export const useNFTStore = create((set, get) => ({
  cache: {},
  setCache: (contractKey, data) => {
    const key = `nft:${contractKey}`;
    console.log(`[NFTStore] Setting cache for ${key}: ${data.holders?.length || 0} holders`);
    set((state) => ({
      cache: {
        ...state.cache,
        [key]: { data, timestamp: Date.now() },
      },
    }));
  },
  getCache: (contractKey) => {
    const key = `nft:${contractKey}`;
    console.log(`[NFTStore] Getting cache for ${key}`);
    const cachedEntry = get().cache[key];
    if (!cachedEntry) {
      console.log(`[NFTStore] Cache miss for ${key}`);
      return null;
    }
    const now = Date.now();
    if (now - cachedEntry.timestamp > CACHE_TTL) {
      console.log(`[NFTStore] Cache expired for ${key}`);
      set((state) => {
        const newCache = { ...state.cache };
        delete newCache[key];
        return { cache: newCache };
      });
      return null;
    }
    console.log(`[NFTStore] Cache hit for ${key}: ${cachedEntry.data.holders?.length || 0} holders`);
    return cachedEntry.data;
  },
}));// config.js
import element280NftStatus from './element280_nft_status.json' assert { type: 'json' };
import element280MainAbi from './abi/element280.json' assert { type: 'json' };
import element280VaultAbi from './abi/element280Vault.json' assert { type: 'json' };
import element369MainAbi from './abi/element369.json' assert { type: 'json' };
import element369VaultAbi from './abi/element369Vault.json' assert { type: 'json' };
import staxMainAbi from './abi/staxNFT.json' assert { type: 'json' };
import staxVaultAbi from './abi/staxVault.json' assert { type: 'json' };
import ascendantMainAbi from './abi/ascendantNFT.json' assert { type: 'json' };
// E280 ABI placeholder (not deployed)
const e280MainAbi = [];

const config = {
  // Supported blockchain networks
  supportedChains: ['ETH', 'BASE'],

  // ABIs for all collections
  abis: {
    element280: {
      main: element280MainAbi,
      vault: element280VaultAbi,
    },
    element369: {
      main: element369MainAbi,
      vault: element369VaultAbi,
    },
    stax: {
      main: staxMainAbi,
      vault: staxVaultAbi,
    },
    ascendant: {
      main: ascendantMainAbi,
      vault: [], // No vault ABI provided for Ascendant
    },
    e280: {
      main: e280MainAbi,
      vault: [],
    },
  },

  // NFT contract configurations
  nftContracts: {
    element280: {
      name: 'Element 280',
      symbol: 'ELMNT',
      chain: 'ETH',
      address: '0x7F090d101936008a26Bf1F0a22a5f92fC0Cf46c9',
      vaultAddress: '0x44c4ADAc7d88f85d3D33A7f856Ebc54E60C31E97',
      deploymentBlock: '20945304',
      tiers: {
        1: { name: 'Common', multiplier: 10, allocation: '100000000000000000000000000' },
        2: { name: 'Common Amped', multiplier: 12, allocation: '100000000000000000000000000' },
        3: { name: 'Rare', multiplier: 100, allocation: '1000000000000000000000000000' },
        4: { name: 'Rare Amped', multiplier: 120, allocation: '1000000000000000000000000000' },
        5: { name: 'Legendary', multiplier: 1000, allocation: '10000000000000000000000000000' },
        6: { name: 'Legendary Amped', multiplier: 1200, allocation: '10000000000000000000000000000' },
      },
      description:
        'Element 280 NFTs can be minted with TitanX or ETH during a presale and redeemed for Element 280 tokens after a cooldown period. Multipliers contribute to a pool used for reward calculations.',
      expectedTotalSupply: 8107,
      expectedBurned: 8776,
      maxTokensPerOwnerQuery: 100,
    },
    element369: {
      name: 'Element 369',
      symbol: 'E369',
      chain: 'ETH',
      address: '0x024D64E2F65747d8bB02dFb852702D588A062575',
      vaultAddress: '0x4e3DBD6333e649AF13C823DAAcDd14f8507ECBc5',
      deploymentBlock: '21224418',
      tiers: {
        1: { name: 'Common', multiplier: 1, price: '100000000000000000000000000' },
        2: { name: 'Rare', multiplier: 10, price: '1000000000000000000000000000' },
        3: { name: 'Legendary', multiplier: 100, price: '10000000000000000000000000000' },
      },
      description:
        'Element 369 NFTs are minted with TitanX or ETH during specific sale cycles. Burning NFTs updates a multiplier pool and tracks burn cycles for reward distribution in the Holder Vault.',
    },
    stax: {
      name: 'Stax',
      symbol: 'STAX',
      chain: 'ETH',
      address: '0x74270Ca3a274B4dbf26be319A55188690CACE6E1',
      vaultAddress: '0x5D27813C32dD705404d1A78c9444dAb523331717',
      deploymentBlock: '21452667',
      tiers: {
        1: { name: 'Common', multiplier: 1, price: '100000000000000000000000000' },
        2: { name: 'Common Amped', multiplier: 1.2, price: '100000000000000000000000000', amplifier: '10000000000000000000000000' },
        3: { name: 'Common Super', multiplier: 1.4, price: '100000000000000000000000000', amplifier: '20000000000000000000000000' },
        4: { name: 'Common LFG', multiplier: 2, price: '100000000000000000000000000', amplifier: '50000000000000000000000000' },
        5: { name: 'Rare', multiplier: 10, price: '1000000000000000000000000000' },
        6: { name: 'Rare Amped', multiplier: 12, price: '1000000000000000000000000000', amplifier: '100000000000000000000000000' },
        7: { name: 'Rare Super', multiplier: 14, price: '1000000000000000000000000000', amplifier: '200000000000000000000000000' },
        8: { name: 'Rare LFG', multiplier: 20, price: '1000000000000000000000000000', amplifier: '500000000000000000000000000' },
        9: { name: 'Legendary', multiplier: 100, price: '10000000000000000000000000000' },
        10: { name: 'Legendary Amped', multiplier: 120, price: '10000000000000000000000000000', amplifier: '1000000000000000000000000000' },
        11: { name: 'Legendary Super', multiplier: 140, price: '10000000000000000000000000000', amplifier: '2000000000000000000000000000' },
        12: { name: 'Legendary LFG', multiplier: 200, price: '10000000000000000000000000000', amplifier: '5000000000000000000000000000' },
      },
      description:
        'Stax NFTs are minted with TitanX or ETH during a presale. Burning NFTs after a cooldown period claims backing rewards, with multipliers contributing to a pool for cycle-based reward calculations.',
    },
    ascendant: {
      name: 'Ascendant',
      symbol: 'ASCNFT',
      chain: 'ETH',
      address: '0x9da95c32c5869c84ba2c020b5e87329ec0adc97f',
      deploymentBlock: '21112535',
      tiers: {
        1: { name: 'Tier 1', price: '7812500000000000000000', multiplier: 1.01 },
        2: { name: 'Tier 2', price: '15625000000000000000000', multiplier: 1.02 },
        3: { name: 'Tier 3', price: '31250000000000000000000', multiplier: 1.03 },
        4: { name: 'Tier 4', price: '62500000000000000000000', multiplier: 1.04 },
        5: { name: 'Tier 5', price: '125000000000000000000000', multiplier: 1.05 },
        6: { name: 'Tier 6', price: '250000000000000000000000', multiplier: 1.06 },
        7: { name: 'Tier 7', price: '500000000000000000000000', multiplier: 1.07 },
        8: { name: 'Tier 8', price: '1000000000000000000000000', multiplier: 1.08 },
      },
      description:
        'Ascendant NFTs are minted with ASCENDANT tokens and offer staking rewards from DragonX pools over 8, 28, and 90-day periods. Features fusion mechanics to combine same-tier NFTs into higher tiers.',
    },
    e280: {
      name: 'E280',
      symbol: 'E280',
      chain: 'BASE',
      address: null,
      deploymentBlock: null,
      tiers: {},
      description: 'E280 NFTs on BASE chain. Contract not yet deployed.',
      disabled: true,
    },
  },

  // Contract addresses
  contractAddresses: {
    element280: { chain: 'ETH', address: '0x7F090d101936008a26Bf1F0a22a5f92fC0Cf46c9' },
    element369: { chain: 'ETH', address: '0x024D64E2F65747d8bB02dFb852702D588A062575' },
    stax: { chain: 'ETH', address: '0x74270Ca3a274B4dbf26be319A55188690CACE6E1' },
    ascendant: { chain: 'ETH', address: '0x9da95c32c5869c84ba2c020b5e87329ec0adc97f' },
    e280: { chain: 'BASE', address: null },
  },

  // Vault addresses
  vaultAddresses: {
    element280: { chain: 'ETH', address: '0x44c4ADAc7d88f85d3D33A7f856Ebc54E60C31E97' },
    element369: { chain: 'ETH', address: '0x4e3DBD6333e649AF13C823DAAcDd14f8507ECBc5' },
    stax: { chain: 'ETH', address: '0x5D27813C32dD705404d1A78c9444dAb523331717' },
    e280: { chain: 'BASE', address: null },
  },

  // Deployment blocks
  deploymentBlocks: {
    element280: { chain: 'ETH', block: '20945304' },
    element369: { chain: 'ETH', block: '21224418' },
    stax: { chain: 'ETH', block: '21452667' },
    ascendant: { chain: 'ETH', block: '21112535' },
    e280: { chain: 'BASE', block: null },
  },

  // Contract tiers
  contractTiers: {
    element280: {
      1: { name: 'Common', multiplier: 10 },
      2: { name: 'Common Amped', multiplier: 12 },
      3: { name: 'Rare', multiplier: 100 },
      4: { name: 'Rare Amped', multiplier: 120 },
      5: { name: 'Legendary', multiplier: 1000 },
      6: { name: 'Legendary Amped', multiplier: 1200 },
    },
    element369: {
      1: { name: 'Common', multiplier: 1 },
      2: { name: 'Rare', multiplier: 10 },
      3: { name: 'Legendary', multiplier: 100 },
      tierOrder: [
        { tierId: '3', name: 'Legendary' },
        { tierId: '2', name: 'Rare' },
        { tierId: '1', name: 'Common' },
      ],
    },
    stax: {
      1: { name: 'Common', multiplier: 1 },
      2: { name: 'Common Amped', multiplier: 1.2 },
      3: { name: 'Common Super', multiplier: 1.4 },
      4: { name: 'Common LFG', multiplier: 2 },
      5: { name: 'Rare', multiplier: 10 },
      6: { name: 'Rare Amped', multiplier: 12 },
      7: { name: 'Rare Super', multiplier: 14 },
      8: { name: 'Rare LFG', multiplier: 20 },
      9: { name: 'Legendary', multiplier: 100 },
      10: { name: 'Legendary Amped', multiplier: 120 },
      11: { name: 'Legendary Super', multiplier: 140 },
      12: { name: 'Legendary LFG', multiplier: 200 },
    },
    ascendant: {
      1: { name: 'Tier 1', multiplier: 1.01 },
      2: { name: 'Tier 2', multiplier: 1.02 },
      3: { name: 'Tier 3', multiplier: 1.03 },
      4: { name: 'Tier 4', multiplier: 1.04 },
      5: { name: 'Tier 5', multiplier: 1.05 },
      6: { name: 'Tier 6', multiplier: 1.06 },
      7: { name: 'Tier 7', multiplier: 1.07 },
      8: { name: 'Tier 8', multiplier: 1.08 },
    },
    e280: {},
  },

  // Contract details
  contractDetails: {
    element280: {
      name: 'Element 280',
      chain: 'ETH',
      pageSize: 100,
      apiEndpoint: '/api/holders/Element280',
      rewardToken: 'ELMNT',
    },
    element369: {
      name: 'Element 369',
      chain: 'ETH',
      pageSize: 1000,
      apiEndpoint: '/api/holders/Element369',
      rewardToken: 'INFERNO/FLUX/E280',
    },
    stax: {
      name: 'Stax',
      chain: 'ETH',
      pageSize: 1000,
      apiEndpoint: '/api/holders/Stax',
      rewardToken: 'X28',
    },
    ascendant: {
      name: 'Ascendant',
      chain: 'ETH',
      pageSize: 1000,
      apiEndpoint: '/api/holders/Ascendant',
      rewardToken: 'DRAGONX',
    },
    e280: {
      name: 'E280',
      chain: 'BASE',
      pageSize: 1000,
      apiEndpoint: '/api/holders/E280',
      rewardToken: 'E280',
      disabled: true,
    },
  },

  // Utility function to get contract details by name
  getContractDetails: (contractName) => {
    return config.nftContracts[contractName] || null;
  },

  alchemy: {
    apiKey: process.env.ALCHEMY_API_KEY || process.env.NEXT_PUBLIC_ALCHEMY_API_KEY,
    network: 'eth-mainnet',
    batchSize: 10,
    batchDelayMs: 1000,
    retryMaxDelayMs: 30000,
    maxRetries: 3,
    timeoutMs: 30000,
  },
  
  // Cache settings
  cache: {
    redis: {
      disableElement280: process.env.DISABLE_ELEMENT280_REDIS === 'true',
      disableElement369: process.env.DISABLE_ELEMENT369_REDIS === 'true',
      disableStax: process.env.DISABLE_STAX_REDIS === 'true',
      disableAscendant: process.env.DISABLE_ASCENDANT_REDIS === 'true',
      disableE280: process.env.DISABLE_E280_REDIS === 'true' || true,
    },
    nodeCache: {
      stdTTL: 3600,
      checkperiod: 120,
    },
  },

  // Debug settings
  debug: {
    enabled: process.env.DEBUG === 'true',
    logLevel: 'debug',
  },

  // Fallback data (optional, for testing)
  fallbackData: {
    element280: process.env.USE_FALLBACK_DATA === 'true' ? element280NftStatus : null,
  },
};

export default config;import NodeCache from 'node-cache';
import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';
import pino from 'pino';
import { Redis } from '@upstash/redis';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const logger = pino({
  level: process.env.LOG_LEVEL || 'info',
  transport: { target: 'pino-pretty' },
});

const cache = new NodeCache({
  stdTTL: 0, // No expiration
  checkperiod: 120,
});

const cacheDir = path.join(__dirname, '../../cache');
const redisEnabled = process.env.DISABLE_STAX_REDIS !== 'true' && process.env.UPSTASH_REDIS_REST_URL && process.env.UPSTASH_REDIS_REST_TOKEN;
let redis = null;

if (redisEnabled) {
  try {
    redis = new Redis({
      url: process.env.UPSTASH_REDIS_REST_URL,
      token: process.env.UPSTASH_REDIS_REST_TOKEN,
    });
    logger.info('utils', 'Upstash Redis initialized');
  } catch (error) {
    logger.error('utils', `Failed to initialize Upstash Redis: ${error.message}`, { stack: error.stack });
    redis = null;
  }
}

// Ensure cache directory exists
async function ensureCacheDir() {
  try {
    await fs.mkdir(cacheDir, { recursive: true });
    await fs.chmod(cacheDir, 0o755);
    logger.debug('utils', `Ensured cache directory: ${cacheDir}`);
  } catch (error) {
    logger.error('utils', `Failed to create/chmod cache directory ${cacheDir}: ${error.message}`, { stack: error.stack });
    throw error;
  }
}

// Initialize cache from disk or Redis
async function initializeCache() {
  try {
    if (redisEnabled && redis) {
      try {
        const data = await redis.get('stax_stax_holders');
        if (data) {
          const parsed = typeof data === 'string' ? JSON.parse(data) : data;
          if (parsed && Array.isArray(parsed.holders) && Number.isInteger(parsed.totalBurned)) {
            const success = cache.set('stax_stax_holders', parsed);
            logger.info('utils', `Initialized cache from Redis: stax_holders, success: ${success}, holders: ${parsed.holders.length}`);
            return true;
          } else {
            logger.warn('utils', 'Invalid cache data in Redis for stax_holders');
          }
        }
      } catch (error) {
        logger.error('utils', `Failed to initialize cache from Redis: ${error.message}`, { stack: error.stack });
      }
    }
    
    await ensureCacheDir();
    const cacheFile = path.join(cacheDir, 'stax_holders.json');
    try {
      const data = await fs.readFile(cacheFile, 'utf8');
      const parsed = JSON.parse(data);
      if (parsed && Array.isArray(parsed.holders) && Number.isInteger(parsed.totalBurned)) {
        const success = cache.set('stax_stax_holders', parsed);
        logger.info('utils', `Initialized cache from disk: stax_holders, success: ${success}, holders: ${parsed.holders.length}`);
        return true;
      } else {
        logger.warn('utils', `Invalid cache data in ${cacheFile}`);
      }
    } catch (error) {
      if (error.code !== 'ENOENT') {
        logger.error('utils', `Failed to read cache from ${cacheFile}: ${error.message}`, { stack: error.stack });
      } else {
        logger.debug('utils', `No cache file at ${cacheFile}`);
      }
    }
    return false;
  } catch (error) {
    logger.error('utils', `Cache initialization error: ${error.message}`, { stack: error.stack });
    return false;
  }
}

// Initialize cache on module load
initializeCache().catch(error => {
  logger.error('utils', `Cache initialization failed: ${error.message}`, { stack: error.stack });
});

async function setCache(key, value, ttl, prefix) {
  try {
    const cacheKey = `${prefix}_${key}`;
    const success = cache.set(cacheKey, value); // No TTL
    logger.debug('utils', `Set cache: ${cacheKey}, success: ${success}, holders: ${value.holders?.length || 'unknown'}`);
    
    if (key === 'stax_holders' && prefix === 'stax') {
      if (redisEnabled && redis) {
        try {
          await redis.set('stax_stax_holders', JSON.stringify(value));
          logger.info('utils', `Persisted stax_holders to Redis, holders: ${value.holders.length}`);
        } catch (error) {
          logger.error('utils', `Failed to persist stax_holders to Redis: ${error.message}`, { stack: error.stack });
        }
      } else {
        const cacheFile = path.join(cacheDir, 'stax_holders.json');
        await ensureCacheDir();
        await fs.writeFile(cacheFile, JSON.stringify(value));
        await fs.chmod(cacheFile, 0o644);
        logger.info('utils', `Persisted stax_holders to ${cacheFile}, holders: ${value.holders.length}`);
      }
    }
    
    return success;
  } catch (error) {
    logger.error('utils', `Failed to set cache ${prefix}_${key}: ${error.message}`, { stack: error.stack });
    return false;
  }
}

async function getCache(key, prefix) {
  try {
    const cacheKey = `${prefix}_${key}`;
    let data = cache.get(cacheKey);
    if (data !== undefined) {
      logger.debug('utils', `Cache hit: ${cacheKey}, holders: ${data.holders?.length || 'unknown'}`);
      return data;
    }

    if (key === 'stax_holders' && prefix === 'stax') {
      if (redisEnabled && redis) {
        try {
          const redisData = await redis.get('stax_stax_holders');
          if (redisData) {
            const parsed = typeof redisData === 'string' ? JSON.parse(redisData) : redisData;
            if (parsed && Array.isArray(parsed.holders) && Number.isInteger(parsed.totalBurned)) {
              const success = cache.set(cacheKey, parsed);
              logger.info('utils', `Loaded stax_holders from Redis, cached: ${success}, holders: ${parsed.holders.length}`);
              return parsed;
            } else {
              logger.warn('utils', `Invalid data in Redis for ${cacheKey}`);
            }
          }
        } catch (error) {
          logger.error('utils', `Failed to load cache from Redis: ${error.message}`, { stack: error.stack });
        }
      }
      
      const cacheFile = path.join(cacheDir, 'stax_holders.json');
      try {
        const fileData = await fs.readFile(cacheFile, 'utf8');
        const parsed = JSON.parse(fileData);
        if (parsed && Array.isArray(parsed.holders) && Number.isInteger(parsed.totalBurned)) {
          const success = cache.set(cacheKey, parsed);
          logger.info('utils', `Loaded stax_holders from ${cacheFile}, cached: ${success}, holders: ${parsed.holders.length}`);
          return parsed;
        } else {
          logger.warn('utils', `Invalid data in ${cacheFile}`);
        }
      } catch (error) {
        if (error.code !== 'ENOENT') {
          logger.error('utils', `Failed to load cache from ${cacheFile}: ${error.message}`, { stack: error.stack });
        } else {
          logger.debug('utils', `No cache file at ${cacheFile}`);
        }
      }
    }

    logger.info('utils', `Cache miss: ${cacheKey}`);
    return null;
  } catch (error) {
    logger.error('utils', `Failed to get cache ${prefix}_${key}: ${error.message}`, { stack: error.stack });
    return null;
  }
}

async function saveCacheState(collection, state, prefix) {
  try {
    const cacheFile = path.join(cacheDir, `cache_state_${prefix.toLowerCase()}.json`);
    await ensureCacheDir();
    await fs.writeFile(cacheFile, JSON.stringify(state));
    await fs.chmod(cacheFile, 0o644);
    logger.debug('utils', `Saved cache state for ${prefix}: ${cacheFile}`);
  } catch (error) {
    logger.error('utils', `Failed to save cache state for ${prefix}: ${error.message}`, { stack: error.stack });
  }
}

async function loadCacheState(collection, prefix) {
  try {
    const cacheFile = path.join(cacheDir, `cache_state_${prefix.toLowerCase()}.json`);
    const data = await fs.readFile(cacheFile, 'utf8');
    const parsed = JSON.parse(data);
    logger.debug('utils', `Loaded cache state for ${prefix}: ${cacheFile}`);
    return parsed;
  } catch (error) {
    if (error.code === 'ENOENT') {
      logger.debug('utils', `No cache state found for ${prefix}`);
      return null;
    }
    logger.error('utils', `Failed to load cache state for ${prefix}: ${error.message}`, { stack: error.stack });
    return null;
  }
}

// Mock client and retry
const client = {
  getBlockNumber: async () => BigInt(22382116),
  getLogs: async () => [],
  readContract: async () => null,
};

function retry(operation, { retries }) {
  return operation();
}

export { client, retry, logger, getCache, setCache, saveCacheState, loadCacheState };import { NextResponse } from 'next/server';
import { logger } from '@/lib/logger.js';
import { getCacheState } from '../route';

export async function GET() {
  try {
    const state = await getCacheState();
    if (!state || !state.progressState) {
      logger.error('Stax', 'Invalid cache state');
      return NextResponse.json({ error: 'Cache state not initialized' }, { status: 500 });
    }
    const progressPercentage = state.progressState.totalNfts > 0
      ? ((state.progressState.processedNfts / state.progressState.totalNfts) * 100).toFixed(1)
      : state.progressState.step === 'completed' ? '100.0' : '0.0';

    return NextResponse.json({
      isPopulating: state.isPopulating,
      totalLiveHolders: state.holderCount,
      totalOwners: state.holderCount,
      phase: state.progressState.step.charAt(0).toUpperCase() + state.progressState.step.slice(1),
      progressPercentage,
      lastProcessedBlock: state.lastProcessedBlock,
      error: state.progressState.error || null,
    });
  } catch (error) {
    logger.error('Stax', `Progress endpoint error: ${error.message}`);
    return NextResponse.json({ error: 'Failed to fetch cache state', details: error.message }, { status: 500 });
  }
}