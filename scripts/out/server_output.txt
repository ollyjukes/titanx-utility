================= Includes the following JS files under ./server =================
app/api/holders/Element280/validate-burned/route.js
app/api/holders/[contract]/progress/route.js
app/api/holders/[contract]/route.js
app/api/utils.js
app/lib/chartOptions.js
app/lib/fetchCollectionData.js
app/lib/logger.js
app/lib/schemas.js
app/lib/serverInit.js
app/lib/useNFTData.js


================= Contents of above files in ./server =================


----- app/api/holders/Element280/validate-burned/route.js -----

// app/api/holders/Element280/validate-burned/route.js
import { NextResponse } from 'next/server';
import config from '@/contracts/config';
import { getTransactionReceipt, log, client, getCache, setCache } from '@/app/api/utils.js';
import { parseAbiItem } from 'viem';

export async function POST(request) {
  if (process.env.DEBUG === 'true') {
    log(`[Element280-Validate-Burned] [DEBUG] Processing POST request for validate-burned`);
  }

  try {
    const { transactionHash } = await request.json();
    if (!transactionHash || typeof transactionHash !== 'string' || !transactionHash.match(/^0x[a-fA-F0-9]{64}$/)) {
      log(`[Element280-Validate-Burned] [VALIDATION] Invalid transaction hash: ${transactionHash || 'undefined'}`);
      return NextResponse.json({ error: 'Invalid transaction hash' }, { status: 400 });
    }

    const contractAddress = config.contractAddresses?.element280?.address;
    if (!contractAddress) {
      log(`[Element280-Validate-Burned] [VALIDATION] Element280 contract address not configured in config.js`);
      return NextResponse.json({ error: 'Contract address not configured' }, { status: 500 });
    }

    const cacheKey = `element280_burn_validation_${transactionHash}`;
    const cachedResult = await getCache(cacheKey, 'element280');
    if (cachedResult) {
      if (process.env.DEBUG === 'true') {
        log(`[Element280-Validate-Burned] [DEBUG] Cache hit for burn validation: ${transactionHash}`);
      }
      return NextResponse.json(cachedResult);
    }

    if (process.env.DEBUG === 'true') {
      log(`[Element280-Validate-Burned] [DEBUG] Fetching transaction receipt for hash: ${transactionHash}`);
    }
    const receipt = await getTransactionReceipt(transactionHash);
    if (!receipt) {
      log(`[Element280-Validate-Burned] [VALIDATION] Transaction receipt not found for hash: ${transactionHash}`);
      return NextResponse.json({ error: 'Transaction not found' }, { status: 404 });
    }

    const burnAddress = '0x0000000000000000000000000000000000000000';
    const transferEvent = parseAbiItem('event Transfer(address indexed from, address indexed to, uint256 indexed tokenId)');
    const burnedTokenIds = [];

    for (const logEntry of receipt.logs) {
      if (
        logEntry.address.toLowerCase() === contractAddress.toLowerCase() &&
        logEntry.topics[0] === transferEvent.topics[0]
      ) {
        try {
          const decodedLog = client.decodeEventLog({
            abi: [transferEvent],
            data: logEntry.data,
            topics: logEntry.topics,
          });
          if (decodedLog.args.to.toLowerCase() === burnAddress) {
            burnedTokenIds.push(decodedLog.args.tokenId.toString());
          }
        } catch (_decodeError) {
          log(`[Element280-Validate-Burned] [ERROR] Failed to decode log entry for transaction ${transactionHash}: ${_decodeError.message}`);
        }
      }
    }

    if (burnedTokenIds.length === 0) {
      log(`[Element280-Validate-Burned] [VALIDATION] No burn events found in transaction: ${transactionHash}`);
      return NextResponse.json({ error: 'No burn events found in transaction' }, { status: 400 });
    }

    const result = {
      transactionHash,
      burnedTokenIds,
      blockNumber: receipt.blockNumber.toString(),
    };

    await setCache(cacheKey, result, config.cache.nodeCache.stdTTL, 'element280');
    if (process.env.DEBUG === 'true') {
      log(`[Element280-Validate-Burned] [DEBUG] Found ${burnedTokenIds.length} burned tokens in transaction: ${transactionHash}`);
    }
    return NextResponse.json(result);
  } catch (error) {
    log(`[Element280-Validate-Burned] [ERROR] Error processing transaction: ${error.message}, stack: ${error.stack}`);
    return NextResponse.json({ error: 'Failed to validate transaction', details: error.message }, { status: 500 });
  }
}
----- app/api/holders/[contract]/progress/route.js -----

import { NextResponse } from 'next/server';
import { logger, loadCacheState } from '@/app/api/utils';
import config from '@/contracts/config';

async function getCacheState(contractKey) {
  const cacheState = {
    isPopulating: false,
    totalOwners: 0,
    progressState: { step: 'idle', processedNfts: 0, totalNfts: 0, processedTiers: 0, totalTiers: 0, error: null, errorLog: [] },
    lastUpdated: null,
    lastProcessedBlock: null,
    globalMetrics: {}, // Added
  };
  try {
    const savedState = await loadCacheState(contractKey, contractKey.toLowerCase());
    if (savedState && typeof savedState === 'object') {
      cacheState.isPopulating = savedState.isPopulating ?? false;
      cacheState.totalOwners = savedState.totalOwners ?? 0;
      cacheState.progressState = {
        step: savedState.progressState?.step ?? 'idle',
        processedNfts: savedState.progressState?.processedNfts ?? 0,
        totalNfts: savedState.progressState?.totalNfts ?? 0,
        processedTiers: savedState.progressState?.processedTiers ?? 0,
        totalTiers: savedState.progressState?.totalTiers ?? 0,
        error: savedState.progressState?.error ?? null,
        errorLog: savedState.progressState?.errorLog ?? [],
      };
      cacheState.lastUpdated = savedState.lastUpdated ?? null;
      cacheState.lastProcessedBlock = savedState.lastProcessedBlock ?? null;
      cacheState.globalMetrics = savedState.globalMetrics ?? {};
    }
  } catch (error) {
    logger.error(contractKey, `Failed to load cache state: ${error.message}`, { stack: error.stack });
    cacheState.progressState.error = `Failed to load cache state: ${error.message}`;
    cacheState.progressState.errorLog.push({
      timestamp: new Date().toISOString(),
      phase: 'load_cache_state',
      error: error.message,
    });
  }
  return cacheState;
}

export async function GET(_request, { params }) {
  const { contract } = await params;
  const contractKey = contract.toLowerCase();

  if (!config.contractDetails[contractKey]) {
    logger.error(contractKey, `Invalid contract: ${contractKey}`);
    return NextResponse.json({ error: `Invalid contract: ${contractKey}` }, { status: 400 });
  }

  if (config.contractDetails[contractKey].disabled) {
    return NextResponse.json({ error: `${contractKey} contract not deployed` }, { status: 400 });
  }

  try {
    const state = await getCacheState(contractKey);
    console.log(`Progress state for ${contractKey}:`, JSON.stringify(state, null, 2)); // Debug log
    if (!state || !state.progressState) {
      logger.error(contractKey, 'Invalid cache state');
      return NextResponse.json({ error: 'Cache state not initialized' }, { status: 500 });
    }

    let progressPercentage = '0.0';
    if (state.progressState.error) {
      progressPercentage = '0.0';
    } else if (state.progressState.step === 'completed') {
      progressPercentage = '100.0';
    } else if (state.progressState.totalNfts > 0) {
      if (state.progressState.step === 'fetching_owners') {
        const ownerProgress = (state.progressState.processedNfts / state.progressState.totalNfts) * 50;
        progressPercentage = Math.min(ownerProgress, 50).toFixed(1);
      } else if (state.progressState.step === 'fetching_tiers') {
        const tierProgress = (state.progressState.processedTiers / state.progressState.totalTiers) * 50;
        progressPercentage = Math.min(50 + tierProgress, 100).toFixed(1);
      }
    }

    return NextResponse.json({
      isPopulating: state.isPopulating,
      totalLiveHolders: state.totalOwners,
      totalOwners: state.totalOwners,
      phase: state.progressState.step.charAt(0).toUpperCase() + state.progressState.step.slice(1),
      progressPercentage,
      lastProcessedBlock: state.lastProcessedBlock,
      error: state.progressState.error || null,
      errorLog: (state.progressState.errorLog || []).slice(-50), // Limit to last 50 errors
      globalMetrics: state.globalMetrics, // Added
    });
  } catch (error) {
    logger.error(contractKey, `Progress endpoint error: ${error.message}`, { stack: error.stack });
    return NextResponse.json({ error: `Failed to fetch ${contractKey} cache state`, details: error.message }, { status: 500 });
  }
}
----- app/api/holders/[contract]/route.js -----

// ./app/api/holders/[contract]/route.js
import { NextResponse } from 'next/server';
import { parseAbiItem, formatUnits, getAddress } from 'viem';
import pLimit from 'p-limit';
import config from '@/contracts/config.js';
import { client, retry, logger, getCache, setCache, saveCacheState, loadCacheState, batchMulticall, getOwnersForContract, validateContract } from '@/app/api/utils';
import { HoldersResponseSchema } from '@/app/lib/schemas';

const limit = pLimit(5);

// Utility to sanitize BigInt values
function sanitizeBigInt(obj) {
  if (typeof obj === 'bigint') return obj.toString();
  if (Array.isArray(obj)) return obj.map(item => sanitizeBigInt(item));
  if (typeof obj === 'object' && obj !== null) {
    const sanitized = {};
    for (const [key, value] of Object.entries(obj)) {
      sanitized[key] = sanitizeBigInt(value);
    }
    return sanitized;
  }
  return obj;
}

// Get cache state for a contract
async function getCacheState(contractKey) {
  const cacheState = {
    isPopulating: false,
    totalOwners: 0,
    totalLiveHolders: 0,
    progressState: { step: 'idle', processedNfts: 0, totalNfts: 0, processedTiers: 0, totalTiers: 0, error: null, errorLog: [] },
    lastUpdated: null,
    lastProcessedBlock: null,
    globalMetrics: {},
  };
  try {
    const savedState = await loadCacheState(contractKey, contractKey.toLowerCase());
    if (savedState && typeof savedState === 'object') {
      Object.assign(cacheState, {
        isPopulating: savedState.isPopulating ?? false,
        totalOwners: savedState.totalOwners ?? 0,
        totalLiveHolders: savedState.totalLiveHolders ?? 0,
        progressState: {
          step: savedState.progressState?.step ?? 'idle',
          processedNfts: savedState.progressState?.processedNfts ?? 0,
          totalNfts: savedState.progressState?.totalNfts ?? 0,
          processedTiers: savedState.progressState?.processedTiers ?? 0,
          totalTiers: savedState.progressState?.totalTiers ?? 0,
          error: savedState.progressState?.error ?? null,
          errorLog: savedState.progressState?.errorLog ?? [],
        },
        lastUpdated: savedState.lastUpdated ?? null,
        lastProcessedBlock: savedState.lastProcessedBlock ?? null,
        globalMetrics: savedState.globalMetrics ?? {},
      });
      if (!config.debug.suppressDebug) {
        logger.debug('utils', `Loaded cache state: totalOwners=${cacheState.totalOwners}, step=${cacheState.progressState.step}`, 'eth', contractKey);
      }
    }
  } catch (error) {
    logger.error('utils', `Failed to load cache state: ${error.message}`, { stack: error.stack }, 'eth', contractKey);
  }
  return cacheState;
}

// Save cache state for a contract
async function saveCacheStateContract(contractKey, cacheState) {
  try {
    await saveCacheState(contractKey, cacheState, contractKey.toLowerCase());
    if (!config.debug.suppressDebug) {
      logger.debug('utils', `Saved cache state: totalOwners=${cacheState.totalOwners}, step=${cacheState.progressState.step}`, 'eth', contractKey);
    }
  } catch (error) {
    logger.error('utils', `Failed to save cache state: ${error.message}`, { stack: error.stack }, 'eth', contractKey);
  }
}

// Fetch new Transfer events (burns and transfers)
async function getNewEvents(contractKey, contractAddress, fromBlock, errorLog) {
  const burnAddress = config.burnAddress || '0x0000000000000000000000000000000000000000';
  const cacheKey = `${contractKey.toLowerCase()}_events_${contractAddress}_${fromBlock}`;
  let cachedEvents = await getCache(cacheKey, contractKey.toLowerCase());

  if (cachedEvents) {
    logger.info('utils', `Events cache hit: ${cacheKey}, count: ${cachedEvents.burnedTokenIds.length + (cachedEvents.transferTokenIds?.length || 0)}`, 'eth', contractKey);
    return cachedEvents;
  }

  let burnedTokenIds = [];
  let transferTokenIds = [];
  let endBlock;
  try {
    endBlock = await client.getBlockNumber();
  } catch (error) {
    logger.error('utils', `Failed to fetch block number: ${error.message}`, { stack: error.stack }, 'eth', contractKey);
    errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_block_number', error: error.message });
    throw error;
  }

  if (fromBlock >= endBlock) {
    logger.info('utils', `No new blocks: fromBlock ${fromBlock} >= endBlock ${endBlock}`, 'eth', contractKey);
    return { burnedTokenIds, transferTokenIds, lastBlock: Number(endBlock) };
  }

  try {
    const logs = await client.getLogs({
      address: contractAddress,
      event: parseAbiItem('event Transfer(address indexed from, address indexed to, uint256 indexed tokenId)'),
      fromBlock: BigInt(fromBlock),
      toBlock: endBlock,
    });
    burnedTokenIds = logs
      .filter(log => log.args.to.toLowerCase() === burnAddress.toLowerCase())
      .map(log => Number(log.args.tokenId));
    transferTokenIds = logs
      .filter(log => log.args.to.toLowerCase() !== burnAddress.toLowerCase())
      .map(log => ({ tokenId: Number(log.args.tokenId), from: log.args.from.toLowerCase(), to: log.args.to.toLowerCase() }));
    const cacheData = { burnedTokenIds, transferTokenIds, lastBlock: Number(endBlock), timestamp: Date.now() };
    await setCache(cacheKey, cacheData, config.cache.nodeCache.stdTTL, contractKey.toLowerCase());
    logger.info('utils', `Cached events: ${cacheKey}, burns: ${burnedTokenIds.length}, transfers: ${transferTokenIds.length}`, 'eth', contractKey);
    return cacheData;
  } catch (error) {
    logger.error('utils', `Failed to fetch events: ${error.message}`, { stack: error.stack }, 'eth', contractKey);
    errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_events', error: error.message });
    throw error;
  }
}

// Utility function to safely serialize objects with BigInt
function safeStringify(obj) {
  return JSON.stringify(obj, (key, value) =>
    typeof value === 'bigint' ? value.toString() : value
  );
}

async function getHoldersMap(contractKey, contractAddress, abi, vaultAddress, vaultAbi, cacheState, forceUpdate = false) {
  if (!contractAddress) throw new Error('Contract address missing');
  if (!abi) throw new Error(`${contractKey} ABI missing`);

  contractKey = contractKey.toLowerCase();
  if (!config.debug.suppressDebug) {
    logger.debug('utils', `Starting getHoldersMap: contractKey=${contractKey}, forceUpdate=${forceUpdate}`, 'eth', contractKey);
  }

  const requiredFunctions = contractKey === 'ascendant'
    ? ['getNFTAttribute', 'userRecords', 'totalShares', 'toDistribute', 'batchClaimableAmount']
    : ['totalSupply', 'totalBurned', 'ownerOf', 'getNftTier'];
  const missingFunctions = requiredFunctions.filter(fn => !abi.some(item => item.name === fn && item.type === 'function'));
  if (missingFunctions.length > 0) throw new Error(`Missing ABI functions: ${missingFunctions.join(', ')}`);

  const burnAddress = config.burnAddress || '0x0000000000000000000000000000000000000000';
  let holdersMap = new Map();
  let totalBurned = cacheState.totalBurned || 0;
  let errorLog = cacheState.progressState.errorLog || [];
  let totalLockedAscendant = 0;
  let totalShares = 0;
  let toDistributeDay8 = 0;
  let toDistributeDay28 = 0;
  let toDistributeDay90 = 0;
  let totalTokens = 0;
  let tokenOwnerMap = new Map();

  const contractTiers = config.nftContracts[contractKey]?.tiers || {};
  const maxTier = Math.max(...Object.keys(contractTiers).map(Number), 0);
  let rarityDistribution = contractKey === 'ascendant' ? Array(3).fill(0) : [];
  let tierDistribution = Array(maxTier + 1).fill(0); // 0 to maxTier inclusive

  cacheState.progressState.step = 'checking_cache';
  cacheState.progressState.progressPercentage = '0%';
  await saveCacheStateContract(contractKey, cacheState);

  let currentBlock;
  try {
    currentBlock = await client.getBlockNumber();
    cacheState.progressState.lastProcessedBlock = Number(currentBlock); // Save immediately
    cacheState.progressState.lastUpdated = Date.now();
    await saveCacheStateContract(contractKey, cacheState);
    if (!config.debug.suppressDebug) {
      logger.debug('utils', `Fetched current block: ${currentBlock}, saved to cacheState`, 'eth', contractKey);
    }
  } catch (error) {
    errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_block_number', error: error.message });
    logger.error('utils', `Failed to fetch block number: ${error.message}`, { stack: error.stack }, 'eth', contractKey);
    throw error;
  }

  // Check cache validity
  const blockThreshold = contractKey === 'element280' ? (config.cache.blockThreshold || 7200) : (config.cache.blockThreshold || 100); // ~24 hours for element280
  const cacheValid = !forceUpdate &&
    cacheState.lastProcessedBlock &&
    cacheState.progressState.step === 'completed' &&
    !cacheState.isPopulating &&
    (Number(currentBlock) - cacheState.lastProcessedBlock < blockThreshold);

  if (!config.debug.suppressDebug) {
    logger.debug('utils', `Cache validity check: cacheValid=${cacheValid}, forceUpdate=${forceUpdate}, lastProcessedBlock=${cacheState.lastProcessedBlock}, step=${cacheState.progressState.step}, isPopulating=${cacheState.isPopulating}, blockDiff=${Number(currentBlock) - cacheState.lastProcessedBlock}, blockThreshold=${blockThreshold}`, 'eth', contractKey);
  }

  let cachedTokenTiers = new Map();
  if (cacheValid && contractKey === 'element280') {
    try {
      const cachedHolders = await getCache(`${contractKey}_holders`, contractKey);
      if (cachedHolders?.holders && Array.isArray(cachedHolders.holders)) {
        holdersMap = new Map(cachedHolders.holders.map(h => [h.wallet, h]));
        totalBurned = cachedHolders.totalBurned || totalBurned;
        totalTokens = cacheState.progressState.totalNfts || 0;
        holdersMap.forEach(holder => {
          holder.tokenIds.forEach(tokenId => tokenOwnerMap.set(Number(tokenId), holder.wallet));
        });
        // Load cached tiers for element280
        const cachedTiers = await getCache(`${contractKey}_tiers`, contractKey) || {};
        Object.entries(cachedTiers).forEach(([tokenId, tierData]) => {
          if (tierData && typeof tierData.tier === 'number') {
            cachedTokenTiers.set(Number(tokenId), tierData);
          }
        });
        if (!config.debug.suppressDebug) {
          logger.debug('utils', `Cache hit: holders=${holdersMap.size}, tiers=${cachedTokenTiers.size}, lastBlock=${cacheState.lastProcessedBlock}`, 'eth', contractKey);
        }
        // Fetch new Transfer events
        const fromBlock = BigInt(cacheState.lastProcessedBlock);
        const { burnedTokenIds, transferTokenIds, lastBlock } = await getNewEvents(contractKey, contractAddress, fromBlock, errorLog);
        if (!config.debug.suppressDebug) {
          logger.debug('utils', `New events: burns=${burnedTokenIds.length}, transfers=${transferTokenIds.length}, fromBlock=${fromBlock}, toBlock=${lastBlock}`, 'eth', contractKey);
        }
        // Process burns
        const updatedTokenIds = new Set();
        burnedTokenIds.forEach(tokenId => {
          const wallet = tokenOwnerMap.get(tokenId);
          if (wallet) {
            const holder = holdersMap.get(wallet);
            if (holder) {
              holder.tokenIds = holder.tokenIds.filter(id => id !== tokenId);
              holder.total -= 1;
              const tier = cachedTokenTiers.get(tokenId)?.tier || 0;
              holder.tiers[tier] -= 1;
              holder.multiplierSum -= contractTiers[tier + 1]?.multiplier || (tier + 1);
              if (holder.total === 0) holdersMap.delete(wallet);
              tokenOwnerMap.delete(tokenId);
              cachedTokenTiers.delete(tokenId);
              totalTokens -= 1;
              totalBurned += 1;
              tierDistribution[tier] -= 1;
            }
          }
        });
        // Process transfers
        transferTokenIds.forEach(({ tokenId, from, to }) => {
          updatedTokenIds.add(tokenId);
          const oldHolder = holdersMap.get(from);
          if (oldHolder) {
            oldHolder.tokenIds = oldHolder.tokenIds.filter(id => id !== tokenId);
            oldHolder.total -= 1;
            const tier = cachedTokenTiers.get(tokenId)?.tier || 0;
            oldHolder.tiers[tier] -= 1;
            oldHolder.multiplierSum -= contractTiers[tier + 1]?.multiplier || (tier + 1);
            if (oldHolder.total === 0) holdersMap.delete(from);
          }
          let newHolder = holdersMap.get(to) || {
            wallet: to,
            tokenIds: [],
            tiers: Array(maxTier + 1).fill(0),
            total: 0,
            multiplierSum: 0,
            claimableRewards: 0,
          };
          newHolder.tokenIds.push(tokenId);
          newHolder.total += 1;
          const tier = cachedTokenTiers.get(tokenId)?.tier || 0;
          newHolder.tiers[tier] += 1;
          newHolder.multiplierSum += contractTiers[tier + 1]?.multiplier || (tier + 1);
          holdersMap.set(to, newHolder);
          tokenOwnerMap.set(tokenId, to);
        });
        // Fetch tiers for tokens without cached tiers
        const missingTierTokenIds = Array.from(updatedTokenIds).filter(tokenId => !cachedTokenTiers.has(tokenId));
        if (missingTierTokenIds.length > 0) {
          cacheState.progressState.step = 'fetching_updated_tiers';
          cacheState.progressState.processedTiers = 0;
          cacheState.progressState.totalTiers = missingTierTokenIds.length;
          cacheState.progressState.progressPercentage = '50%';
          await saveCacheStateContract(contractKey, cacheState);
          const tierCalls = missingTierTokenIds.map(tokenId => ({
            address: contractAddress,
            abi,
            functionName: 'getNftTier',
            args: [BigInt(tokenId)]
          }));
          const tierResults = [];
          const chunkSize = config.nftContracts[contractKey]?.maxTokensPerOwnerQuery || 1000;
          for (let i = 0; i < tierCalls.length; i += chunkSize) {
            const chunk = tierCalls.slice(i, i + chunkSize);
            const results = await retry(
              () => batchMulticall(chunk, config.alchemy.batchSize),
              { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
            );
            tierResults.push(...results);
            cacheState.progressState.processedTiers = Math.min(i + chunkSize, tierCalls.length);
            cacheState.progressState.progressPercentage = `${Math.round(50 + (i / tierCalls.length) * 20)}%`;
            await saveCacheStateContract(contractKey, cacheState);
            if (!config.debug.suppressDebug) {
              logger.debug('utils', `Processed updated tiers for ${cacheState.progressState.processedTiers}/${tierCalls.length} tokens`, 'eth', contractKey);
            }
          }
          tierResults.forEach((result, i) => {
            const tokenId = missingTierTokenIds[i];
            if (result.status === 'success') {
              const tier = Number(result.result) || 0;
              cachedTokenTiers.set(tokenId, { tier, timestamp: Date.now() });
            } else {
              errorLog.push({
                timestamp: new Date().toISOString(),
                phase: 'fetch_updated_tier',
                tokenId,
                error: result.error || 'unknown error'
              });
            }
          });
          // Update holders with new tiers
          missingTierTokenIds.forEach(tokenId => {
            const wallet = tokenOwnerMap.get(tokenId);
            if (wallet) {
              const holder = holdersMap.get(wallet);
              if (holder) {
                const oldTierIndex = holder.tokenIds.indexOf(tokenId);
                if (oldTierIndex >= 0) {
                  const oldTier = holder.tiers.findIndex((count, i) => count > 0 && i !== oldTierIndex);
                  if (oldTier >= 0) {
                    holder.tiers[oldTier] -= 1;
                    holder.multiplierSum -= contractTiers[oldTier + 1]?.multiplier || (oldTier + 1);
                    tierDistribution[oldTier] -= 1;
                  }
                }
                const newTier = cachedTokenTiers.get(tokenId)?.tier || 0;
                holder.tiers[newTier] += 1;
                holder.multiplierSum += contractTiers[newTier + 1]?.multiplier || (newTier + 1);
                tierDistribution[newTier] += 1;
              }
            }
          });
          await setCache(`${contractKey}_tiers`, Object.fromEntries(cachedTokenTiers), config.cache.nodeCache.stdTTL || 86400, contractKey); // 24 hours TTL
        }
        cacheState.progressState.totalNfts = totalTokens;
        cacheState.progressState.totalTiers = totalTokens;
        cacheState.progressState.totalLiveHolders = totalTokens;
        cacheState.globalMetrics = {
          totalMinted: totalTokens + totalBurned,
          totalLive: totalTokens,
          totalBurned,
          tierDistribution,
        };
        cacheState.progressState.isPopulating = false;
        cacheState.progressState.step = 'completed';
        cacheState.progressState.processedNfts = totalTokens;
        cacheState.progressState.processedTiers = missingTierTokenIds.length;
        cacheState.progressState.progressPercentage = '100%';
        cacheState.progressState.lastProcessedBlock = Number(currentBlock);
        cacheState.progressState.lastUpdated = Date.now();
        await saveCacheStateContract(contractKey, cacheState);
        const holderList = Array.from(holdersMap.values());
        holderList.sort((a, b) => b.multiplierSum - a.multiplierSum || b.total - a.total);
        holderList.forEach((holder, index) => {
          holder.rank = index + 1;
          holder.percentage = (holder.total / totalTokens * 100) || 0;
          holder.displayMultiplierSum = holder.multiplierSum;
        });
        await setCache(`${contractKey}_holders`, { holders: holderList, totalBurned, timestamp: Date.now() }, 0, contractKey);
        logger.info('utils', `Updated cached holders for ${contractKey}, lastBlock=${cacheState.lastProcessedBlock}, updatedTokens=${missingTierTokenIds.length}`, 'eth', contractKey);
        return { holdersMap, totalBurned, lastBlock: Number(currentBlock), errorLog, rarityDistribution };
      } else {
        logger.warn('utils', `Invalid holders cache data for ${contractKey}`, 'eth', contractKey);
      }
    } catch (error) {
      logger.error('utils', `Failed to load cache for ${contractKey}: ${error.message}`, { stack: error.stack }, 'eth', contractKey);
      errorLog.push({ timestamp: new Date().toISOString(), phase: 'load_cache', error: error.message });
    }
  } else if (cacheValid) {
    // Original cache logic for non-element280 contracts
    try {
      const cachedHolders = await getCache(`${contractKey}_holders`, contractKey);
      if (cachedHolders?.holders) {
        holdersMap = new Map(cachedHolders.holders.map(h => [h.wallet, h]));
        totalBurned = cachedHolders.totalBurned || totalBurned;
        rarityDistribution = cachedHolders.rarityDistribution || rarityDistribution;
        totalTokens = cacheState.progressState.totalNfts || 0;
        holdersMap.forEach(holder => {
          holder.tokenIds.forEach(tokenId => tokenOwnerMap.set(Number(tokenId), holder.wallet));
        });
        if (contractKey === 'ascendant') {
          totalLockedAscendant = cacheState.globalMetrics.totalLockedAscendant || 0;
          totalShares = cacheState.globalMetrics.totalShares || 0;
          toDistributeDay8 = cacheState.globalMetrics.toDistributeDay8 || 0;
          toDistributeDay28 = cacheState.globalMetrics.toDistributeDay28 || 0;
          toDistributeDay90 = cacheState.globalMetrics.toDistributeDay90 || 0;
        }
        cacheState.progressState.isPopulating = false;
        cacheState.progressState.step = 'cached';
        cacheState.progressState.progressPercentage = '100%';
        await saveCacheStateContract(contractKey, cacheState);
        logger.info('utils', `Using cached holders for ${contractKey}, lastBlock=${cacheState.lastProcessedBlock}`, 'eth', contractKey);
        return { holdersMap, totalBurned, lastBlock: cacheState.lastProcessedBlock, errorLog, rarityDistribution };
      }
    } catch (error) {
      logger.error('utils', `Failed to load cache for ${contractKey}: ${error.message}`, { stack: error.stack }, 'eth', contractKey);
    }
  }

  cacheState.progressState.step = 'fetching_supply';
  cacheState.progressState.isPopulating = true;
  cacheState.progressState.progressPercentage = '10%';
  await saveCacheStateContract(contractKey, cacheState);

  const isBurnContract = ['stax', 'element280', 'element369'].includes(contractKey);
  if (contractKey === 'ascendant') {
    try {
      const [totalSharesRaw, toDistributeDay8Raw, toDistributeDay28Raw, toDistributeDay90Raw] = await retry(
        () => Promise.all([
          client.readContract({ address: contractAddress, abi, functionName: 'totalShares' }),
          client.readContract({ address: contractAddress, abi, functionName: 'toDistribute', args: [0] }),
          client.readContract({ address: contractAddress, abi, functionName: 'toDistribute', args: [1] }),
          client.readContract({ address: contractAddress, abi, functionName: 'toDistribute', args: [2] })
        ]),
        { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
      );
      totalShares = parseFloat(formatUnits(totalSharesRaw, 18));
      toDistributeDay8 = parseFloat(formatUnits(toDistributeDay8Raw, 18));
      toDistributeDay28 = parseFloat(formatUnits(toDistributeDay28Raw, 18));
      toDistributeDay90 = parseFloat(formatUnits(toDistributeDay90Raw, 18));
      if (!config.debug.suppressDebug) {
        logger.debug('utils', `Ascendant metrics: totalShares=${totalShares}, toDistributeDay8=${toDistributeDay8}`, 'eth', contractKey);
      }
    } catch (error) {
      logger.error('utils', `Failed to fetch ascendant metrics: ${error.message}`, { stack: error.stack }, 'eth', contractKey);
      errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_ascendant_metrics', error: error.message });
      throw error;
    }
  } else {
    try {
      const [totalSupply, burnedCount] = await retry(
        () => Promise.all([
          client.readContract({ address: contractAddress, abi, functionName: 'totalSupply' }),
          client.readContract({ address: contractAddress, abi, functionName: 'totalBurned' }).catch(() => 0)
        ]),
        { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
      );
      totalTokens = Number(totalSupply);
      totalBurned = Number(burnedCount);
      if (!config.debug.suppressDebug) {
        logger.debug('utils', `Total tokens: ${totalTokens}, totalBurned: ${totalBurned}`, 'eth', contractKey);
      }
    } catch (error) {
      logger.error('utils', `Supply fetch error: ${error.message}`, { stack: error.stack }, 'eth', contractKey);
      errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_supply', error: error.message });
      throw error;
    }
  }

  cacheState.progressState.step = 'fetching_holders';
  cacheState.progressState.progressPercentage = '20%';
  await saveCacheStateContract(contractKey, cacheState);

  try {
    const owners = await retry(
      () => getOwnersForContract(contractAddress, abi, { withTokenBalances: true, maxPages: 100 }),
      { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
    );

    const filteredOwners = owners.filter(
      owner => owner?.ownerAddress && owner.ownerAddress.toLowerCase() !== burnAddress.toLowerCase() && owner.tokenBalances?.length > 0
    );
    if (!config.debug.suppressDebug) {
      logger.debug('utils', `Filtered owners: ${filteredOwners.length}`, 'eth', contractKey);
    }

    tokenOwnerMap.clear();
    totalTokens = 0;
    const seenTokenIds = new Set();

    filteredOwners.forEach(owner => {
      if (!owner.ownerAddress) return;
      let wallet;
      try {
        wallet = getAddress(owner.ownerAddress).toLowerCase();
      } catch (e) {
        logger.warn('utils', `Invalid wallet address: ${owner.ownerAddress}`, 'eth', contractKey);
        errorLog.push({ timestamp: new Date().toISOString(), phase: 'process_owner', ownerAddress: owner.ownerAddress, error: 'Invalid wallet address' });
        return;
      }
      owner.tokenBalances.forEach(tb => {
        if (!tb.tokenId) return;
        const tokenId = Number(tb.tokenId);
        if (seenTokenIds.has(tokenId)) {
          logger.warn('utils', `Duplicate tokenId ${tokenId} for wallet ${wallet}`, 'eth', contractKey);
          errorLog.push({ timestamp: new Date().toISOString(), phase: 'process_token', tokenId, wallet, error: 'Duplicate tokenId' });
          return;
        }
        seenTokenIds.add(tokenId);
        tokenOwnerMap.set(tokenId, wallet);
        totalTokens++;
      });
    });
    if (!config.debug.suppressDebug) {
      logger.debug('utils', `Total tokens (Alchemy): ${totalTokens}, unique tokenIds: ${seenTokenIds.size}`, 'eth', contractKey);
    }
  } catch (error) {
    logger.warn('utils', `Failed to fetch owners via getOwnersForContract: ${error.message}, falling back to Transfer events`, 'eth', contractKey);
    errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_owners_alchemy', error: error.message });

    const fromBlock = BigInt(config.getDeploymentBlocks()[contractKey]?.block || 0);
    const toBlock = currentBlock;
    tokenOwnerMap.clear();
    totalTokens = 0;
    const seenTokenIds = new Set();

    const transferLogs = await retry(
      async () => {
        const logs = await client.getLogs({
          address: contractAddress,
          event: parseAbiItem('event Transfer(address indexed from, address indexed to, uint256 indexed tokenId)'),
          fromBlock,
          toBlock,
        });
        return logs;
      },
      { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
    );

    for (const log of transferLogs) {
      const from = log.args.from.toLowerCase();
      const to = log.args.to.toLowerCase();
      const tokenId = Number(log.args.tokenId);

      if (to === burnAddress.toLowerCase()) {
        totalBurned += 1;
        tokenOwnerMap.delete(tokenId);
        seenTokenIds.delete(tokenId);
        continue;
      }

      if (from === '0x0000000000000000000000000000000000000000') {
        if (!seenTokenIds.has(tokenId)) {
          tokenOwnerMap.set(tokenId, to);
          seenTokenIds.add(tokenId);
          totalTokens++;
        }
      } else {
        tokenOwnerMap.set(tokenId, to);
        seenTokenIds.add(tokenId);
      }
    }
    if (!config.debug.suppressDebug) {
      logger.debug('utils', `Total tokens (Transfer events): ${totalTokens}, unique tokenIds: ${seenTokenIds.size}`, 'eth', contractKey);
    }
  }

  cacheState.progressState.totalNfts = totalTokens;
  cacheState.progressState.totalTiers = totalTokens;
  cacheState.progressState.totalLiveHolders = totalTokens;
  cacheState.progressState.progressPercentage = '30%';
  await saveCacheStateContract(contractKey, cacheState);

  if (totalTokens === 0) {
    cacheState.progressState.step = 'completed';
    cacheState.progressState.progressPercentage = '100%';
    cacheState.globalMetrics = {
      ...(contractKey === 'element280' || contractKey === 'stax' || contractKey === 'ascendant' ? { totalMinted: totalTokens + totalBurned } : {}),
      totalLive: totalTokens,
      totalBurned,
      tierDistribution: Array(maxTier + 1).fill(0),
      ...(contractKey === 'ascendant' ? {
        totalLockedAscendant: 0,
        totalShares: 0,
        toDistributeDay8: 0,
        toDistributeDay28: 0,
        toDistributeDay90: 0,
        pendingRewards: 0,
        rarityDistribution: Array(3).fill(0)
      } : {})
    };
    await saveCacheStateContract(contractKey, cacheState);
    await setCache(`${contractKey}_tiers`, {}, config.cache.nodeCache.stdTTL || 86400, contractKey);
    logger.info('utils', `No tokens found, returning empty holdersMap`, 'eth', contractKey);
    return { holdersMap, totalBurned, lastBlock: Number(currentBlock), errorLog, rarityDistribution };
  }

  cacheState.progressState.step = 'fetching_records';
  cacheState.progressState.progressPercentage = '40%';
  await saveCacheStateContract(contractKey, cacheState);

  const tokenIds = Array.from(tokenOwnerMap.keys());
  const recordCalls = contractKey === 'ascendant' ? tokenIds.map(tokenId => ({
    address: contractAddress,
    abi,
    functionName: 'userRecords',
    args: [BigInt(tokenId)]
  })) : [];
  const recordResults = contractKey === 'ascendant' ? [] : tokenIds.map(() => ({ status: 'success', result: [] }));
  if (contractKey === 'ascendant') {
    const chunkSize = config.nftContracts[contractKey]?.maxTokensPerOwnerQuery || 1000;
    for (let i = 0; i < recordCalls.length; i += chunkSize) {
      const chunk = recordCalls.slice(i, i + chunkSize);
      const results = await retry(
        () => batchMulticall(chunk, config.alchemy.batchSize),
        { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
      );
      recordResults.push(...results);
      cacheState.progressState.progressPercentage = `${Math.round(40 + (i / recordCalls.length) * 10)}%`;
      await saveCacheStateContract(contractKey, cacheState);
    }
  }

  cacheState.progressState.step = 'fetching_tiers';
  cacheState.progressState.processedTiers = 0;
  cacheState.progressState.progressPercentage = '50%';
  await saveCacheStateContract(contractKey, cacheState);

  // Load cached tiers for element280
  if (contractKey === 'element280') {
    const cachedTiers = await getCache(`${contractKey}_tiers`, contractKey) || {};
    Object.entries(cachedTiers).forEach(([tokenId, tierData]) => {
      if (tierData && typeof tierData.tier === 'number') {
        cachedTokenTiers.set(Number(tokenId), tierData);
      }
    });
    if (!config.debug.suppressDebug) {
      logger.debug('utils', `Cached tiers loaded: ${cachedTokenTiers.size}, missing tiers for ${tokenIds.length - cachedTokenTiers.size} tokens`, 'eth', contractKey);
    }
  }

  const missingTierTokenIds = contractKey === 'element280' ? tokenIds.filter(tokenId => !cachedTokenTiers.has(tokenId)) : tokenIds;
  const tierCalls = missingTierTokenIds.map(tokenId => ({
    address: contractAddress,
    abi,
    functionName: contractKey === 'ascendant' ? 'getNFTAttribute' : 'getNftTier',
    args: [BigInt(tokenId)]
  }));

  const tierResults = [];
  const chunkSize = config.nftContracts[contractKey]?.maxTokensPerOwnerQuery || 1000;
  for (let i = 0; i < tierCalls.length; i += chunkSize) {
    const chunk = tierCalls.slice(i, i + chunkSize);
    const results = await retry(
      () => batchMulticall(chunk, config.alchemy.batchSize),
      { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
    );
    tierResults.push(...results);
    cacheState.progressState.processedTiers = Math.min(i + chunkSize, missingTierTokenIds.length);
    cacheState.progressState.progressPercentage = `${Math.round(50 + (i / tierCalls.length) * 20)}%`;
    await saveCacheStateContract(contractKey, cacheState);
    if (!config.debug.suppressDebug) {
      logger.debug('utils', `Processed tiers for ${cacheState.progressState.processedTiers}/${missingTierTokenIds.length} tokens`, 'eth', contractKey);
    }
  }

  // Cache new tier results for element280
  if (contractKey === 'element280') {
    tierResults.forEach((result, i) => {
      const tokenId = missingTierTokenIds[i];
      if (result.status === 'success') {
        const tier = Number(result.result) || 0;
        cachedTokenTiers.set(tokenId, { tier, timestamp: Date.now() });
      } else {
        errorLog.push({
          timestamp: new Date().toISOString(),
          phase: 'fetch_tier',
          tokenId,
          error: result.error || 'unknown error'
        });
      }
    });
    await setCache(`${contractKey}_tiers`, Object.fromEntries(cachedTokenTiers), config.cache.nodeCache.stdTTL || 86400, contractKey);
  }

  // Combine cached and new tier results for element280
  const allTierResults = contractKey === 'element280' ? tokenIds.map(tokenId => {
    if (cachedTokenTiers.has(tokenId)) {
      const tierData = cachedTokenTiers.get(tokenId);
      return { status: 'success', result: tierData.tier };
    }
    const index = missingTierTokenIds.indexOf(tokenId);
    return index >= 0 ? tierResults[index] : { status: 'failure', error: 'Missing tier data' };
  }) : tierResults;

  cacheState.progressState.step = 'fetching_rewards';
  cacheState.progressState.progressPercentage = '70%';
  await saveCacheStateContract(contractKey, cacheState);

  const rewardCalls = contractKey === 'ascendant' ? [{
    address: contractAddress,
    abi,
    functionName: 'batchClaimableAmount',
    args: [tokenIds.map(id => BigInt(id))]
  }, {
    address: contractAddress,
    abi,
    functionName: 'toDistribute',
    args: [0]
  }, {
    address: contractAddress,
    abi,
    functionName: 'toDistribute',
    args: [1]
  }, {
    address: contractAddress,
    abi,
    functionName: 'toDistribute',
    args: [2]
  }, {
    address: contractAddress,
    abi,
    functionName: 'totalShares',
    args: []
  }] : [];

  const rewardResults = contractKey === 'ascendant' ? await retry(
    () => batchMulticall(rewardCalls, config.alchemy.batchSize),
    { retries: config.alchemy.maxRetries, delay: config.alchemy.batchDelayMs }
  ) : [];

  if (contractKey === 'ascendant') {
    if (rewardResults[0].status === 'success') {
      const claimable = parseFloat(formatUnits(rewardResults[0].result || 0, 18));
      holdersMap.forEach(holder => {
        holder.claimableRewards = claimable / totalTokens * holder.total;
      });
    }
    toDistributeDay8 = rewardResults[1].status === 'success' ? parseFloat(formatUnits(rewardResults[1].result || 0, 18)) : toDistributeDay8;
    toDistributeDay28 = rewardResults[2].status === 'success' ? parseFloat(formatUnits(rewardResults[2].result || 0, 18)) : toDistributeDay28;
    toDistributeDay90 = rewardResults[3].status === 'success' ? parseFloat(formatUnits(rewardResults[3].result || 0, 18)) : toDistributeDay90;
    totalShares = rewardResults[4].status === 'success' ? parseFloat(formatUnits(rewardResults[4].result || 0, 18)) : totalShares;
  }

  cacheState.progressState.step = 'building_holders';
  cacheState.progressState.progressPercentage = '80%';
  await saveCacheStateContract(contractKey, cacheState);

  tokenIds.forEach((tokenId, i) => {
    const wallet = tokenOwnerMap.get(tokenId);
    if (!wallet) {
      logger.warn('utils', `No owner found for token ${tokenId}`, 'eth', contractKey);
      errorLog.push({ timestamp: new Date().toISOString(), phase: 'process_token', tokenId, error: 'No owner found' });
      return;
    }

    let shares = 0;
    let lockedAscendant = 0;
    if (contractKey === 'ascendant') {
      const recordResult = recordResults[i];
      if (recordResult.status === 'success' && Array.isArray(recordResult.result)) {
        shares = parseFloat(formatUnits(recordResult.result[0] || 0, 18));
        lockedAscendant = parseFloat(formatUnits(recordResult.result[1] || 0, 18));
        totalLockedAscendant += lockedAscendant;
      } else {
        logger.error('utils', `Failed to fetch userRecords for token ${tokenId}: ${recordResult.error || 'unknown error'}`, 'eth', contractKey);
        errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_records', tokenId, wallet, error: recordResult.error || 'unknown error' });
        return;
      }
    }

    let tier = 0;
    let rarityNumber = 0;
    let rarity = 0;
    const tierResult = allTierResults[i];
    if (!config.debug.suppressDebug) {
      logger.debug('utils', `Raw tierResult for token ${tokenId}: status=${tierResult.status}, result=${safeStringify(tierResult.result)}`, 'eth', contractKey);
    }

    if (tierResult.status === 'success') {
      if (contractKey === 'ascendant') {
        const result = tierResult.result;
        let parsedResult;
        if (Array.isArray(result) && result.length >= 3) {
          parsedResult = {
            rarityNumber: Number(result[0]) || 0,
            tier: Number(result[1]) || 0,
            rarity: Number(result[2]) || 0
          };
        } else if (typeof result === 'object' && result !== null && 'rarityNumber' in result) {
          parsedResult = {
            rarityNumber: Number(result.rarityNumber) || 0,
            tier: Number(result.tier) || 0,
            rarity: Number(result.rarity) || 0
          };
        } else {
          logger.warn('utils', `Invalid getNFTAttribute result for token ${tokenId}: result=${safeStringify(result)}`, 'eth', contractKey);
          errorLog.push({
            timestamp: new Date().toISOString(),
            phase: 'fetch_tier',
            tokenId,
            wallet,
            error: `Invalid getNFTAttribute result: ${safeStringify(result)}`
          });
          return;
        }
        rarityNumber = parsedResult.rarityNumber;
        tier = parsedResult.tier;
        rarity = parsedResult.rarity;
        if (!config.debug.suppressDebug) {
          logger.debug('utils', `Parsed attributes for token ${tokenId} (ascendant): tier=${tier}, rarityNumber=${rarityNumber}, rarity=${rarity}`, 'eth', contractKey);
        }
      } else {
        tier = typeof tierResult.result === 'bigint' ? Number(tierResult.result) : Number(tierResult.result) || 0;
      }

      if (isNaN(tier) || tier < 0 || tier > maxTier) {
        logger.warn('utils', `Invalid tier for token ${tokenId} in ${contractKey}: tier=${tier}, maxTier=${maxTier}, defaulting to 0`, 'eth', contractKey);
        errorLog.push({
          timestamp: new Date().toISOString(),
          phase: 'fetch_tier',
          tokenId,
          wallet,
          error: `Invalid tier ${tier}`,
          details: { rawResult: safeStringify(tierResult.result), maxTier, parsedTier: tier }
        });
        tier = 0;
      }
    } else {
      logger.error('utils', `Failed to fetch tier for token ${tokenId}: ${tierResult.error || 'unknown error'}`, 'eth', contractKey);
      errorLog.push({
        timestamp: new Date().toISOString(),
        phase: 'fetch_tier',
        tokenId,
        wallet,
        error: tierResult.error || 'unknown error',
        details: { rawResult: safeStringify(tierResult.result) }
      });
      return;
    }

    if (contractKey === 'ascendant' && rarity >= 0 && rarity < rarityDistribution.length) {
      rarityDistribution[rarity] += 1;
    } else if (contractKey === 'ascendant') {
      logger.warn('utils', `Invalid rarity for token ${tokenId}: rarity=${rarity}, maxRarity=${rarityDistribution.length - 1}`, 'eth', contractKey);
      errorLog.push({
        timestamp: new Date().toISOString(),
        phase: 'fetch_rarity',
        tokenId,
        wallet,
        error: `Invalid rarity ${rarity}`
      });
    }

    const holder = holdersMap.get(wallet) || {
      wallet,
      tokenIds: [],
      tiers: Array(maxTier + 1).fill(0),
      total: 0,
      multiplierSum: 0,
      ...(contractKey === 'element369' ? { infernoRewards: 0, fluxRewards: 0, e280Rewards: 0 } : {}),
      ...(contractKey === 'element280' || contractKey === 'stax' ? { claimableRewards: 0 } : {}),
      ...(contractKey === 'ascendant' ? {
        shares: 0,
        lockedAscendant: 0,
        pendingDay8: toDistributeDay8 / totalTokens * 8 / 100,
        pendingDay28: toDistributeDay28 / totalTokens * 28 / 100,
        pendingDay90: toDistributeDay90 / totalTokens * 90 / 100,
        claimableRewards: 0,
        tokens: []
      } : {})
    };

    if (holder.tokenIds.includes(tokenId)) {
      logger.warn('utils', `Duplicate tokenId ${tokenId} for wallet ${wallet} in holdersMap`, 'eth', contractKey);
      errorLog.push({ timestamp: new Date().toISOString(), phase: 'build_holders', tokenId, wallet, error: 'Duplicate tokenId in holdersMap' });
      return;
    }

    holder.tokenIds.push(tokenId);
    holder.total += 1;
    holder.tiers[tier] += 1;
    holder.multiplierSum += contractTiers[tier + 1]?.multiplier || (tier + 1);
    if (contractKey === 'ascendant') {
      holder.shares += shares;
      holder.lockedAscendant += lockedAscendant;
      holder.tokens.push({
        tokenId: Number(tokenId),
        tier: tier + 1,
        rawTier: tier,
        rarityNumber,
        rarity
      });
    }
    holdersMap.set(wallet, holder);
    tierDistribution[tier] += 1;
  });

  cacheState.progressState.step = 'finalizing';
  cacheState.progressState.progressPercentage = '90%';
  await saveCacheStateContract(contractKey, cacheState);

  const totalLiveHolders = holdersMap.size;
  cacheState.progressState.totalOwners = totalLiveHolders;
  let holderList = Array.from(holdersMap.values());
  holderList.forEach((holder, index) => {
    holder.rank = index + 1;
    holder.percentage = (holder.total / totalTokens * 100) || 0;
    holder.displayMultiplierSum = holder.multiplierSum;
  });

  holderList.sort((a, b) => {
    if (contractKey === 'ascendant') {
      return b.shares - a.shares || b.total - a.total;
    }
    return b.total - a.total || b.multiplierSum - a.multiplierSum;
  });
  holderList.forEach((holder, index) => {
    holder.rank = index + 1;
  });

  cacheState.globalMetrics = {
    ...(contractKey === 'element280' || contractKey === 'stax' || contractKey === 'ascendant' ? { totalMinted: totalTokens + totalBurned } : {}),
    totalLive: totalTokens,
    totalBurned,
    tierDistribution,
    ...(contractKey === 'ascendant' ? {
      totalLockedAscendant,
      totalShares,
      toDistributeDay8,
      toDistributeDay28,
      toDistributeDay90,
      pendingRewards: toDistributeDay8 + toDistributeDay28 + toDistributeDay90,
      rarityDistribution
    } : {})
  };
  cacheState.progressState.isPopulating = false;
  cacheState.progressState.step = 'completed';
  cacheState.progressState.processedNfts = totalTokens;
  cacheState.progressState.processedTiers = missingTierTokenIds.length;
  cacheState.progressState.progressPercentage = '100%';
  cacheState.progressState.lastProcessedBlock = Number(currentBlock);
  cacheState.progressState.lastUpdated = Date.now();
  await saveCacheStateContract(contractKey, cacheState);

  await setCache(`${contractKey}_holders`, { holders: holderList, totalBurned, timestamp: Date.now(), rarityDistribution }, 0, contractKey);
  if (contractKey === 'element280') {
    await setCache(`${contractKey}_tiers`, Object.fromEntries(cachedTokenTiers), config.cache.nodeCache.stdTTL || 86400, contractKey);
  }
  logger.info('utils', `Completed holders map with ${holderList.length} holders, totalBurned=${totalBurned}, cachedTiers=${cachedTokenTiers.size}`, 'eth', contractKey);
  if (!config.debug.suppressDebug) {
    logger.debug('utils', `Tier distribution for ${contractKey}: ${tierDistribution}`, 'eth', contractKey);
    if (contractKey === 'ascendant') {
      logger.debug('utils', `Rarity distribution for ${contractKey}: ${rarityDistribution}`, 'eth', contractKey);
    }
  }

  return { holdersMap, totalBurned, lastBlock: Number(currentBlock), errorLog, rarityDistribution };
}

// Populate holders map cache
async function populateHoldersMapCache(contractKey, contractAddress, abi, vaultAddress, vaultAbi, forceUpdate = false) {
  try {
    const cacheState = await getCacheState(contractKey.toLowerCase());
    if (!forceUpdate && cacheState.isPopulating) {
      logger.info('utils', `Cache population already in progress for ${contractKey}`, 'eth', contractKey);
      return { status: 'pending', holders: [] };
    }

    cacheState.isPopulating = true;
    await saveCacheStateContract(contractKey.toLowerCase(), cacheState);

    const { holdersMap, totalBurned } = await getHoldersMap(
      contractKey,
      contractAddress,
      abi,
      vaultAddress,
      vaultAbi,
      cacheState,
      forceUpdate
    );

    const holderList = [];
    for (const [wallet, data] of holdersMap) {
      holderList.push({
        wallet,
        total: data.total,
        tokenIds: data.tokenIds,
        tiers: data.tiers,
        multiplierSum: data.multiplierSum,
        shares: data.shares || 0,
        lockedAscendant: data.lockedAscendant || 0,
        claimableRewards: data.claimableRewards || 0,
        pendingDay8: data.pendingDay8 || 0,
        pendingDay28: data.pendingDay28 || 0,
        pendingDay90: data.pendingDay90 || 0,
        infernoRewards: data.infernoRewards || 0,
        fluxRewards: data.fluxRewards || 0,
        e280Rewards: data.e280Rewards || 0,
        percentage: data.percentage || 0,
        displayMultiplierSum: data.displayMultiplierSum || data.multiplierSum,
        rank: 0, // Will be set later
        ...(contractKey === 'ascendant' ? { tokens: data.tokens || [] } : {}) // Include tokens for ascendant
      });
    }

    // Sort and set ranks
    holderList.sort((a, b) => (contractKey === 'ascendant' ? b.shares - a.shares : b.multiplierSum - a.multiplierSum) || b.total - a.total);
    holderList.forEach((holder, index) => {
      holder.rank = index + 1;
    });

    const isBurnContract = ['stax', 'element280', 'element369'].includes(contractKey.toLowerCase());
    const cacheTotalBurned = isBurnContract ? totalBurned : 0; // 0 for ascendant
    const cacheData = {
      holders: holderList,
      totalBurned: cacheTotalBurned,
      timestamp: Date.now(),
    };

    // Validate cache data
    if (!Array.isArray(cacheData.holders) || (isBurnContract && typeof cacheData.totalBurned !== 'number')) {
      logger.error('utils', `Invalid cache data for ${contractKey}: ${JSON.stringify(cacheData)}`, 'eth', contractKey);
      throw new Error('Invalid cache data');
    }

    logger.info('utils', `Saving cache for ${contractKey}: totalBurned=${cacheTotalBurned}, holders=${holderList.length}`, 'eth', contractKey);
    await setCache(`${contractKey.toLowerCase()}_holders`, cacheData, 0, contractKey.toLowerCase());

    cacheState.isPopulating = false;
    cacheState.phase = 'Completed';
    cacheState.progressPercentage = '100.0';
    cacheState.totalLiveHolders = holderList.length;
    cacheState.totalOwners = holderList.length;
    await saveCacheStateContract(contractKey.toLowerCase(), cacheState);

    logger.info('utils', `Cache populated: ${holderList.length} holders, totalBurned: ${cacheTotalBurned}`, 'eth', contractKey);
    return { status: 'success', holders: holderList };
  } catch (error) {
    logger.error('utils', `Failed to populate holders cache for ${contractKey}: ${error.message}`, { stack: error.stack }, 'eth', contractKey);
    cacheState.isPopulating = false;
    cacheState.error = error.message;
    await saveCacheStateContract(contractKey.toLowerCase(), cacheState);
    return { status: 'error', holders: [] };
  }
}

// GET handler
export async function GET(request, { params }) {
  const { contract } = await params;
  const contractKey = contract.toLowerCase();

  if (!config.nftContracts[contractKey]) {
    logger.error('utils', `Invalid contract: ${contractKey}`, 'eth', contractKey);
    return NextResponse.json({ error: 'Invalid contract' }, { status: 400 });
  }

  const { contractAddress, abi } = config.nftContracts[contractKey];
  const cacheState = await getCacheState(contractKey);

  const { searchParams } = new URL(request.url);
  const page = parseInt(searchParams.get('page') || '0', 10);
  const pageSize = parseInt(searchParams.get('pageSize') || config.contractDetails[contractKey].pageSize, 10);

  const cachedData = await getCache(`${contractKey}_holders`, contractKey);
  const isBurnContract = ['stax', 'element280', 'element369'].includes(contractKey);

  if (cachedData) {
    const holders = cachedData.holders.slice(page * pageSize, (page + 1) * pageSize);
    const totalPages = Math.ceil(cachedData.holders.length / pageSize);
    const totalTokens = cachedData.holders.reduce((sum, h) => sum + h.total, 0);
    const totalBurned = isBurnContract ? Number(cachedData.totalBurned) || 0 : 0;
    const maxTier = Math.max(...Object.keys(config.nftContracts[contractKey]?.tiers || {}).map(Number), 0);
    const response = {
      holders: sanitizeBigInt(holders),
      totalPages,
      totalTokens,
      totalBurned,
      summary: {
        totalLive: totalTokens,
        totalBurned,
        totalMinted: totalTokens + totalBurned,
        tierDistribution: cachedData.holders.reduce((acc, h) => {
          h.tiers.forEach((count, i) => acc[i] = (acc[i] || 0) + count);
          return acc;
        }, Array(contractKey === 'ascendant' ? maxTier + 1 : maxTier).fill(0)), // 9 tiers for ascendant
        multiplierPool: cachedData.holders.reduce((sum, h) => sum + h.multiplierSum, 0),
        ...(contractKey === 'ascendant' ? {
          rarityDistribution: cacheState.globalMetrics.rarityDistribution || Array(3).fill(0)
        } : {})
      },
      globalMetrics: cacheState.globalMetrics || {},
    };
    if (!config.debug.suppressDebug) {
      logger.debug('utils', `GET response for ${contractKey}: holders=${holders.length}, totalPages=${totalPages}`, 'eth', contractKey);
    }
    return NextResponse.json(response);
  }

  const { status, holders } = await populateHoldersMapCache(contractKey, contractAddress, abi, null, null);
  if (status === 'error') {
    logger.error('utils', `Cache population failed for ${contractKey}`, 'eth', contractKey);
    throw new Error('Cache population failed');
  }

  const paginatedHolders = holders.slice(page * pageSize, (page + 1) * pageSize);
  const totalPages = Math.ceil(holders.length / pageSize);
  const cachedDataAfterPopulation = await getCache(`${contractKey}_holders`, contractKey);
  const totalBurned = isBurnContract ? Number(cachedDataAfterPopulation?.totalBurned) || 0 : 0;
  const totalTokens = holders.reduce((sum, h) => sum + h.total, 0);
  const maxTier = Math.max(...Object.keys(config.nftContracts[contractKey]?.tiers || {}).map(Number), 0);
  const response = {
    holders: sanitizeBigInt(paginatedHolders),
    totalPages,
    totalTokens,
    totalBurned,
    summary: {
      totalLive: totalTokens,
      totalBurned,
      totalMinted: totalTokens + totalBurned,
      tierDistribution: holders.reduce((acc, h) => {
        h.tiers.forEach((count, i) => acc[i] = (acc[i] || 0) + count);
        return acc;
      }, Array(contractKey === 'ascendant' ? maxTier + 1 : maxTier).fill(0)), // 9 tiers for ascendant
      multiplierPool: holders.reduce((sum, h) => sum + h.multiplierSum, 0),
      ...(contractKey === 'ascendant' ? {
        rarityDistribution: cacheState.globalMetrics.rarityDistribution || Array(3).fill(0)
      } : {})
    },
    globalMetrics: cacheState.globalMetrics || {},
  };
  if (!config.debug.suppressDebug) {
    logger.debug('utils', `GET response for ${contractKey}: holders=${paginatedHolders.length}, totalPages=${totalPages}`, 'eth', contractKey);
  }
  return NextResponse.json(response);
}

// POST handler
export async function POST(request, { params }) {
  const resolvedParams = await params; // Await params for Next.js App Router
  const { contract: contractKey } = resolvedParams;
  const normalizedContractKey = contractKey.toLowerCase();

  const { forceUpdate = false } = await request.json().catch(() => ({}));

  // Early validation of contractKey
  if (!config.nftContracts[normalizedContractKey]) {
    logger.error('utils', `Invalid contract: ${normalizedContractKey}`, 'eth', normalizedContractKey);
    return NextResponse.json({ message: `Invalid contract: ${normalizedContractKey}`, status: 'error' }, { status: 400 });
  }

  let contractAddress, abi, vaultAddress, vaultAbi;
  try {
    const contractConfig = config.nftContracts[normalizedContractKey];
    ({ contractAddress, abi, vaultAddress, vaultAbi } = contractConfig);
    logger.info('utils', `POST for ${normalizedContractKey}: abiType=${Array.isArray(abi) ? 'array' : typeof abi}, abiLength=${Array.isArray(abi) ? abi.length : 'N/A'}`, 'eth', normalizedContractKey);
    if (!contractAddress) {
      throw new Error(`Contract address not configured for ${normalizedContractKey}`);
    }
    if (!Array.isArray(abi) && !contractConfig.disabled) {
      throw new Error(`Invalid ABI for ${normalizedContractKey}: expected array, got ${typeof abi}`);
    }
    if (validateContract) {
      try {
        await validateContract(normalizedContractKey);
      } catch (error) {
        logger.warn('utils', `validateContract failed for ${normalizedContractKey}: ${error.message}. Proceeding without validation.`, 'eth', normalizedContractKey);
      }
    }
  } catch (error) {
    logger.error('utils', `Validation error for ${normalizedContractKey}: ${error.message}`, { stack: error.stack }, 'eth', normalizedContractKey);
    return NextResponse.json({ message: error.message, status: 'error' }, { status: 400 });
  }

  const cacheState = await getCacheState(normalizedContractKey);
  if (cacheState.isPopulating) {
    logger.info('utils', `Cache population already in progress for ${normalizedContractKey}`, 'eth', normalizedContractKey);
    return NextResponse.json({ message: `${normalizedContractKey} cache population already in progress`, status: 'in_progress' }, { status: 202 });
  }

  if (forceUpdate) {
    await setCache(`${normalizedContractKey}_holders`, null, 0, normalizedContractKey);
    cacheState.progressState = { step: 'idle', processedNfts: 0, totalNfts: 0, processedTiers: 0, totalTiers: 0, error: null, errorLog: [] };
    logger.info('utils', `Cleared cache for ${normalizedContractKey} due to forceUpdate`, 'eth', normalizedContractKey);
  }

  cacheState.isPopulating = true;
  await saveCacheStateContract(normalizedContractKey, cacheState);

  setTimeout(async () => {
    try {
      await getHoldersMap(normalizedContractKey, contractAddress, abi, vaultAddress, vaultAbi, cacheState, forceUpdate);
      logger.info('utils', `Cache population completed for ${normalizedContractKey}: ${cacheState.totalOwners} holders`, 'eth', normalizedContractKey);
    } catch (error) {
      cacheState.progressState.error = error.message;
      cacheState.progressState.errorLog.push({
        timestamp: new Date().toISOString(),
        phase: 'cache_update',
        error: error.message,
      });
      logger.error('utils', `Cache population failed for ${normalizedContractKey}: ${error.message}`, { stack: error.stack }, 'eth', normalizedContractKey);
    } finally {
      cacheState.isPopulating = false;
      cacheState.lastUpdated = new Date().toISOString();
      await saveCacheStateContract(normalizedContractKey, cacheState);
    }
  }, 0);

  logger.info('utils', `Cache population triggered for ${normalizedContractKey}`, 'eth', normalizedContractKey);
  return NextResponse.json({ message: `${normalizedContractKey} cache population triggered`, status: 'success' }, { status: 202 });
}
----- app/api/utils.js -----

import NodeCache from 'node-cache';
import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';
import { Redis } from '@upstash/redis';
import { createPublicClient, http } from 'viem';
import { mainnet } from 'viem/chains';
import { Alchemy } from 'alchemy-sdk';
import config from '@/contracts/config';
import pLimit from 'p-limit';
import { logger } from '@/app/lib/logger';
import chalk from 'chalk';

console.log(chalk.cyan('[Utils] Initializing utils...'));
logger.info('utils', 'Utils module loaded', 'eth', 'general').catch(error => {
  console.error(chalk.red('[Utils] Logger error:'), error.message);
});

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const isDebug = process.env.DEBUG === 'true';
const isProduction = process.env.NODE_ENV === 'production';

const cache = new NodeCache({
  stdTTL: 0,
  checkperiod: 120,
});

const cacheDir = path.join(process.cwd(), 'cache');

const redisEnabled = Object.keys(config.nftContracts).some(
  contract => process.env[`DISABLE_${contract.toUpperCase()}_REDIS`] !== 'true' && process.env.UPSTASH_REDIS_REST_URL && process.env.UPSTASH_REDIS_REST_TOKEN
);
let redis = null;

if (redisEnabled) {
  try {
    redis = new Redis({
      url: process.env.UPSTASH_REDIS_REST_URL,
      token: process.env.UPSTASH_REDIS_REST_TOKEN,
    });
    logger.info('utils', 'Upstash Redis initialized', 'eth', 'general');
  } catch (error) {
    logger.error('utils', `Failed to initialize Upstash Redis: ${error.message}`, { stack: error.stack }, 'eth', 'general');
    redis = null;
  }
}

const alchemyApiKey = config.alchemy.apiKey || process.env.NEXT_PUBLIC_ALCHEMY_API_KEY;
if (!alchemyApiKey) {
  logger.error('utils', 'Alchemy API key is missing', {}, 'eth', 'general');
  throw new Error('Alchemy API key is missing');
}

const client = createPublicClient({
  chain: mainnet,
  transport: http(`https://eth-mainnet.g.alchemy.com/v2/${alchemyApiKey}`),
});

const alchemy = new Alchemy({
  apiKey: config.alchemy.apiKey,
  network: 'eth-mainnet',
});

async function ensureCacheDir() {
  try {
    await fs.mkdir(cacheDir, { recursive: true });
    await fs.chmod(cacheDir, 0o755);
    logger.info('utils', `Created/chmod cache directory: ${cacheDir}`, 'eth', 'general');
  } catch (error) {
    logger.error('utils', `Failed to create/chmod cache directory ${cacheDir}: ${error.message}`, { stack: error.stack }, 'eth', 'general');
    throw error;
  }
}

async function initializeCache() {
  try {
    logger.info('utils', 'Starting cache initialization', 'eth', 'general');
    await ensureCacheDir();

    const testKey = 'test_node_cache';
    const testValue = { ready: true };
    const nodeCacheSuccess = cache.set(testKey, testValue);
    if (nodeCacheSuccess) {
      logger.info('utils', 'Node-cache is ready', 'eth', 'general');
      cache.del(testKey);
    } else {
      logger.error('utils', 'Node-cache failed to set test key', {}, 'eth', 'general');
    }

    if (redisEnabled && redis) {
      try {
        await redis.set('test_redis', JSON.stringify(testValue));
        const redisData = await redis.get('test_redis');
        if (redisData && JSON.parse(redisData).ready) {
          logger.info('utils', 'Redis cache is ready', 'eth', 'general');
          await redis.del('test_redis');
        } else {
          logger.error('utils', 'Redis cache test failed: invalid data', {}, 'eth', 'general');
        }
      } catch (error) {
        logger.error('utils', `Redis cache test failed: ${error.message}`, { stack: error.stack }, 'eth', 'general');
      }
    }

    const collections = Object.keys(config.nftContracts).filter(key => !config.nftContracts[key].disabled).map(key => key.toLowerCase());
    for (const collection of collections) {
      const cacheFile = path.join(cacheDir, `${collection}_holders.json`);
      try {
        await fs.access(cacheFile);
        logger.info('utils', `Cache file exists: ${cacheFile}`, 'eth', collection);
      } catch (error) {
        if (error.code === 'ENOENT') {
          await fs.writeFile(cacheFile, JSON.stringify({ holders: [], totalBurned: 0, timestamp: Date.now() }));
          await fs.chmod(cacheFile, 0o644);
          logger.info('utils', `Created empty cache file: ${cacheFile}`, 'eth', collection);
        } else {
          logger.error('utils', `Failed to access cache file ${cacheFile}: ${error.message}`, { stack: error.stack }, 'eth', collection);
        }
      }
    }

    logger.info('utils', 'Cache initialization completed', 'eth', 'general');
    return true;
  } catch (error) {
    logger.error('utils', `Cache initialization error: ${error.message}`, { stack: error.stack }, 'eth', 'general');
    return false;
  }
}

async function retry(operation, { retries, delay = 1000 }) {
  let lastError;
  for (let attempt = 1; attempt <= retries; attempt++) {
    try {
      return await operation();
    } catch (error) {
      lastError = error;
      if (error.message.includes('429') && attempt === retries) {
        logger.error('utils', `Circuit breaker: Rate limit exceeded after ${retries} attempts`, {}, 'eth', 'general');
        throw new Error('Rate limit exceeded');
      }
      logger.warn('utils', `Retry attempt ${attempt}/${retries} failed: ${error.message}`, 'eth', 'general');
      await new Promise(resolve => setTimeout(resolve, delay * Math.min(attempt, 3)));
    }
  }
  throw lastError;
}

async function batchMulticall(calls, batchSize = config.alchemy.batchSize || 10) {
  const results = [];
  const delay = async () => new Promise(resolve => setTimeout(resolve, config.alchemy.batchDelayMs || 500));

  const concurrencyLimit = pLimit(3);
  const batchPromises = [];
  for (let i = 0; i < calls.length; i += batchSize) {
    const batch = calls.slice(i, i + batchSize);
    batchPromises.push(
      concurrencyLimit(async () => {
        try {
          await delay();
          const batchResults = await client.multicall({
            contracts: batch.map(call => ({
              address: call.address,
              abi: call.abi,
              functionName: call.functionName,
              args: call.args || [],
            })),
            allowFailure: true,
          });

          const batchResult = batchResults.map((result, index) => ({
            status: result.status === 'success' ? 'success' : 'failure',
            result: result.status === 'success' ? result.result : null,
            error: result.status === 'failure' ? result.error?.message || 'Unknown error' : null,
          }));
          return batchResult;
        } catch (error) {
          logger.error('utils', `Batch multicall failed: ${error.message}`, { stack: error.stack }, 'eth', 'general');
          return batch.map(() => ({
            status: 'failure',
            result: null,
            error: error.message,
          }));
        }
      })
    );
  }

  const batchResults = (await Promise.all(batchPromises)).flat();
  results.push(...batchResults);
  return results;
}

async function getOwnersForContract(contractAddress, abi, options = {}) {
  let owners = [];
  let pageKey = options.pageKey || null;
  const maxPages = options.maxPages || 10;
  let pageCount = 0;

  logger.debug('utils', `Fetching owners for contract: ${contractAddress} with options: ${JSON.stringify(options)}`, 'eth', 'general');

  do {
    try {
      const response = await alchemy.nft.getOwnersForContract(contractAddress, {
        withTokenBalances: options.withTokenBalances || false,
        pageKey,
      });

      logger.debug('utils', `Raw Alchemy response: ownersExists=${!!response.owners}, isArray=${Array.isArray(response.owners)}, ownersLength=${response.owners?.length || 0}, pageKey=${response.pageKey || null}, responseKeys=${Object.keys(response)}, sampleOwners=${JSON.stringify(response.owners?.slice(0, 2) || [])}`, 'eth', 'general');

      if (!response.owners || !Array.isArray(response.owners)) {
        logger.error('utils', `Invalid Alchemy response for ${contractAddress}: ${JSON.stringify(response)}`, {}, 'eth', 'general');
        throw new Error('Invalid owners response from Alchemy API');
      }

      for (const owner of response.owners) {
        const tokenBalances = owner.tokenBalances || [];
        logger.debug('utils', `Processing owner: ${owner.ownerAddress}, tokenBalancesCount=${tokenBalances.length}`, 'eth', 'general');

        if (tokenBalances.length > 0) {
          const validBalances = tokenBalances.filter(
            tb => tb.tokenId && Number(tb.balance) > 0
          );
          if (validBalances.length > 0) {
            owners.push({
              ownerAddress: owner.ownerAddress.toLowerCase(),
              tokenBalances: validBalances.map(tb => ({
                tokenId: Number(tb.tokenId),
                balance: Number(tb.balance),
              })),
            });
          }
        }
      }

      pageKey = response.pageKey || null;
      pageCount++;
      logger.debug('utils', `Fetched page ${pageCount}, owners: ${owners.length}, pageKey: ${pageKey}`, 'eth', 'general');

      if (pageCount >= maxPages) {
        logger.warn('utils', `Reached max pages (${maxPages}) for owner fetching`, 'eth', 'general');
        break;
      }
    } catch (error) {
      logger.error('utils', `Failed to fetch owners for ${contractAddress}: ${error.message}`, { stack: error.stack }, 'eth', 'general');
      throw error;
    }
  } while (pageKey);

  logger.debug('utils', `Processed owners: count=${owners.length}, sample=${JSON.stringify(owners.slice(0, 2))}`, 'eth', 'general');
  logger.info('utils', `Fetched ${owners.length} owners for contract: ${contractAddress}`, 'eth', 'general');
  return owners;
}

async function setCache(key, value, ttl, prefix) {
  try {
    const cacheKey = `${prefix}_${key}`;
    const success = cache.set(cacheKey, value);
    logger.info('utils', `Set in-memory cache: ${cacheKey}, success: ${success}, holders: ${value.holders?.length || 'unknown'}`, 'eth', prefix.toLowerCase());

    if (key === `${prefix.toLowerCase()}_holders` && Object.keys(config.nftContracts).map(k => k.toLowerCase()).includes(prefix.toLowerCase())) {
      if (redisEnabled && redis && process.env[`DISABLE_${prefix.toUpperCase()}_REDIS`] !== 'true') {
        try {
          await redis.set(cacheKey, JSON.stringify(value));
          logger.info('utils', `Persisted ${cacheKey} to Redis, holders: ${value.holders.length}`, 'eth', prefix.toLowerCase());
        } catch (error) {
          logger.error('utils', `Failed to persist ${cacheKey} to Redis: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
        }
      } else {
        const cacheFile = path.join(cacheDir, `${prefix.toLowerCase()}_holders.json`);
        logger.info('utils', `Writing to cache file: ${cacheFile}`, 'eth', prefix.toLowerCase());
        await ensureCacheDir();
        try {
          await fs.writeFile(cacheFile, JSON.stringify(value));
          await fs.chmod(cacheFile, 0o644);
          logger.info('utils', `Persisted ${cacheKey} to ${cacheFile}, holders: ${value.holders.length}`, 'eth', prefix.toLowerCase());
        } catch (error) {
          logger.error('utils', `Failed to write cache file ${cacheFile}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
          throw error;
        }
      }
    }
    return success;
  } catch (error) {
    logger.error('utils', `Failed to set cache ${prefix}_${key}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
    return false;
  }
}

async function getCache(key, prefix) {
  try {
    const cacheKey = `${prefix}_${key}`;
    let data = cache.get(cacheKey);
    if (data !== undefined) {
      logger.debug('utils', `Cache hit: ${cacheKey}, holders: ${data.holders?.length || 'unknown'}`, 'eth', prefix.toLowerCase());
      return data;
    }

    if (key === `${prefix.toLowerCase()}_holders` && Object.keys(config.nftContracts).map(k => k.toLowerCase()).includes(prefix.toLowerCase())) {
      if (redisEnabled && redis && process.env[`DISABLE_${prefix.toUpperCase()}_REDIS`] !== 'true') {
        try {
          const redisData = await redis.get(cacheKey);
          if (redisData) {
            const parsed = typeof redisData === 'string' ? JSON.parse(redisData) : redisData;
            if (parsed && Array.isArray(parsed.holders) && Number.isInteger(parsed.totalBurned)) {
              const success = cache.set(cacheKey, parsed);
              logger.info('utils', `Loaded ${cacheKey} from Redis, cached: ${success}, holders: ${parsed.holders.length}`, 'eth', prefix.toLowerCase());
              return parsed;
            } else {
              logger.warn('utils', `Invalid data in Redis for ${cacheKey}`, 'eth', prefix.toLowerCase());
            }
          }
        } catch (error) {
          logger.error('utils', `Failed to load cache from Redis for ${cacheKey}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
        }
      }

      const cacheFile = path.join(cacheDir, `${prefix.toLowerCase()}_holders.json`);
      try {
        const fileData = await fs.readFile(cacheFile, 'utf8');
        const parsed = JSON.parse(fileData);
        if (parsed && Array.isArray(parsed.holders) && Number.isInteger(parsed.totalBurned)) {
          const success = cache.set(cacheKey, parsed);
          logger.info('utils', `Loaded ${cacheKey} from ${cacheFile}, cached: ${success}, holders: ${parsed.holders.length}`, 'eth', prefix.toLowerCase());
          return parsed;
        } else {
          logger.warn('utils', `Invalid data in ${cacheFile}`, 'eth', prefix.toLowerCase());
        }
      } catch (error) {
        if (error.code !== 'ENOENT') {
          logger.error('utils', `Failed to load cache from ${cacheFile}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
        } else {
          logger.debug('utils', `No cache file at ${cacheFile}`, 'eth', prefix.toLowerCase());
        }
      }
    }

    logger.info('utils', `Cache miss: ${cacheKey}`, 'eth', prefix.toLowerCase());
    return null;
  } catch (error) {
    logger.error('utils', `Failed to get cache ${prefix}_${key}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
    return null;
  }
}

async function saveCacheState(collection, state, prefix) {
  try {
    const cacheFile = path.join(cacheDir, `cache_state_${prefix.toLowerCase()}.json`);
    await ensureCacheDir();
    await fs.writeFile(cacheFile, JSON.stringify(state, null, 2));
    await fs.chmod(cacheFile, 0o644);
    logger.debug('utils', `Saved cache state for ${prefix}: ${cacheFile}`, 'eth', prefix.toLowerCase());
  } catch (error) {
    logger.error('utils', `Failed to save cache state for ${prefix}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
  }
}

async function loadCacheState(collection, prefix) {
  try {
    const cacheFile = path.join(cacheDir, `cache_state_${prefix.toLowerCase()}.json`);
    const data = await fs.readFile(cacheFile, 'utf8');
    const parsed = JSON.parse(data);
    logger.debug('utils', `Loaded cache state for ${prefix}: ${cacheFile}`, 'eth', prefix.toLowerCase());
    return parsed;
  } catch (error) {
    if (error.code === 'ENOENT') {
      logger.debug('utils', `No cache state found for ${prefix}, initializing new state`, 'eth', prefix.toLowerCase());
      const defaultState = {
        isPopulating: false,
        totalOwners: 0,
        totalLiveHolders: 0,
        progressState: {
          step: 'idle',
          processedNfts: 0,
          totalNfts: 0,
          processedTiers: 0,
          totalTiers: 0,
          error: null,
          errorLog: [],
        },
        lastUpdated: null,
        lastProcessedBlock: null,
        globalMetrics: {},
      };
      await saveCacheState(collection, defaultState, prefix);
      return defaultState;
    }
    logger.error('utils', `Failed to load cache state for ${prefix}: ${error.message}`, { stack: error.stack }, 'eth', prefix.toLowerCase());
    return null;
  }
}

async function getTransactionReceipt(transactionHash) {
  try {
    const receipt = await client.getTransactionReceipt({ hash: transactionHash });
    logger.debug('utils', `Fetched transaction receipt for ${transactionHash}`, 'eth', 'general');
    return receipt;
  } catch (error) {
    logger.error('utils', `Failed to fetch transaction receipt for ${transactionHash}: ${error.message}`, { stack: error.stack }, 'eth', 'general');
    throw error;
  }
}

async function validateContract(contractKey) {
  const normalizedKey = contractKey.toLowerCase();
  if (!config.contractDetails[normalizedKey]) {
    throw new Error(`Invalid contract: ${normalizedKey}`);
  }
  if (config.contractDetails[normalizedKey].disabled) {
    throw new Error(`${normalizedKey} contract not deployed`);
  }
  const abi = config.abis[normalizedKey]?.nft;
  if (!abi || !Array.isArray(abi)) {
    throw new Error(`${normalizedKey} NFT ABI missing or invalid`);
  }
  const contractAddress = config.contractAddresses[normalizedKey]?.address;
  if (!contractAddress || !/^0x[a-fA-F0-9]{40}$/.test(contractAddress)) {
    throw new Error(`${normalizedKey} contract address missing or invalid`);
  }
  return {
    contractAddress,
    abi,
  };
}

async function log(scope, message, chain = 'eth', collection = 'general') {
  await logger.info(scope, message, chain, collection);
}

export {
  client,
  retry,
  logger,
  getCache,
  setCache,
  saveCacheState,
  loadCacheState,
  batchMulticall,
  getOwnersForContract,
  getTransactionReceipt,
  initializeCache,
  validateContract,
  log,
};
----- app/lib/chartOptions.js -----

export const barChartOptions = {
    responsive: true,
    plugins: {
      legend: { position: 'top', labels: { color: '#e5e7eb' } }, // Gray-200
      title: {
        display: true,
        text: 'NFT Tier Distribution',
        color: '#e5e7eb',
        font: { size: 16, weight: 'bold' },
      },
    },
    scales: {
      y: {
        beginAtZero: true,
        title: { display: true, text: 'Number of NFTs', color: '#e5e7eb' },
        ticks: { color: '#d1d5db' }, // Gray-300
      },
      x: {
        title: { display: true, text: 'Tiers', color: '#e5e7eb' },
        ticks: { color: '#d1d5db' },
      },
    },
  };
----- app/lib/fetchCollectionData.js -----

import config from '@/contracts/config';
import { HoldersResponseSchema, ProgressResponseSchema } from '@/app/lib/schemas';

// Debounce utility to prevent concurrent POST requests
const debounce = (func, wait) => {
  let timeout;
  return (...args) => {
    clearTimeout(timeout);
    return new Promise(resolve => {
      timeout = setTimeout(() => resolve(func(...args)), wait);
    });
  };
};

export async function fetchCollectionData(apiKey, apiEndpoint, pageSize) {
  console.log(`[FetchCollectionData] [INFO] Fetching ${apiKey} from ${apiEndpoint}`);
  try {
    if (apiKey === 'e280' || config.contractDetails[apiKey]?.disabled) {
      console.log(`[FetchCollectionData] [INFO] ${apiKey} is disabled`);
      return { holders: [], totalTokens: 0, totalShares: 0, totalBurned: 0, error: 'Contract not deployed' };
    }

    const endpoint = apiEndpoint.startsWith('http')
      ? apiEndpoint
      : `${process.env.NEXT_PUBLIC_API_BASE_URL || 'http://localhost:3000'}${apiEndpoint}`;

    const pollProgress = async () => {
      const res = await fetch(`${endpoint}/progress`, {
        cache: 'no-store',
        signal: AbortSignal.timeout(config.alchemy.timeoutMs),
      });
      if (!res.ok) {
        const errorText = await res.text();
        throw new Error(`Progress fetch failed: ${res.status} ${errorText}`);
      }
      const progress = await res.json();
      console.log(`[FetchCollectionData] [DEBUG] Progress: ${JSON.stringify(progress)}`);
      const validation = ProgressResponseSchema.safeParse(progress);
      if (!validation.success) {
        console.error(`[FetchCollectionData] [ERROR] Invalid progress data: ${JSON.stringify(validation.error.errors)}`);
        throw new Error('Invalid progress data');
      }
      return validation.data;
    };

    let allHolders = [];
    let totalTokens = 0;
    let totalShares = 0;
    let totalBurned = 0;
    let summary = {};
    let page = 0;
    let totalPages = Infinity;
    let postAttempts = 0;
    const maxPostAttempts = 5;
    let pollAttempts = 0;
    const maxPollAttempts = 600; // 300 seconds / 500ms = 600 attempts
    const maxPollTime = 300000; // 300 seconds
    const startTime = Date.now();

    // Debounced POST request
    const triggerPost = debounce(async () => {
      console.log(`[FetchCollectionData] [INFO] Triggering POST for ${apiKey}, attempt ${postAttempts + 1}/${maxPostAttempts}`);
      const res = await fetch(endpoint, {
        method: 'POST',
        cache: 'no-store',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ forceUpdate: false }), // Never force update automatically
      });
      if (!res.ok) {
        const errorText = await res.text();
        console.error(`[FetchCollectionData] [ERROR] POST failed: ${res.status} ${errorText}`);
        throw new Error(`POST request failed: ${res.status} ${errorText}`);
      }
      const response = await res.json();
      if (response.error) {
        throw new Error(`POST response error: ${response.error}`);
      }
      console.log(`[FetchCollectionData] [INFO] POST successful: ${JSON.stringify(response)}`);
      return response;
    }, 2000);

    let progress = await pollProgress();
    // Only trigger POST for element280 if phase is Idle and no valid data exists
    if (apiKey === 'element280' && progress.phase === 'Idle' && progress.totalOwners === 0 && progress.lastProcessedBlock === null) {
      if (postAttempts >= maxPostAttempts) {
        console.error(`[FetchCollectionData] [ERROR] Max POST attempts (${maxPostAttempts}) reached for ${apiKey}`);
        return { holders: [], totalTokens: 0, totalShares: 0, totalBurned: 0, error: 'Max POST attempts reached for cache population' };
      }
      try {
        console.log(`[FetchCollectionData] [DEBUG] Sending POST request, attempt ${postAttempts + 1}/${maxPostAttempts}`);
        await triggerPost();
        postAttempts++;
      } catch (error) {
        console.error(`[FetchCollectionData] [ERROR] POST attempt failed: ${error.message}`);
        postAttempts++;
        if (postAttempts >= maxPostAttempts) {
          console.error(`[FetchCollectionData] [ERROR] Max POST attempts (${maxPostAttempts}) reached after error`);
          return { holders: [], totalTokens: 0, totalShares: 0, totalBurned: 0, error: `Max POST attempts reached: ${error.message}` };
        }
      }
    } else if (apiKey !== 'element280' && (progress.phase === 'Idle' || progress.totalOwners === 0)) {
      // Original POST triggering logic for other contracts
      if (postAttempts >= maxPostAttempts) {
        console.error(`[FetchCollectionData] [ERROR] Max POST attempts (${maxPostAttempts}) reached for ${apiKey}`);
        return { holders: [], totalTokens: 0, totalShares: 0, totalBurned: 0, error: 'Max POST attempts reached for cache population' };
      }
      try {
        console.log(`[FetchCollectionData] [DEBUG] Sending POST request, attempt ${postAttempts + 1}/${maxPostAttempts}`);
        await triggerPost();
        postAttempts++;
      } catch (error) {
        console.error(`[FetchCollectionData] [ERROR] POST attempt failed: ${error.message}`);
        postAttempts++;
        if (postAttempts >= maxPostAttempts) {
          console.error(`[FetchCollectionData] [ERROR] Max POST attempts (${maxPostAttempts}) reached after error`);
          return { holders: [], totalTokens: 0, totalShares: 0, totalBurned: 0, error: `Max POST attempts reached: ${error.message}` };
        }
      }
    }

    while (progress.phase !== 'Completed' && progress.phase !== 'Error') {
      if (Date.now() - startTime > maxPollTime) {
        console.error(`[FetchCollectionData] [ERROR] Cache population timeout for ${apiKey}`);
        return { holders: [], totalTokens: 0, totalShares: 0, totalBurned: 0, error: 'Cache population timed out' };
      }
      if (pollAttempts >= maxPollAttempts) {
        console.error(`[FetchCollectionData] [ERROR] Max poll attempts (${maxPollAttempts}) reached for ${apiKey}`);
        return { holders: [], totalTokens: 0, totalShares: 0, totalBurned: 0, error: 'Max poll attempts reached' };
      }
      console.log(`[FetchCollectionData] [INFO] Waiting for ${apiKey} cache: ${progress.phase} (${progress.progressPercentage}%), poll attempt ${pollAttempts + 1}/${maxPollAttempts}`);
      await new Promise(resolve => setTimeout(resolve, config.alchemy.batchDelayMs));
      try {
        progress = await pollProgress();
      } catch (error) {
        console.error(`[FetchCollectionData] [ERROR] Poll attempt failed: ${error.message}`);
        return { holders: [], totalTokens: 0, totalShares: 0, totalBurned: 0, error: `Polling failed: ${error.message}` };
      }
      pollAttempts++;
    }

    if (progress.phase === 'Error') {
      console.error(`[FetchCollectionData] [ERROR] Cache population failed for ${apiKey}: ${progress.error || 'Unknown error'}`);
      return { holders: [], totalTokens: 0, totalShares: 0, totalBurned: 0, error: `Cache population failed: ${progress.error || 'Unknown error'}` };
    }

    while (page < totalPages) {
      const url = `${endpoint}?page=${page}&pageSize=${pageSize}`;
      console.log(`[FetchCollectionData] [DEBUG] Fetching ${url}`);
      const res = await fetch(url, { cache: 'force-cache' });
      console.log(`[FetchCollectionData] [DEBUG] Status: ${res.status}, headers: ${JSON.stringify([...res.headers])}`);

      if (res.status === 202) {
        console.log(`[FetchCollectionData] [INFO] Cache still populating for ${apiKey}, retrying...`);
        await new Promise(resolve => setTimeout(resolve, config.alchemy.batchDelayMs));
        continue;
      }

      if (!res.ok) {
        const errorText = await res.text();
        console.error(`[FetchCollectionData] [ERROR] Failed: ${res.status} ${errorText}`);
        return { holders: [], totalTokens: 0, totalShares: 0, totalBurned: 0, error: `API request failed: ${res.status} ${errorText}` };
      }

      const json = await res.json();
      console.log(`[FetchCollectionData] [DEBUG] Response: ${JSON.stringify(json, (k, v) => typeof v === 'bigint' ? v.toString() : v)}`);

      if (json.error) {
        console.error(`[FetchCollectionData] [ERROR] API error for ${apiKey}: ${json.error}`);
        return { holders: [], totalTokens: 0, totalShares: 0, totalBurned: 0, error: json.error };
      }

      const validation = HoldersResponseSchema.safeParse(json);
      if (!validation.success) {
        console.error(`[FetchCollectionData] [ERROR] Invalid holders data: ${JSON.stringify(validation.error.errors)}`);
        return { holders: [], totalTokens: 0, totalShares: 0, totalBurned: 0, error: 'Invalid holders data' };
      }

      allHolders = allHolders.concat(json.holders);
      totalTokens = json.totalTokens || json.summary?.totalLive || totalTokens;
      totalShares = json.totalShares || json.summary?.multiplierPool || totalTokens;
      totalBurned = json.totalBurned || totalBurned;
      summary = json.summary || summary;
      totalPages = json.totalPages || 1;
      page++;
      console.log(`[FetchCollectionData] [INFO] Fetched page ${page}: ${json.holders.length} holders`);
    }

    return { holders: allHolders, totalTokens, totalShares, totalBurned, summary };
  } catch (error) {
    console.error(`[FetchCollectionData] [ERROR] ${apiKey}: ${error.message}, stack: ${error.stack}`);
    return { holders: [], totalTokens: 0, totalShares: 0, totalBurned: 0, error: error.message };
  }
}
----- app/lib/logger.js -----

import fs from 'fs/promises';
import path from 'path';
import chalk from 'chalk';

// Use process.cwd() to reference the project root
const logDir = path.join(process.cwd(), 'logs');

console.log(chalk.cyan('[Logger] Initializing logger...'));
console.log(chalk.cyan('[Logger] process.env.DEBUG:'), process.env.DEBUG);
console.log(chalk.cyan('[Logger] process.env.NODE_ENV:'), process.env.NODE_ENV);
console.log(chalk.cyan('[Logger] Log directory:'), logDir);

const isDebug = process.env.DEBUG === 'true';
console.log(chalk.cyan('[Logger] isDebug:'), isDebug);

async function ensureLogDir() {
  try {
    await fs.mkdir(logDir, { recursive: true });
    await fs.chmod(logDir, 0o755);
    console.log(chalk.cyan('[Logger] Created or verified log directory:'), logDir);
  } catch (error) {
    console.error(chalk.red('[Logger] Failed to create log directory:'), error.message);
  }
}

ensureLogDir().catch(error => {
  console.error(chalk.red('[Logger] ensureLogDir error:'), error.message);
});

export const logger = {
  info: async (scope, message, chain = 'eth', collection = 'general') => {
    const timestamp = new Date().toISOString();
    const log = `[${timestamp}] [${scope}] [INFO] ${message}`;
    console.log(chalk.green(log));
    if (isDebug) {
      try {
        const logFile = path.join(logDir, `cache-${chain}-${collection.toLowerCase()}-${timestamp.split('T')[0]}.log`);
        await fs.appendFile(logFile, `${log}\n`);
        console.log(chalk.cyan('[Logger] Wrote INFO log to:'), logFile);
      } catch (error) {
        console.error(chalk.red('[Logger] Failed to write INFO log:'), error.message);
      }
    }
  },
  warn: async (scope, message, chain = 'eth', collection = 'general') => {
    const timestamp = new Date().toISOString();
    const log = `[${timestamp}] [${scope}] [WARN] ${message}`;
    console.log(chalk.yellow(log));
    if (isDebug) {
      try {
        const logFile = path.join(logDir, `cache-${chain}-${collection.toLowerCase()}-${timestamp.split('T')[0]}.log`);
        await fs.appendFile(logFile, `${log}\n`);
        console.log(chalk.cyan('[Logger] Wrote WARN log to:'), logFile);
      } catch (error) {
        console.error(chalk.red('[Logger] Failed to write WARN log:'), error.message);
      }
    }
  },
  error: async (scope, message, details = {}, chain = 'eth', collection = 'general') => {
    const timestamp = new Date().toISOString();
    const log = `[${timestamp}] [${scope}] [ERROR] ${message} ${JSON.stringify(details)}`;
    console.error(chalk.red(log));
    if (isDebug) {
      try {
        const logFile = path.join(logDir, `cache-${chain}-${collection.toLowerCase()}-${timestamp.split('T')[0]}.log`);
        await fs.appendFile(logFile, `${log}\n`);
        console.log(chalk.cyan('[Logger] Wrote ERROR log to:'), logFile);
      } catch (error) {
        console.error(chalk.red('[Logger] Failed to write ERROR log:'), error.message);
      }
    }
  },
  debug: async (scope, message, chain = 'eth', collection = 'general') => {
    if (!isDebug) return;
    const timestamp = new Date().toISOString();
    const log = `[${timestamp}] [${scope}] [DEBUG] ${message}`;
    console.log(chalk.blue(log));
    try {
      const logFile = path.join(logDir, `cache-${chain}-${collection.toLowerCase()}-${timestamp.split('T')[0]}.log`);
      await fs.appendFile(logFile, `${log}\n`);
      console.log(chalk.cyan('[Logger] Wrote DEBUG log to:'), logFile);
    } catch (error) {
      console.error(chalk.red('[Logger] Failed to write DEBUG log:'), error.message);
    }
  },
};

try {
  logger.info('startup', 'Logger module loaded').catch(error => {
    console.error(chalk.red('[Logger] Startup log error:'), error.message);
  });
} catch (error) {
  console.error(chalk.red('[Logger] Immediate log error:'), error.message);
}
----- app/lib/schemas.js -----

import { z } from 'zod';

export const HoldersResponseSchema = z.object({
  holders: z.array(
    z.object({
      wallet: z.string(),
      tokenIds: z.array(z.number()),
      tiers: z.array(z.number()),
      total: z.number(),
      multiplierSum: z.number(),
      shares: z.number().optional(),
      lockedAscendant: z.number().optional(),
      claimableRewards: z.number().optional(),
      pendingDay8: z.number().optional(),
      pendingDay28: z.number().optional(),
      pendingDay90: z.number().optional(),
      infernoRewards: z.number().optional(),
      fluxRewards: z.number().optional(),
      e280Rewards: z.number().optional(),
      percentage: z.number().optional(),
      displayMultiplierSum: z.number().optional(),
      rank: z.number(),
      tokens: z.array(
        z.object({
          tokenId: z.number(),
          tier: z.number(),
          rawTier: z.number().optional(), // Added for debugging
          rarityNumber: z.number(),
          rarity: z.number()
        })
      ).optional()
    })
  ),
  totalPages: z.number(),
  totalTokens: z.number(),
  totalBurned: z.number().nullable(),
  summary: z.object({
    totalLive: z.number(),
    totalBurned: z.number().nullable(),
    totalMinted: z.number(),
    tierDistribution: z.array(z.number()),
    multiplierPool: z.number(),
    rarityDistribution: z.array(z.number()).optional()
  }),
  globalMetrics: z.object({}).optional(),
  contractKey: z.string().optional(),
}).refine(
  (data) => {
    const contractKey = data.contractKey?.toLowerCase();
    if (['stax', 'element280', 'element369'].includes(contractKey)) {
      return typeof data.totalBurned === 'number' && data.totalBurned >= 0 && data.summary != null;
    }
    return true;
  },
  {
    message: 'totalBurned must be a non-negative number and summary must exist for stax, element280, and element369',
    path: ['totalBurned', 'summary'],
  }
);

export const ProgressResponseSchema = z.object({
  isPopulating: z.boolean(),
  totalLiveHolders: z.number(),
  totalOwners: z.number(),
  phase: z.string(),
  progressPercentage: z.string(),
  lastProcessedBlock: z.number().nullable(),
  lastUpdated: z.number().nullable(),
  error: z.string().nullable(),
  errorLog: z.array(z.any()),
  globalMetrics: z.object({}).optional(),
});
----- app/lib/serverInit.js -----

// File: server/lib/serverInit.js
import { logger } from '@/app/lib/logger';
import { initializeCache } from '@/app/api new code/utils';
import chalk from 'chalk';

console.log(chalk.cyan('[ServerInit] Initializing server...'));

try {
  logger.info('serverInit', 'Server initialization started');
  await initializeCache();
} catch (error) {
  logger.error('serverInit', `Initialize cache error: ${error.message}`, { stack: error.stack });
  console.error(chalk.red('[ServerInit] Initialization error:'), error.message);
}

export const serverInit = true;
----- app/lib/useNFTData.js -----

// client/lib/useNFTData.js
'use client';
import { useQuery } from '@tanstack/react-query';
import { useNFTStore } from '@/app/store';
import config from '@/contracts/config';
import { HoldersResponseSchema } from '@/app/lib/schemas';

async function fetchNFTData(apiKey, apiEndpoint, pageSize, page = 0) {
  if (apiKey === 'e280' || config.contractDetails[apiKey]?.disabled) {
    return { holders: [], totalTokens: 0, totalBurned: 0, error: 'Contract not deployed' };
  }

  const endpoint = apiEndpoint.startsWith('http') ? apiEndpoint : `${process.env.NEXT_PUBLIC_API_BASE_URL || 'http://localhost:3000'}${apiEndpoint}`;
  const progressUrl = `${endpoint}/progress`;

  const progressRes = await fetch(progressUrl, { cache: 'no-store' });
  if (!progressRes.ok) throw new Error(`Progress fetch failed: ${progressRes.status}`);
  const progress = await progressRes.json();

  if (progress.isPopulating || progress.phase !== 'Completed') {
    throw new Error('Cache is populating');
  }

  let allHolders = [];
  let totalTokens = 0;
  let totalShares = 0;
  let totalBurned = 0;
  let summary = {};
  let totalPages = Infinity;

  while (page < totalPages) {
    const url = `${endpoint}?page=${page}&pageSize=${pageSize}`;
    const res = await fetch(url, { cache: 'force-cache' });
    if (!res.ok) throw new Error(`API request failed: ${res.status}`);
    const json = await res.json();

    if (json.message === 'Cache is populating' || json.isCachePopulating) {
      throw new Error('Cache is populating');
    }

    const validation = HoldersResponseSchema.safeParse(json);
    if (!validation.success) {
      throw new Error(`Invalid holders schema: ${JSON.stringify(validation.error.errors)}`);
    }

    allHolders = allHolders.concat(json.holders);
    totalTokens = json.totalTokens || json.summary?.totalLive || totalTokens;
    totalShares = json.totalShares || json.summary?.multiplierPool || totalShares;
    totalBurned = json.totalBurned || totalBurned;
    summary = json.summary || summary;
    totalPages = json.totalPages || 1;
    page++;
  }

  return { holders: allHolders, totalTokens, totalShares, totalBurned, summary };
}

export function useNFTData(apiKey, pageSize) {
  const { getCache, setCache } = useNFTStore();

  return useQuery({
    queryKey: ['nft', apiKey],
    queryFn: async () => {
      const cachedData = getCache(apiKey);
      if (cachedData) return cachedData;

      const data = await fetchNFTData(apiKey, config.contractDetails[apiKey].apiEndpoint, pageSize);
      setCache(apiKey, data);
      return data;
    },
    retry: config.alchemy.maxRetries,
    retryDelay: attempt => config.alchemy.batchDelayMs * (attempt + 1),
    staleTime: 30 * 60 * 1000, // 30 minutes
    refetchInterval: progress => (progress?.isPopulating ? 2000 : false),
    onError: error => console.error(`[useNFTData] [ERROR] ${apiKey}: ${error.message}`),
  });
}

================= Includes the following JS files under ./server =================
app/api/holders/Element280/validate-burned/route.js
app/api/holders/[contract]/progress/route.js
app/api/holders/[contract]/route.js
app/api/utils.js
app/lib/chartOptions.js
app/lib/fetchCollectionData.js
app/lib/logger.js
app/lib/schemas.js
app/lib/serverInit.js
app/lib/useNFTData.js
