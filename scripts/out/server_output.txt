================= Includes the following JS files under ./server =================
app/api/holders/Ascendant/route copy.js
app/api/holders/Ascendant/route.js
app/api/holders/E280/route.js
app/api/holders/Element280/route.js
app/api/holders/Element280/validate-burned/route.js
app/api/holders/Element369/route.js
app/api/holders/Stax/route.js
app/api/holders/[contract]/progress/route.js
app/api/holders/[contract]/route.js
app/api/holders/blockchain/events.js
app/api/holders/blockchain/multicall.js
app/api/holders/blockchain/owners.js
app/api/holders/cache/holders.js
app/api/holders/cache/state.js
app/api/utils.js
app/api/utils/cache.js
app/api/utils/client.js
app/api/utils/logging.js
app/api/utils/retry.js


================= Contents of above files in ./server =================


----- app/api/holders/Ascendant/route copy.js -----

import { NextResponse } from 'next/server';
import { alchemy, client, CACHE_TTL, log, batchMulticall } from '../../utils';
import { contractAddresses, contractTiers } from '@/app/nft-contracts';
import { formatUnits, getAddress } from 'viem';
import { v4 as uuidv4 } from 'uuid';
import ascendantABI from '../../../../abi/ascendantNFT.json';

let cache = {};
let tokenCache = new Map();

// Utility to sanitize BigInt values by converting them to strings
function sanitizeBigInt(obj, path = 'root') {
  if (typeof obj === 'bigint') {
    log(`[BigInt Detected] at ${path}: ${obj}`);
    return obj.toString();
  }
  if (Array.isArray(obj)) {
    return obj.map((item, i) => sanitizeBigInt(item, `${path}[${i}]`));
  }
  if (typeof obj === 'object' && obj !== null) {
    const sanitized = {};
    for (const [key, value] of Object.entries(obj)) {
      sanitized[key] = sanitizeBigInt(value, `${path}.${key}`);
    }
    return sanitized;
  }
  return obj;
}

// Utility to safely log objects containing BigInt
function safeLog(obj) {
  return JSON.stringify(obj, (key, value) =>
    typeof value === 'bigint' ? value.toString() : value
  );
}

// Utility to safely serialize response objects containing BigInt
function safeSerialize(obj) {
  return JSON.parse(JSON.stringify(obj, (key, value) =>
    typeof value === 'bigint' ? value.toString() : value
  ));
}

// Fetch data for all holders with pagination
async function getAllHolders(page = 0, pageSize = 1000, requestId = '') {
  const contractAddress = contractAddresses.ascendantNFT;
  const tiers = contractTiers.ascendantNFT;
  const cacheKey = `${contractAddress}-all-${page}-${pageSize}`;
  const now = Date.now();

  if (cache[cacheKey] && now - cache[cacheKey].timestamp < CACHE_TTL) {
    log(`[${requestId}] Returning cached data for ${cacheKey}`);
    return cache[cacheKey].data;
  }

  log(`[${requestId}] Fetching holders, page=${page}, pageSize=${pageSize}`);
  if (!contractAddress || !tiers) {
    throw new Error('Missing contract address or tiers');
  }

  const retry = async (fn, attempts = 3, delay = 1000) => {
    for (let i = 0; i < attempts; i++) {
      try {
        return await fn();
      } catch (error) {
        if (i === attempts - 1) throw error;
        log(`[${requestId}] Retry ${i + 1}/${attempts} failed: ${error.message}`);
        await new Promise((res) => setTimeout(res, delay * 2 ** i));
      }
    }
  };

  let owners = [];
  let pageKey = null;
  do {
    const response = await retry(() =>
      alchemy.nft.getOwnersForContract(contractAddress, {
        block: 'latest',
        withTokenBalances: true,
        pageKey,
      })
    );
    owners = owners.concat(response.owners);
    pageKey = response.pageKey;
  } while (pageKey);
  log(`[${requestId}] Raw owners count: ${owners.length}`);

  const burnAddress = '0x0000000000000000000000000000000000000000';
  const filteredOwners = owners.filter(
    (owner) => owner?.ownerAddress && owner.ownerAddress.toLowerCase() !== burnAddress && owner.tokenBalances?.length > 0
  );
  log(`[${requestId}] Filtered live owners count: ${filteredOwners.length}`);

  const tokenOwnerMap = new Map();
  let totalTokens = 0;
  filteredOwners.forEach((owner) => {
    if (!owner.ownerAddress) {
      log(`[${requestId}] Skipping owner with missing ownerAddress`);
      return;
    }
    let wallet;
    try {
      wallet = getAddress(owner.ownerAddress);
    } catch (e) {
      log(`[${requestId}] Invalid ownerAddress: ${owner.ownerAddress}, error: ${e.message}`);
      return;
    }
    owner.tokenBalances.forEach((tb) => {
      if (!tb.tokenId) {
        log(`[${requestId}] Skipping token with missing tokenId for owner ${wallet}`);
        return;
      }
      const tokenId = Number(tb.tokenId);
      tokenOwnerMap.set(tokenId, wallet);
      totalTokens++;
    });
  });
  log(`[${requestId}] Total tokens checked: ${totalTokens}`);

  const allTokenIds = Array.from(tokenOwnerMap.keys());
  const start = page * pageSize;
  const end = Math.min(start + pageSize, allTokenIds.length);
  const paginatedTokenIds = allTokenIds.slice(start, end);
  log(`[${requestId}] Paginated token IDs: ${paginatedTokenIds.length}`);

  if (paginatedTokenIds.length === 0) {
    const result = {
      holders: [],
      totalTokens,
      totalLockedAscendant: 0,
      totalShares: 0,
      toDistributeDay8: 0,
      toDistributeDay28: 0,
      toDistributeDay90: 0,
      pendingRewards: 0,
      page,
      pageSize,
      totalPages: Math.ceil(totalTokens / pageSize),
    };
    cache[cacheKey] = { timestamp: now, data: result };
    return result;
  }

  const tierCalls = paginatedTokenIds.map((tokenId) => ({
    address: contractAddress,
    abi: ascendantABI,
    functionName: 'getNFTAttribute',
    args: [BigInt(tokenId)],
  }));
  const recordCalls = paginatedTokenIds.map((tokenId) => ({
    address: contractAddress,
    abi: ascendantABI,
    functionName: 'userRecords',
    args: [BigInt(tokenId)],
  }));

  log(`[${requestId}] Tier calls: ${safeLog(tierCalls.map((c) => ({ tokenId: c.args[0], functionName: c.functionName })))}`);
  log(`[${requestId}] Record calls: ${safeLog(recordCalls.map((c) => ({ tokenId: c.args[0], functionName: c.functionName })))}`);

  const [rawTierResults, rawRecordResults] = await Promise.all([
    retry(() => batchMulticall(tierCalls)),
    retry(() => batchMulticall(recordCalls)),
  ]);

  log(`[${requestId}] Raw tierResults: ${safeLog(rawTierResults)}`);

  const tierResults = sanitizeBigInt(rawTierResults, 'tierResults');
  const recordResults = sanitizeBigInt(rawRecordResults, 'recordResults');

  const totalSharesRaw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: ascendantABI,
      functionName: 'totalShares',
    })
  );
  const totalShares = parseFloat(formatUnits(totalSharesRaw.toString(), 18));
  const toDistributeDay8Raw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: ascendantABI,
      functionName: 'toDistribute',
      args: [0],
    })
  );
  const toDistributeDay8 = parseFloat(formatUnits(toDistributeDay8Raw.toString(), 18));
  const toDistributeDay28Raw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: ascendantABI,
      functionName: 'toDistribute',
      args: [1],
    })
  );
  const toDistributeDay28 = parseFloat(formatUnits(toDistributeDay28Raw.toString(), 18));
  const toDistributeDay90Raw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: ascendantABI,
      functionName: 'toDistribute',
      args: [2],
    })
  );
  const toDistributeDay90 = parseFloat(formatUnits(toDistributeDay90Raw.toString(), 18));

  const maxTier = Math.max(...Object.keys(tiers).map(Number));
  const holdersMap = new Map();
  let totalLockedAscendant = 0;

  const walletTokenIds = new Map();
  paginatedTokenIds.forEach((tokenId) => {
    const wallet = tokenOwnerMap.get(tokenId);
    if (!wallet) {
      log(`[${requestId}] Skipping token ${tokenId}: no wallet found`);
      return;
    }
    if (!walletTokenIds.has(wallet)) {
      walletTokenIds.set(wallet, []);
    }
    walletTokenIds.get(wallet).push(tokenId);
  });

  const claimableCalls = Array.from(walletTokenIds.entries()).map(([wallet, tokenIds]) => ({
    address: contractAddress,
    abi: ascendantABI,
    functionName: 'batchClaimableAmount',
    args: [tokenIds.map((id) => BigInt(id))],
  }));

  log(`[${requestId}] Claimable calls: ${safeLog(claimableCalls.map((c) => ({ tokenIds: c.args[0] })))}`);

  const rawClaimableResults = await retry(() => batchMulticall(claimableCalls));
  const claimableResults = sanitizeBigInt(rawClaimableResults, 'claimableResults');

  paginatedTokenIds.forEach((tokenId, i) => {
    const wallet = tokenOwnerMap.get(tokenId);
    if (!wallet) {
      log(`[${requestId}] Skipping token ${tokenId}: no wallet found in holdersMap`);
      return;
    }
    if (!holdersMap.has(wallet)) {
      holdersMap.set(wallet, {
        wallet,
        total: 0,
        multiplierSum: 0,
        tiers: Array(maxTier + 1).fill(0),
        shares: 0,
        lockedAscendant: 0,
        pendingDay8: 0,
        pendingDay28: 0,
        pendingDay90: 0,
        claimableRewards: 0,
      });
    }
    const holder = holdersMap.get(wallet);

    const tierResult = tierResults[i];
    let tier;
    if (tierResult?.status === 'success') {
      if (Array.isArray(tierResult.result) && tierResult.result.length >= 2) {
        tier = Number(tierResult.result[1]);
      } else if (typeof tierResult.result === 'object' && tierResult.result.tier !== undefined) {
        tier = Number(tierResult.result.tier);
      }
    }
    if (tier >= 1 && tier <= maxTier) {
      holder.tiers[tier] += 1;
      holder.total += 1;
      holder.multiplierSum += tiers[tier]?.multiplier || 0;
    } else {
      log(`[${requestId}] Invalid tier ${tier} for tokenId ${tokenId}: ${safeLog(tierResult)}`);
    }

    const recordResult = recordResults[i];
    if (recordResult?.status === 'success' && Array.isArray(recordResult.result)) {
      const sharesRaw = recordResult.result[0] || '0';
      const lockedAscendantRaw = recordResult.result[1] || '0';
      const shares = parseFloat(formatUnits(sharesRaw, 18));
      const lockedAscendant = parseFloat(formatUnits(lockedAscendantRaw, 18));
      holder.shares += shares;
      holder.lockedAscendant += lockedAscendant;
      totalLockedAscendant += lockedAscendant;
    } else {
      log(`[${requestId}] Invalid recordResult for tokenId ${tokenId}: ${safeLog(recordResult)}`);
    }
  });

  let claimableIndex = 0;
  for (const [wallet, tokenIds] of walletTokenIds.entries()) {
    const holder = holdersMap.get(wallet);
    if (!holder) {
      log(`[${requestId}] Skipping claimable for wallet ${wallet}: no holder found`);
      claimableIndex++;
      continue;
    }
    if (claimableResults[claimableIndex]?.status === 'success') {
      const claimableRaw = claimableResults[claimableIndex].result || '0';
      holder.claimableRewards = parseFloat(formatUnits(claimableRaw, 18));
    } else {
      holder.claimableRewards = 0;
    }
    claimableIndex++;
  }

  const holders = Array.from(holdersMap.values());
  const totalMultiplierSum = holders.reduce((sum, h) => sum + h.multiplierSum, 0);
  const pendingRewardPerShareDay8 = totalShares > 0 ? toDistributeDay8 / totalShares : 0;
  const pendingRewardPerShareDay28 = totalShares > 0 ? toDistributeDay28 / totalShares : 0;
  const pendingRewardPerShareDay90 = totalShares > 0 ? toDistributeDay90 / totalShares : 0;

  holders.forEach((holder) => {
    holder.pendingDay8 = holder.shares * pendingRewardPerShareDay8;
    holder.pendingDay28 = holder.shares * pendingRewardPerShareDay28;
    holder.pendingDay90 = holder.shares * pendingRewardPerShareDay90;
    holder.percentage = totalMultiplierSum > 0 ? (holder.multiplierSum / totalMultiplierSum) * 100 : 0;
    holder.rank = 0;
    holder.displayMultiplierSum = holder.multiplierSum;
  });

  holders.sort((a, b) => b.shares - a.shares || b.multiplierSum - a.multiplierSum || b.total - a.total);
  holders.forEach((holder, index) => (holder.rank = index + 1));

  const result = {
    holders: sanitizeBigInt(holders),
    totalTokens,
    totalLockedAscendant,
    totalShares,
    toDistributeDay8,
    toDistributeDay28,
    toDistributeDay90,
    pendingRewards: toDistributeDay8 + toDistributeDay28 + toDistributeDay90,
    page,
    pageSize,
    totalPages: Math.ceil(totalTokens / pageSize),
  };

  cache[cacheKey] = { timestamp: now, data: result };
  return result;
}

// Fetch data for a specific wallet
async function getHolderData(wallet, requestId = '') {
  const contractAddress = contractAddresses.ascendantNFT;
  const tiers = contractTiers.ascendantNFT;
  const cacheKey = `${contractAddress}-${wallet}`;
  const now = Date.now();

  delete cache[cacheKey]; // Force fresh data

  if (!/^0x[a-fA-F0-9]{40}$/.test(wallet)) {
    throw new Error('Invalid wallet address');
  }

  const checksummedWallet = getAddress(wallet);
  log(`[${requestId}] getHolderData start: wallet=${checksummedWallet}, contract=${contractAddress}`);

  const retry = async (fn, attempts = 3, delay = 1000) => {
    for (let i = 0; i < attempts; i++) {
      try {
        return await fn();
      } catch (error) {
        if (i === attempts - 1) throw error;
        log(`[${requestId}] Retry ${i + 1}/${attempts} failed: ${error.message}`);
        await new Promise((res) => setTimeout(res, delay * 2 ** i));
      }
    }
  };

  const nfts = await retry(() =>
    alchemy.nft.getNftsForOwner(checksummedWallet, { contractAddresses: [contractAddress] })
  );
  log(`[${requestId}] ${contractAddress} - Initial NFTs for ${checksummedWallet}: ${nfts.totalCount}`);

  if (nfts.totalCount === 0) return null;

  const tokenIds = nfts.ownedNfts
    .filter((nft) => nft.contract.address.toLowerCase() === contractAddress.toLowerCase())
    .map((nft) => Number(nft.tokenId));
  log(`[${requestId}] ${contractAddress} - Token IDs for ${checksummedWallet}: [${tokenIds.join(', ')}]`);

  if (tokenIds.length === 0) return null;

  const tierCalls = tokenIds.map((tokenId) => ({
    address: contractAddress,
    abi: ascendantABI,
    functionName: 'getNFTAttribute',
    args: [BigInt(tokenId)],
  }));
  const recordCalls = tokenIds.map((tokenId) => ({
    address: contractAddress,
    abi: ascendantABI,
    functionName: 'userRecords',
    args: [BigInt(tokenId)],
  }));
  const claimableCall = [
    {
      address: contractAddress,
      abi: ascendantABI,
      functionName: 'batchClaimableAmount',
      args: [tokenIds.map((id) => BigInt(id))],
    },
  ];

  log(`[${requestId}] Tier calls: ${safeLog(tierCalls.map((c) => ({ tokenId: c.args[0], functionName: c.functionName })))}`);
  log(`[${requestId}] Record calls: ${safeLog(recordCalls.map((c) => ({ tokenId: c.args[0], functionName: c.functionName })))}`);
  log(`[${requestId}] Claimable call: ${safeLog(claimableCall)}`);

  const [rawTierResults, rawRecordResults, rawClaimableResults] = await Promise.all([
    retry(() => batchMulticall(tierCalls)),
    retry(() => batchMulticall(recordCalls)),
    retry(() => batchMulticall(claimableCall)),
  ]);

  log(`[${requestId}] Raw tierResults: ${safeLog(rawTierResults)}`);

  const tierResults = sanitizeBigInt(rawTierResults, 'tierResults');
  const recordResults = sanitizeBigInt(rawRecordResults, 'recordResults');
  const claimableResults = sanitizeBigInt(rawClaimableResults, 'claimableResults');

  let claimableRewards = 0;
  if (claimableResults[0]?.status === 'success') {
    const claimableRaw = claimableResults[0].result || '0';
    claimableRewards = parseFloat(formatUnits(claimableRaw, 18));
  }

  const maxTier = Math.max(...Object.keys(tiers).map(Number));
  const tiersArray = Array(maxTier + 1).fill(0);
  let total = 0;
  let multiplierSum = 0;
  let shares = 0;
  let lockedAscendant = 0;

  tokenIds.forEach((tokenId, i) => {
    const tierResult = tierResults[i];
    let tier;
    if (tierResult?.status === 'success') {
      if (Array.isArray(tierResult.result) && tierResult.result.length >= 2) {
        tier = Number(tierResult.result[1]);
      } else if (typeof tierResult.result === 'object' && tierResult.result.tier !== undefined) {
        tier = Number(tierResult.result.tier);
      }
    }
    if (tier >= 1 && tier <= maxTier) {
      tiersArray[tier] += 1;
      total += 1;
      multiplierSum += tiers[tier]?.multiplier || 0;
    } else {
      log(`[${requestId}] Invalid tier ${tier} for tokenId ${tokenId}: ${safeLog(tierResult)}`);
    }

    const recordResult = recordResults[i];
    if (recordResult?.status === 'success' && Array.isArray(recordResult.result)) {
      const sharesRaw = recordResult.result[0] || '0';
      const lockedAscendantRaw = recordResult.result[1] || '0';
      const tokenShares = parseFloat(formatUnits(sharesRaw, 18));
      const tokenLockedAscendant = parseFloat(formatUnits(lockedAscendantRaw, 18));
      shares += tokenShares;
      lockedAscendant += tokenLockedAscendant;
    } else {
      log(`[${requestId}] Invalid recordResult for tokenId ${tokenId}: ${safeLog(recordResult)}`);
    }
  });

  const totalSharesRaw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: ascendantABI,
      functionName: 'totalShares',
    })
  );
  const totalShares = parseFloat(formatUnits(totalSharesRaw.toString(), 18));

  const toDistributeDay8Raw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: ascendantABI,
      functionName: 'toDistribute',
      args: [0],
    })
  );
  const toDistributeDay8 = parseFloat(formatUnits(toDistributeDay8Raw.toString(), 18));

  const toDistributeDay28Raw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: ascendantABI,
      functionName: 'toDistribute',
      args: [1],
    })
  );
  const toDistributeDay28 = parseFloat(formatUnits(toDistributeDay28Raw.toString(), 18));

  const toDistributeDay90Raw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: ascendantABI,
      functionName: 'toDistribute',
      args: [2],
    })
  );
  const toDistributeDay90 = parseFloat(formatUnits(toDistributeDay90Raw.toString(), 18));

  const pendingRewardPerShareDay8 = totalShares > 0 ? toDistributeDay8 / totalShares : 0;
  const pendingRewardPerShareDay28 = totalShares > 0 ? toDistributeDay28 / totalShares : 0;
  const pendingRewardPerShareDay90 = totalShares > 0 ? toDistributeDay90 / totalShares : 0;

  const totalMultiplierSum = multiplierSum || 1;
  const percentage = (multiplierSum / totalMultiplierSum) * 100;
  const rank = 1;

  const result = {
    wallet: checksummedWallet,
    rank,
    total,
    multiplierSum,
    displayMultiplierSum: multiplierSum,
    percentage,
    tiers: tiersArray,
    shares,
    lockedAscendant,
    pendingDay8: shares * pendingRewardPerShareDay8,
    pendingDay28: shares * pendingRewardPerShareDay28,
    pendingDay90: shares * pendingRewardPerShareDay90,
    claimableRewards,
  };

  const sanitizedResult = sanitizeBigInt(result);
  cache[cacheKey] = { timestamp: now, data: sanitizedResult };
  return sanitizedResult;
}

// API endpoint handler
export async function GET(request) {
  const requestId = uuidv4();
  const { searchParams } = new URL(request.url);
  const wallet = searchParams.get('wallet');
  const page = parseInt(searchParams.get('page') || '0', 10);
  const pageSize = parseInt(searchParams.get('pageSize') || '1000', 10);
  log(`[${requestId}] Received request: page=${page}, pageSize=${pageSize}, wallet=${wallet}`);

  try {
    if (wallet) {
      const holderData = await getHolderData(wallet, requestId);
      const response = { holders: holderData ? [holderData] : [] };
      return NextResponse.json(safeSerialize(response));
    }

    const result = await getAllHolders(page, pageSize, requestId);
    return NextResponse.json(safeSerialize(result));
  } catch (error) {
    console.error(`[${requestId}] [PROD_ERROR] AscendantNFT API error: ${error.message}`);
    return NextResponse.json({ error: `Server error: ${error.message}` }, { status: 500 });
  }
}
----- app/api/holders/Ascendant/route.js -----

import { NextResponse } from 'next/server';
import { alchemy, client, CACHE_TTL, log, batchMulticall } from '../../utils';
import { contractAddresses, contractTiers } from '@/app/nft-contracts';
import { formatUnits, getAddress } from 'viem';
import { v4 as uuidv4 } from 'uuid';
import ascendantABI from '../../../../abi/ascendantNFT.json';

let cache = {};
let tokenCache = new Map();

// Utility to sanitize BigInt values
function sanitizeBigInt(obj) {
  if (typeof obj === 'bigint') {
    return obj.toString();
  }
  if (Array.isArray(obj)) {
    return obj.map((item) => sanitizeBigInt(item));
  }
  if (typeof obj === 'object' && obj !== null) {
    const sanitized = {};
    for (const [key, value] of Object.entries(obj)) {
      sanitized[key] = sanitizeBigInt(value);
    }
    return sanitized;
  }
  return obj;
}

// Utility to serialize response objects
function safeSerialize(obj) {
  return JSON.parse(JSON.stringify(obj, (key, value) =>
    typeof value === 'bigint' ? value.toString() : value
  ));
}

// Fetch data for all holders with pagination
async function getAllHolders(page = 0, pageSize = 1000, requestId = '') {
  const contractAddress = contractAddresses.ascendantNFT;
  const tiers = contractTiers.ascendantNFT;
  const cacheKey = `${contractAddress}-all-${page}-${pageSize}`;
  const now = Date.now();

  if (cache[cacheKey] && now - cache[cacheKey].timestamp < CACHE_TTL) {
    return cache[cacheKey].data;
  }

  if (!contractAddress || !tiers) {
    throw new Error('Missing contract address or tiers');
  }

  const retry = async (fn, attempts = 3, delay = 1000) => {
    for (let i = 0; i < attempts; i++) {
      try {
        return await fn();
      } catch (error) {
        if (i === attempts - 1) throw error;
        await new Promise((res) => setTimeout(res, delay * 2 ** i));
      }
    }
  };

  let owners = [];
  let pageKey = null;
  do {
    const response = await retry(() =>
      alchemy.nft.getOwnersForContract(contractAddress, {
        block: 'latest',
        withTokenBalances: true,
        pageKey,
      })
    );
    owners = owners.concat(response.owners);
    pageKey = response.pageKey;
  } while (pageKey);

  const burnAddress = '0x0000000000000000000000000000000000000000';
  const filteredOwners = owners.filter(
    (owner) => owner?.ownerAddress && owner.ownerAddress.toLowerCase() !== burnAddress && owner.tokenBalances?.length > 0
  );

  const tokenOwnerMap = new Map();
  let totalTokens = 0;
  filteredOwners.forEach((owner) => {
    if (!owner.ownerAddress) return;
    let wallet;
    try {
      wallet = getAddress(owner.ownerAddress);
    } catch (e) {
      return;
    }
    owner.tokenBalances.forEach((tb) => {
      if (!tb.tokenId) return;
      const tokenId = Number(tb.tokenId);
      tokenOwnerMap.set(tokenId, wallet);
      totalTokens++;
    });
  });

  const allTokenIds = Array.from(tokenOwnerMap.keys());
  const start = page * pageSize;
  const end = Math.min(start + pageSize, allTokenIds.length);
  const paginatedTokenIds = allTokenIds.slice(start, end);

  if (paginatedTokenIds.length === 0) {
    const result = {
      holders: [],
      totalTokens,
      totalLockedAscendant: 0,
      totalShares: 0,
      toDistributeDay8: 0,
      toDistributeDay28: 0,
      toDistributeDay90: 0,
      pendingRewards: 0,
      page,
      pageSize,
      totalPages: Math.ceil(totalTokens / pageSize),
    };
    cache[cacheKey] = { timestamp: now, data: result };
    return result;
  }

  const tierCalls = paginatedTokenIds.map((tokenId) => ({
    address: contractAddress,
    abi: ascendantABI,
    functionName: 'getNFTAttribute',
    args: [BigInt(tokenId)],
  }));
  const recordCalls = paginatedTokenIds.map((tokenId) => ({
    address: contractAddress,
    abi: ascendantABI,
    functionName: 'userRecords',
    args: [BigInt(tokenId)],
  }));

  const [rawTierResults, rawRecordResults] = await Promise.all([
    retry(() => batchMulticall(tierCalls)),
    retry(() => batchMulticall(recordCalls)),
  ]);

  const tierResults = sanitizeBigInt(rawTierResults);
  const recordResults = sanitizeBigInt(rawRecordResults);

  const totalSharesRaw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: ascendantABI,
      functionName: 'totalShares',
    })
  );
  const totalShares = parseFloat(formatUnits(totalSharesRaw.toString(), 18));
  const toDistributeDay8Raw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: ascendantABI,
      functionName: 'toDistribute',
      args: [0],
    })
  );
  const toDistributeDay8 = parseFloat(formatUnits(toDistributeDay8Raw.toString(), 18));
  const toDistributeDay28Raw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: ascendantABI,
      functionName: 'toDistribute',
      args: [1],
    })
  );
  const toDistributeDay28 = parseFloat(formatUnits(toDistributeDay28Raw.toString(), 18));
  const toDistributeDay90Raw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: ascendantABI,
      functionName: 'toDistribute',
      args: [2],
    })
  );
  const toDistributeDay90 = parseFloat(formatUnits(toDistributeDay90Raw.toString(), 18));

  const maxTier = Math.max(...Object.keys(tiers).map(Number));
  const holdersMap = new Map();
  let totalLockedAscendant = 0;

  const walletTokenIds = new Map();
  paginatedTokenIds.forEach((tokenId) => {
    const wallet = tokenOwnerMap.get(tokenId);
    if (!wallet) return;
    if (!walletTokenIds.has(wallet)) {
      walletTokenIds.set(wallet, []);
    }
    walletTokenIds.get(wallet).push(tokenId);
  });

  const claimableCalls = Array.from(walletTokenIds.entries()).map(([wallet, tokenIds]) => ({
    address: contractAddress,
    abi: ascendantABI,
    functionName: 'batchClaimableAmount',
    args: [tokenIds.map((id) => BigInt(id))],
  }));

  const rawClaimableResults = await retry(() => batchMulticall(claimableCalls));
  const claimableResults = sanitizeBigInt(rawClaimableResults);

  paginatedTokenIds.forEach((tokenId, i) => {
    const wallet = tokenOwnerMap.get(tokenId);
    if (!wallet) return;
    if (!holdersMap.has(wallet)) {
      holdersMap.set(wallet, {
        wallet,
        total: 0,
        multiplierSum: 0,
        tiers: Array(maxTier + 1).fill(0),
        shares: 0,
        lockedAscendant: 0,
        pendingDay8: 0,
        pendingDay28: 0,
        pendingDay90: 0,
        claimableRewards: 0,
      });
    }
    const holder = holdersMap.get(wallet);

    const tierResult = tierResults[i];
    let tier;
    if (tierResult?.status === 'success') {
      if (Array.isArray(tierResult.result) && tierResult.result.length >= 2) {
        tier = Number(tierResult.result[1]);
      } else if (typeof tierResult.result === 'object' && tierResult.result.tier !== undefined) {
        tier = Number(tierResult.result.tier);
      }
    }
    if (tier >= 1 && tier <= maxTier) {
      holder.tiers[tier] += 1;
      holder.total += 1;
      holder.multiplierSum += tiers[tier]?.multiplier || 0;
    }

    const recordResult = recordResults[i];
    if (recordResult?.status === 'success' && Array.isArray(recordResult.result)) {
      const sharesRaw = recordResult.result[0] || '0';
      const lockedAscendantRaw = recordResult.result[1] || '0';
      const shares = parseFloat(formatUnits(sharesRaw, 18));
      const lockedAscendant = parseFloat(formatUnits(lockedAscendantRaw, 18));
      holder.shares += shares;
      holder.lockedAscendant += lockedAscendant;
      totalLockedAscendant += lockedAscendant;
    }
  });

  let claimableIndex = 0;
  for (const [wallet, tokenIds] of walletTokenIds.entries()) {
    const holder = holdersMap.get(wallet);
    if (!holder) {
      claimableIndex++;
      continue;
    }
    if (claimableResults[claimableIndex]?.status === 'success') {
      const claimableRaw = claimableResults[claimableIndex].result || '0';
      holder.claimableRewards = parseFloat(formatUnits(claimableRaw, 18));
    }
    claimableIndex++;
  }

  const holders = Array.from(holdersMap.values());
  const totalMultiplierSum = holders.reduce((sum, h) => sum + h.multiplierSum, 0);
  const pendingRewardPerShareDay8 = totalShares > 0 ? toDistributeDay8 / totalShares : 0;
  const pendingRewardPerShareDay28 = totalShares > 0 ? toDistributeDay28 / totalShares : 0;
  const pendingRewardPerShareDay90 = totalShares > 0 ? toDistributeDay90 / totalShares : 0;

  holders.forEach((holder) => {
    holder.pendingDay8 = holder.shares * pendingRewardPerShareDay8;
    holder.pendingDay28 = holder.shares * pendingRewardPerShareDay28;
    holder.pendingDay90 = holder.shares * pendingRewardPerShareDay90;
    holder.percentage = totalMultiplierSum > 0 ? (holder.multiplierSum / totalMultiplierSum) * 100 : 0;
    holder.rank = 0;
    holder.displayMultiplierSum = holder.multiplierSum;
  });

  holders.sort((a, b) => b.shares - a.shares || b.multiplierSum - a.multiplierSum || b.total - a.total);
  holders.forEach((holder, index) => (holder.rank = index + 1));

  const result = {
    holders: sanitizeBigInt(holders),
    totalTokens,
    totalLockedAscendant,
    totalShares,
    toDistributeDay8,
    toDistributeDay28,
    toDistributeDay90,
    pendingRewards: toDistributeDay8 + toDistributeDay28 + toDistributeDay90,
    page,
    pageSize,
    totalPages: Math.ceil(totalTokens / pageSize),
  };

  cache[cacheKey] = { timestamp: now, data: result };
  return result;
}

// Fetch data for a specific wallet
async function getHolderData(wallet, requestId = '') {
  const contractAddress = contractAddresses.ascendantNFT;
  const tiers = contractTiers.ascendantNFT;
  const cacheKey = `${contractAddress}-${wallet}`;
  const now = Date.now();

  if (cache[cacheKey] && now - cache[cacheKey].timestamp < CACHE_TTL) {
    return cache[cacheKey].data;
  }

  if (!/^0x[a-fA-F0-9]{40}$/.test(wallet)) {
    throw new Error('Invalid wallet address');
  }

  const checksummedWallet = getAddress(wallet);

  const retry = async (fn, attempts = 3, delay = 1000) => {
    for (let i = 0; i < attempts; i++) {
      try {
        return await fn();
      } catch (error) {
        if (i === attempts - 1) throw error;
        await new Promise((res) => setTimeout(res, delay * 2 ** i));
      }
    }
  };

  const nfts = await retry(() =>
    alchemy.nft.getNftsForOwner(checksummedWallet, { contractAddresses: [contractAddress] })
  );

  if (nfts.totalCount === 0) return null;

  const tokenIds = nfts.ownedNfts
    .filter((nft) => nft.contract.address.toLowerCase() === contractAddress.toLowerCase())
    .map((nft) => Number(nft.tokenId));

  if (tokenIds.length === 0) return null;

  const tierCalls = tokenIds.map((tokenId) => ({
    address: contractAddress,
    abi: ascendantABI,
    functionName: 'getNFTAttribute',
    args: [BigInt(tokenId)],
  }));
  const recordCalls = tokenIds.map((tokenId) => ({
    address: contractAddress,
    abi: ascendantABI,
    functionName: 'userRecords',
    args: [BigInt(tokenId)],
  }));
  const claimableCall = [
    {
      address: contractAddress,
      abi: ascendantABI,
      functionName: 'batchClaimableAmount',
      args: [tokenIds.map((id) => BigInt(id))],
    },
  ];

  const [rawTierResults, rawRecordResults, rawClaimableResults] = await Promise.all([
    retry(() => batchMulticall(tierCalls)),
    retry(() => batchMulticall(recordCalls)),
    retry(() => batchMulticall(claimableCall)),
  ]);

  const tierResults = sanitizeBigInt(rawTierResults);
  const recordResults = sanitizeBigInt(rawRecordResults);
  const claimableResults = sanitizeBigInt(rawClaimableResults);

  let claimableRewards = 0;
  if (claimableResults[0]?.status === 'success') {
    const claimableRaw = claimableResults[0].result || '0';
    claimableRewards = parseFloat(formatUnits(claimableRaw, 18));
  }

  const maxTier = Math.max(...Object.keys(tiers).map(Number));
  const tiersArray = Array(maxTier + 1).fill(0);
  let total = 0;
  let multiplierSum = 0;
  let shares = 0;
  let lockedAscendant = 0;

  tokenIds.forEach((tokenId, i) => {
    const tierResult = tierResults[i];
    let tier;
    if (tierResult?.status === 'success') {
      if (Array.isArray(tierResult.result) && tierResult.result.length >= 2) {
        tier = Number(tierResult.result[1]);
      } else if (typeof tierResult.result === 'object' && tierResult.result.tier !== undefined) {
        tier = Number(tierResult.result.tier);
      }
    }
    if (tier >= 1 && tier <= maxTier) {
      tiersArray[tier] += 1;
      total += 1;
      multiplierSum += tiers[tier]?.multiplier || 0;
    }

    const recordResult = recordResults[i];
    if (recordResult?.status === 'success' && Array.isArray(recordResult.result)) {
      const sharesRaw = recordResult.result[0] || '0';
      const lockedAscendantRaw = recordResult.result[1] || '0';
      const tokenShares = parseFloat(formatUnits(sharesRaw, 18));
      const tokenLockedAscendant = parseFloat(formatUnits(lockedAscendantRaw, 18));
      shares += tokenShares;
      lockedAscendant += tokenLockedAscendant;
    }
  });

  const totalSharesRaw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: ascendantABI,
      functionName: 'totalShares',
    })
  );
  const totalShares = parseFloat(formatUnits(totalSharesRaw.toString(), 18));

  const toDistributeDay8Raw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: ascendantABI,
      functionName: 'toDistribute',
      args: [0],
    })
  );
  const toDistributeDay8 = parseFloat(formatUnits(toDistributeDay8Raw.toString(), 18));

  const toDistributeDay28Raw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: ascendantABI,
      functionName: 'toDistribute',
      args: [1],
    })
  );
  const toDistributeDay28 = parseFloat(formatUnits(toDistributeDay28Raw.toString(), 18));

  const toDistributeDay90Raw = await retry(() =>
    client.readContract({
      address: contractAddress,
      abi: ascendantABI,
      functionName: 'toDistribute',
      args: [2],
    })
  );
  const toDistributeDay90 = parseFloat(formatUnits(toDistributeDay90Raw.toString(), 18));

  const pendingRewardPerShareDay8 = totalShares > 0 ? toDistributeDay8 / totalShares : 0;
  const pendingRewardPerShareDay28 = totalShares > 0 ? toDistributeDay28 / totalShares : 0;
  const pendingRewardPerShareDay90 = totalShares > 0 ? toDistributeDay90 / totalShares : 0;

  const totalMultiplierSum = multiplierSum || 1;
  const percentage = (multiplierSum / totalMultiplierSum) * 100;
  const rank = 1;

  const result = {
    wallet: checksummedWallet,
    rank,
    total,
    multiplierSum,
    displayMultiplierSum: multiplierSum,
    percentage,
    tiers: tiersArray,
    shares,
    lockedAscendant,
    pendingDay8: shares * pendingRewardPerShareDay8,
    pendingDay28: shares * pendingRewardPerShareDay28,
    pendingDay90: shares * pendingRewardPerShareDay90,
    claimableRewards,
  };

  const sanitizedResult = sanitizeBigInt(result);
  cache[cacheKey] = { timestamp: now, data: sanitizedResult };
  return sanitizedResult;
}

// API endpoint handler
export async function GET(request) {
  const requestId = uuidv4();
  const { searchParams } = new URL(request.url);
  const wallet = searchParams.get('wallet');
  const page = parseInt(searchParams.get('page') || '0', 10);
  const pageSize = parseInt(searchParams.get('pageSize') || '1000', 10);

  try {
    if (wallet) {
      const holderData = await getHolderData(wallet, requestId);
      const response = { holders: holderData ? [holderData] : [] };
      return NextResponse.json(safeSerialize(response));
    }

    const result = await getAllHolders(page, pageSize, requestId);
    return NextResponse.json(safeSerialize(result));
  } catch (error) {
    console.error(`[${requestId}] [PROD_ERROR] AscendantNFT API error: ${error.message}`);
    return NextResponse.json({ error: `Server error: ${error.message}` }, { status: 500 });
  }
}
----- app/api/holders/E280/route.js -----

// app/api/holders/E280/route.js
import { NextResponse } from 'next/server';
import { log } from '../../utils';

export async function GET(request) {
  log('GET /api/holders/E280: Data not available yet');
  return NextResponse.json({ message: 'E280 data will go live after deployment' });
}
----- app/api/holders/Element280/route.js -----

// app/api/holders/Element280/route.js
import { NextResponse } from 'next/server';
import { alchemy, client, nftAbi, element280VaultAbi, CACHE_TTL, log, batchMulticall } from '../../utils';
import { contractAddresses, contractTiers, vaultAddresses } from '@/app/nft-contracts';

let cache = {};
let tokenCache = new Map();

async function getAllHolders(contractAddress, tiers, page = 0, pageSize = 1000) {
  const contractName = 'element280';
  const cacheKey = `${contractAddress}-all-${page}-${pageSize}`;
  const now = Date.now();

  if (cache[cacheKey] && (now - cache[cacheKey].timestamp) < CACHE_TTL) {
    log(`getAllHolders: Returning cached data for ${cacheKey}`);
    return cache[cacheKey].data;
  }

  log(`getAllHolders start: ${contractName} at ${contractAddress}, page=${page}, pageSize=${pageSize}`);
  const ownersResponse = await alchemy.nft.getOwnersForContract(contractAddress, { withTokenBalances: true });
  log(`${contractName} - Raw owners count: ${ownersResponse.owners.length}`);

  const burnAddress = '0x0000000000000000000000000000000000000000';
  const filteredOwners = ownersResponse.owners.filter(
    owner => owner.ownerAddress.toLowerCase() !== burnAddress && owner.tokenBalances.length > 0
  );
  log(`${contractName} - Filtered live owners count: ${filteredOwners.length}`);

  const tokenOwnerMap = new Map();
  let totalTokens = 0;
  filteredOwners.forEach(owner => {
    const wallet = owner.ownerAddress.toLowerCase();
    owner.tokenBalances.forEach(tb => {
      const tokenId = BigInt(tb.tokenId);
      tokenOwnerMap.set(tokenId, wallet);
      totalTokens++;
    });
  });
  log(`${contractName} - Total tokens checked: ${totalTokens}`);

  const allTokenIds = Array.from(tokenOwnerMap.keys());
  const start = page * pageSize;
  const end = Math.min(start + pageSize, allTokenIds.length);
  const paginatedTokenIds = allTokenIds.slice(start, end);
  log(`${contractName} - Paginated token IDs: ${paginatedTokenIds.length} (start=${start}, end=${end})`);

  const ownerOfCalls = paginatedTokenIds.map(tokenId => ({
    address: contractAddress,
    abi: nftAbi,
    functionName: 'ownerOf',
    args: [tokenId],
  }));

  const ownerOfResults = await batchMulticall(ownerOfCalls);
  const validTokenIds = [];
  paginatedTokenIds.forEach((tokenId, i) => {
    const owner = ownerOfResults[i]?.status === 'success' && ownerOfResults[i].result.toLowerCase();
    const cacheKey = `${contractAddress}-${tokenId}-owner`;
    if (owner && owner !== burnAddress) {
      validTokenIds.push(tokenId);
      tokenCache.set(cacheKey, owner);
    } else {
      tokenCache.set(cacheKey, null);
    }
  });
  log(`${contractName} - Valid token IDs after ownerOf: ${validTokenIds.length}`);

  if (validTokenIds.length === 0) {
    log(`${contractName} - No valid tokens found in this page`);
    return { holders: [], totalTokens, page, pageSize, totalPages: Math.ceil(allTokenIds.length / pageSize) };
  }

  const tierCalls = validTokenIds.map(tokenId => ({
    address: contractAddress,
    abi: nftAbi,
    functionName: 'getNftTier',
    args: [tokenId],
  }));

  log(`${contractName} - Starting tier multicall for ${tierCalls.length} tokens`);
  const tierResults = await batchMulticall(tierCalls);
  log(`${contractName} - Tier results length: ${tierResults.length}`);
  const maxTier = Math.max(...Object.keys(tiers).map(Number));
  const holdersMap = new Map();
  let totalNftsHeld = 0;

  tierResults.forEach((result, i) => {
    if (!result) {
      log(`${contractName} - Undefined tier result at index ${i}, tokenId: ${validTokenIds[i]}`);
      return;
    }
    if (result.status === 'success') {
      const tokenId = validTokenIds[i];
      const wallet = tokenOwnerMap.get(tokenId);
      const tier = Number(result.result);
      const cacheKey = `${contractAddress}-${tokenId}-tier`;
      tokenCache.set(cacheKey, tier);

      if (tier >= 1 && tier <= maxTier) {
        if (!holdersMap.has(wallet)) {
          holdersMap.set(wallet, {
            wallet,
            total: 0,
            multiplierSum: 0,
            tiers: Array(maxTier + 1).fill(0),
            claimableRewards: 0,
          });
        }

        const holder = holdersMap.get(wallet);
        holder.total += 1;
        holder.multiplierSum += tiers[tier]?.multiplier || 0;
        holder.tiers[tier] += 1;
        totalNftsHeld += 1;
      } else {
        log(`${contractName} - Invalid tier ${tier} for token ${tokenId}`);
      }
    } else {
      log(`${contractName} - Failed tier fetch at index ${i}, tokenId: ${validTokenIds[i]}`);
    }
  });
  log(`${contractName} - Total NFTs held after tier check: ${totalNftsHeld}`);

  // Fetch claimable rewards from vault
  const holders = Array.from(holdersMap.values());
  const rewardCalls = holders.map(holder => ({
    address: vaultAddresses.element280,
    abi: element280VaultAbi,
    functionName: 'claimableReward',
    args: [holder.wallet],
  }));

  const rewardResults = await batchMulticall(rewardCalls);
  holders.forEach((holder, i) => {
    if (rewardResults[i]?.status === 'success') {
      holder.claimableRewards = Number(rewardResults[i].result);
    } else {
      holder.claimableRewards = 0;
      log(`${contractName} - Failed to fetch rewards for ${holder.wallet}`);
    }
  });

  const totalMultiplierSum = holders.reduce((sum, h) => sum + h.multiplierSum, 0);
  holders.forEach(holder => {
    holder.percentage = totalMultiplierSum > 0 ? (holder.multiplierSum / totalMultiplierSum) * 100 : 0;
    holder.rank = 0;
    holder.displayMultiplierSum = holder.multiplierSum / 10;
  });

  const sortFn = (a, b) => b.multiplierSum - a.multiplierSum || b.total - a.total;
  holders.sort(sortFn);
  holders.forEach((holder, index) => (holder.rank = index + 1));

  const result = {
    holders,
    totalTokens,
    page,
    pageSize,
    totalPages: Math.ceil(allTokenIds.length / pageSize),
  };

  cache[cacheKey] = { timestamp: now, data: result };
  log(`${contractName} - Final holders count: ${holders.length}`);
  return result;
}

async function getHolderData(contractAddress, wallet, tiers) {
  const contractName = 'element280';
  const cacheKey = `${contractAddress}-${wallet}`;
  const now = Date.now();

  if (cache[cacheKey] && (now - cache[cacheKey].timestamp) < CACHE_TTL) {
    log(`getHolderData: Returning cached data for ${cacheKey}`);
    return cache[cacheKey].data;
  }

  if (!/^0x[a-fA-F0-9]{40}$/.test(wallet)) {
    throw new Error('Invalid wallet address');
  }

  log(`getHolderData start: wallet=${wallet}, contract=${contractAddress}`);
  const nfts = await alchemy.nft.getNftsForOwner(wallet, { contractAddresses: [contractAddress] });
  log(`${contractAddress} - Initial NFTs for ${wallet}: ${nfts.totalCount}`);

  if (nfts.totalCount === 0) return null;

  const walletLower = wallet.toLowerCase();
  const tokenIds = nfts.ownedNfts.map(nft => BigInt(nft.tokenId));
  const ownerOfCalls = tokenIds.map(tokenId => ({
    address: contractAddress,
    abi: nftAbi,
    functionName: 'ownerOf',
    args: [tokenId],
  }));

  const ownerOfResults = await batchMulticall(ownerOfCalls);
  const validTokenIds = tokenIds.filter((tokenId, i) => {
    const owner = ownerOfResults[i]?.status === 'success' && ownerOfResults[i].result.toLowerCase();
    const cacheKey = `${contractAddress}-${tokenId}-owner`;
    tokenCache.set(cacheKey, owner);
    return owner === walletLower;
  });
  log(`${contractAddress} - Valid token IDs for ${wallet}: ${validTokenIds.length}`);

  if (validTokenIds.length === 0) return null;

  const tierCalls = validTokenIds.map(tokenId => ({
    address: contractAddress,
    abi: nftAbi,
    functionName: 'getNftTier',
    args: [tokenId],
  }));

  const tierResults = await batchMulticall(tierCalls);
  const maxTier = Math.max(...Object.keys(tiers).map(Number));
  const tiersArray = Array(maxTier + 1).fill(0);
  let total = 0;
  let multiplierSum = 0;

  tierResults.forEach((result, i) => {
    if (!result) {
      log(`${contractAddress} - Undefined tier result for wallet ${wallet} at index ${i}, tokenId: ${validTokenIds[i]}`);
      return;
    }
    if (result.status === 'success') {
      const tier = Number(result.result);
      const tokenId = validTokenIds[i];
      const cacheKey = `${contractAddress}-${tokenId}-tier`;
      tokenCache.set(cacheKey, tier);
      if (tier >= 1 && tier <= maxTier) {
        tiersArray[tier] += 1;
        total += 1;
        multiplierSum += tiers[tier]?.multiplier || 0;
      }
    }
  });
  log(`${contractAddress} - Total NFTs for ${wallet} after tier check: ${total}`);

  const rewardResult = await client.readContract({
    address: vaultAddresses.element280,
    abi: element280VaultAbi,
    functionName: 'claimableReward',
    args: [walletLower],
  });
  const claimableRewards = Number(rewardResult) || 0;

  const allHolders = await getAllHolders(contractAddress, tiers, 0, 1000);
  const totalMultiplierSum = allHolders.holders.reduce((sum, h) => sum + h.multiplierSum, 0);
  const percentage = totalMultiplierSum > 0 ? (multiplierSum / totalMultiplierSum) * 100 : 0;
  const holder = allHolders.holders.find(h => h.wallet === walletLower) || { rank: allHolders.holders.length + 1 };

  const result = {
    wallet: walletLower,
    rank: holder.rank,
    total,
    multiplierSum,
    displayMultiplierSum: multiplierSum / 10,
    percentage,
    tiers: tiersArray,
    claimableRewards,
  };

  cache[cacheKey] = { timestamp: now, data: result };
  log(`${contractAddress} - Final data for ${wallet}: total=${total}, multiplierSum=${multiplierSum}, claimableRewards=${claimableRewards}`);
  return result;
}

export async function GET(request) {
  const { searchParams } = new URL(request.url);
  const wallet = searchParams.get('wallet');
  const page = Math.max(0, parseInt(searchParams.get('page') || '0', 10));
  const pageSize = Math.max(1, Math.min(1000, parseInt(searchParams.get('pageSize') || '1000', 10)));

  const address = contractAddresses['element280'];
  if (!address) {
    return NextResponse.json({ error: 'Element280 contract address not found' }, { status: 400 });
  }

  try {
    if (wallet) {
      const holderData = await getHolderData(address, wallet, contractTiers['element280']);
      return NextResponse.json({ holders: holderData ? [holderData] : [] });
    }

    const result = await getAllHolders(address, contractTiers['element280'], page, pageSize);
    return NextResponse.json(result);
  } catch (error) {
    log(`Error in GET /api/holders/Element280: ${error.message}`);
    return NextResponse.json({ error: `Server error: ${error.message}` }, { status: 500 });
  }
}
----- app/api/holders/Element280/validate-burned/route.js -----

// app/api/holders/Element280/validate-burned/route.js
import { NextResponse } from 'next/server';
import config from '@/contracts/config';
import { getTransactionReceipt, getCache, setCache } from '@/app/api/utils/cache';
import { log } from '@/app/api/utils/logging';
import { client } from '@/app/api/utils/client';
import { parseAbiItem } from 'viem';

export async function POST(request) {
  if (process.env.DEBUG === 'true') {
    log(`[Element280-Validate-Burned] [DEBUG] Processing POST request for validate-burned`);
  }

  try {
    const { transactionHash } = await request.json();
    if (!transactionHash || typeof transactionHash !== 'string' || !transactionHash.match(/^0x[a-fA-F0-9]{64}$/)) {
      log(`[Element280-Validate-Burned] [VALIDATION] Invalid transaction hash: ${transactionHash || 'undefined'}`);
      return NextResponse.json({ error: 'Invalid transaction hash' }, { status: 400 });
    }

    const contractAddress = config.contractAddresses?.element280?.address;
    if (!contractAddress) {
      log(`[Element280-Validate-Burned] [VALIDATION] Element280 contract address not configured in config.js`);
      return NextResponse.json({ error: 'Contract address not configured' }, { status: 500 });
    }

    const cacheKey = `element280_burn_validation_${transactionHash}`;
    const cachedResult = await getCache(cacheKey, 'element280');
    if (cachedResult) {
      if (process.env.DEBUG === 'true') {
        log(`[Element280-Validate-Burned] [DEBUG] Cache hit for burn validation: ${transactionHash}`);
      }
      return NextResponse.json(cachedResult);
    }

    if (process.env.DEBUG === 'true') {
      log(`[Element280-Validate-Burned] [DEBUG] Fetching transaction receipt for hash: ${transactionHash}`);
    }
    const receipt = await getTransactionReceipt(transactionHash);
    if (!receipt) {
      log(`[Element280-Validate-Burned] [VALIDATION] Transaction receipt not found for hash: ${transactionHash}`);
      return NextResponse.json({ error: 'Transaction not found' }, { status: 404 });
    }

    const burnAddress = '0x0000000000000000000000000000000000000000';
    const transferEvent = parseAbiItem('event Transfer(address indexed from, address indexed to, uint256 indexed tokenId)');
    const burnedTokenIds = [];

    for (const logEntry of receipt.logs) {
      if (
        logEntry.address.toLowerCase() === contractAddress.toLowerCase() &&
        logEntry.topics[0] === transferEvent.topics[0]
      ) {
        try {
          const decodedLog = client.decodeEventLog({
            abi: [transferEvent],
            data: logEntry.data,
            topics: logEntry.topics,
          });
          if (decodedLog.args.to.toLowerCase() === burnAddress) {
            burnedTokenIds.push(decodedLog.args.tokenId.toString());
          }
        } catch (_decodeError) {
          log(`[Element280-Validate-Burned] [ERROR] Failed to decode log entry for transaction ${transactionHash}: ${_decodeError.message}`);
        }
      }
    }

    if (burnedTokenIds.length === 0) {
      log(`[Element280-Validate-Burned] [VALIDATION] No burn events found in transaction: ${transactionHash}`);
      return NextResponse.json({ error: 'No burn events found in transaction' }, { status: 400 });
    }

    const result = {
      transactionHash,
      burnedTokenIds,
      blockNumber: receipt.blockNumber.toString(),
    };

    await setCache(cacheKey, result, config.cache.nodeCache.stdTTL, 'element280');
    if (process.env.DEBUG === 'true') {
      log(`[Element280-Validate-Burned] [DEBUG] Found ${burnedTokenIds.length} burned tokens in transaction: ${transactionHash}`);
    }
    return NextResponse.json(result);
  } catch (error) {
    log(`[Element280-Validate-Burned] [ERROR] Error processing transaction: ${error.message}, stack: ${error.stack}`);
    return NextResponse.json({ error: 'Failed to validate transaction', details: error.message }, { status: 500 });
  }
}
----- app/api/holders/Element369/route.js -----

// app/api/holders/Element369/route.js
import { NextResponse } from 'next/server';
import { contractDetails, nftContracts } from '../../../nft-contracts';
import { client, alchemy, cache, CACHE_TTL, log, batchMulticall, element369VaultAbi, element369Abi } from '../../utils';

const contractAddress = nftContracts.element369.address;
const vaultAddress = nftContracts.element369.vaultAddress;
const tiersConfig = nftContracts.element369.tiers;

export async function GET(request) {
  const { searchParams } = new URL(request.url);
  const page = parseInt(searchParams.get('page') || '0');
  const pageSize = parseInt(searchParams.get('pageSize') || '1000');
  const wallet = searchParams.get('wallet');

  log(`[Element369] Request: page=${page}, pageSize=${pageSize}, wallet=${wallet}`);

  try {
    if (!contractAddress || !vaultAddress) {
      throw new Error('Element369 contract or vault address missing');
    }

    const cacheKey = `element369_holders_${page}_${pageSize}_${wallet || 'all'}`;
    if (cache[cacheKey] && Date.now() - cache[cacheKey].timestamp < CACHE_TTL) {
      log(`[Element369] Cache hit: ${cacheKey}`);
      return NextResponse.json(cache[cacheKey].data);
    }
    log(`[Element369] Cache miss: ${cacheKey}`);

    // Fetch owners
    const ownersResponse = await alchemy.nft.getOwnersForContract(contractAddress, {
      block: 'latest',
      withTokenBalances: true,
    });
    log(`[Element369] Owners fetched: ${ownersResponse.owners.length}`);

    const burnAddress = '0x0000000000000000000000000000000000000000';
    const filteredOwners = ownersResponse.owners.filter(
      owner => owner.ownerAddress.toLowerCase() !== burnAddress && owner.tokenBalances.length > 0
    );
    log(`[Element369] Live owners: ${filteredOwners.length}`);

    // Build token-to-owner map
    const tokenOwnerMap = new Map();
    const ownerTokens = new Map();
    let totalTokens = 0;
    filteredOwners.forEach(owner => {
      const wallet = owner.ownerAddress.toLowerCase();
      const tokenIds = owner.tokenBalances.map(tb => BigInt(tb.tokenId));
      tokenIds.forEach(tokenId => {
        tokenOwnerMap.set(tokenId, wallet);
        totalTokens++;
      });
      ownerTokens.set(wallet, tokenIds);
    });
    log(`[Element369] Total tokens: ${totalTokens}`);

    // Paginate
    const allTokenIds = Array.from(tokenOwnerMap.keys());
    const start = page * pageSize;
    const end = Math.min(start + pageSize, allTokenIds.length);
    const paginatedTokenIds = allTokenIds.slice(start, end);
    log(`[Element369] Paginated tokens: ${paginatedTokenIds.length}`);

    // Fetch tiers
    const tierCalls = paginatedTokenIds.map(tokenId => ({
      address: contractAddress,
      abi: element369Abi,
      functionName: 'getNftTier',
      args: [tokenId],
    }));
    const tierResults = await batchMulticall(tierCalls);
    log(`[Element369] Tiers fetched for ${tierResults.length} tokens`);

    // Build holders
    const maxTier = Math.max(...Object.keys(tiersConfig).map(Number));
    const holdersMap = new Map();

    tierResults.forEach((result, i) => {
      if (result?.status === 'success') {
        const tokenId = paginatedTokenIds[i];
        const wallet = tokenOwnerMap.get(tokenId);
        const tier = Number(result.result);

        if (tier >= 1 && tier <= maxTier && wallet) {
          if (!holdersMap.has(wallet)) {
            holdersMap.set(wallet, {
              wallet,
              total: 0,
              multiplierSum: 0,
              tiers: Array(maxTier + 1).fill(0),
              infernoRewards: 0,
              fluxRewards: 0,
              e280Rewards: 0,
            });
          }
          const holder = holdersMap.get(wallet);
          holder.total += 1;
          holder.multiplierSum += tiersConfig[tier]?.multiplier || 0;
          holder.tiers[tier] += 1;
        } else {
          log(`[Element369] Invalid tier ${tier} for token ${tokenId}`);
        }
      } else {
        log(`[Element369] Tier fetch failed for token ${paginatedTokenIds[i]}: ${result?.error || 'Unknown'}`);
      }
    });

    // Fetch current cycle for debugging
    let currentCycle = 0;
    try {
      currentCycle = await client.readContract({
        address: vaultAddress,
        abi: element369VaultAbi,
        functionName: 'getCurrentE369Cycle',
      });
      log(`[Element369] Current cycle: ${currentCycle}`);
    } catch (error) {
      log(`[Element369] Error fetching cycle: ${error.message}`);
    }

    // Fetch rewards
    const holders = Array.from(holdersMap.values());
    const rewardCalls = holders.map(holder => {
      const tokenIds = ownerTokens.get(holder.wallet) || [];
      return {
        address: vaultAddress,
        abi: element369VaultAbi,
        functionName: 'getRewards',
        args: [tokenIds, holder.wallet, false], // isBacking: false for claimable rewards
      };
    });

    log(`[Element369] Fetching rewards for ${holders.length} holders`);
    const rewardsResults = await batchMulticall(rewardCalls);

    holders.forEach((holder, i) => {
      if (rewardsResults[i]?.status === 'success' && rewardsResults[i].result) {
        const [availability, burned, infernoPool, fluxPool, e280Pool] = rewardsResults[i].result;
        holder.infernoRewards = Number(infernoPool) / 1e18;
        holder.fluxRewards = Number(fluxPool) / 1e18;
        holder.e280Rewards = Number(e280Pool) / 1e18;
        log(
          `[Element369] Rewards for ${holder.wallet.slice(0, 6)}...: ` +
          `Inferno=${holder.infernoRewards.toFixed(4)}, ` +
          `Flux=${holder.fluxRewards.toFixed(4)}, ` +
          `E280=${holder.e280Rewards.toFixed(4)}, ` +
          `Tokens=${availability.length}, Burned=${burned.filter(b => b).length}, ` +
          `Availability=${availability.join(',')}`
        );
        if (holder.infernoRewards === 0 && holder.fluxRewards === 0 && holder.e280Rewards === 0) {
          log(`[Element369] Zero rewards for ${holder.wallet}: Tokens=${ownerTokens.get(holder.wallet).join(',')}`);
        }
      } else {
        holder.infernoRewards = 0;
        holder.fluxRewards = 0;
        holder.e280Rewards = 0;
        log(`[Element369] Reward fetch failed for ${holder.wallet.slice(0, 6)}...: ${rewardsResults[i]?.error || 'Unknown'}`);
      }
      holder.displayMultiplierSum = holder.multiplierSum;
      holder.percentage = 0;
      holder.rank = 0;
    });

    // Calculate percentages and ranks
    const totalMultiplierSum = holders.reduce((sum, h) => sum + h.multiplierSum, 0);
    holders.forEach((holder, index) => {
      holder.percentage = totalMultiplierSum > 0 ? (holder.multiplierSum / totalMultiplierSum) * 100 : 0;
      holder.rank = index + 1;
      holder.displayMultiplierSum = holder.multiplierSum;
    });

    // Sort holders
    holders.sort((a, b) => b.multiplierSum - a.multiplierSum || b.total - a.total);

    const response = {
      holders,
      totalTokens,
      page,
      pageSize,
      totalPages: Math.ceil(totalTokens / pageSize),
    };
    cache[cacheKey] = { data: response, timestamp: Date.now() };
    log(`[Element369] Success: ${holders.length} holders`);

    return NextResponse.json(response);
  } catch (error) {
    log(`[Element369] Error: ${error.message}`);
    console.error('[Element369] Error stack:', error.stack);
    return NextResponse.json({ error: 'Failed to fetch Element369 data' }, { status: 500 });
  }
}
----- app/api/holders/Stax/route.js -----

// app/api/holders/Stax/route.js
import { NextResponse } from 'next/server';
import { contractDetails, nftContracts } from '../../../nft-contracts';
import { client, alchemy, cache, CACHE_TTL, log, batchMulticall, staxNFTAbi, staxVaultAbi } from '../../utils';

const contractAddress = nftContracts.staxNFT.address;
const vaultAddress = nftContracts.staxNFT.vaultAddress;
const tiersConfig = nftContracts.staxNFT.tiers;

export async function GET(request) {
  const { searchParams } = new URL(request.url);
  const page = parseInt(searchParams.get('page') || '0');
  const pageSize = parseInt(searchParams.get('pageSize') || '1000');
  const wallet = searchParams.get('wallet');

  log(`[Stax] Request: page=${page}, pageSize=${pageSize}, wallet=${wallet}`);

  try {
    if (!contractAddress || !vaultAddress) {
      throw new Error('Stax contract or vault address missing');
    }

    const cacheKey = `stax_holders_${page}_${pageSize}_${wallet || 'all'}`;
    if (cache[cacheKey] && Date.now() - cache[cacheKey].timestamp < CACHE_TTL) {
      log(`[Stax] Cache hit: ${cacheKey}`);
      return NextResponse.json(cache[cacheKey].data);
    }
    log(`[Stax] Cache miss: ${cacheKey}`);

    // Fetch owners
    const ownersResponse = await alchemy.nft.getOwnersForContract(contractAddress, {
      block: 'latest',
      withTokenBalances: true,
    });
    log(`[Stax] Owners fetched: ${ownersResponse.owners.length}`);

    const burnAddresses = [
      '0x0000000000000000000000000000000000000000',
      '0x000000000000000000000000000000000000dead',
    ];
    const filteredOwners = ownersResponse.owners.filter(
      owner => !burnAddresses.includes(owner.ownerAddress.toLowerCase()) && owner.tokenBalances.length > 0
    );
    log(`[Stax] Live owners: ${filteredOwners.length}`);

    // Build token-to-owner map
    const tokenOwnerMap = new Map();
    const ownerTokens = new Map();
    let totalTokens = 0;
    filteredOwners.forEach(owner => {
      const wallet = owner.ownerAddress.toLowerCase();
      const tokenIds = owner.tokenBalances.map(tb => BigInt(tb.tokenId));
      tokenIds.forEach(tokenId => {
        tokenOwnerMap.set(tokenId, wallet);
        totalTokens++;
      });
      ownerTokens.set(wallet, tokenIds);
    });
    log(`[Stax] Total tokens: ${totalTokens}`);

    // Paginate
    const allTokenIds = Array.from(tokenOwnerMap.keys());
    const start = page * pageSize;
    const end = Math.min(start + pageSize, allTokenIds.length);
    const paginatedTokenIds = allTokenIds.slice(start, end);
    log(`[Stax] Paginated tokens: ${paginatedTokenIds.length}`);

    // Fetch tiers
    const tierCalls = paginatedTokenIds.map(tokenId => ({
      address: contractAddress,
      abi: staxNFTAbi,
      functionName: 'getNftTier',
      args: [tokenId],
    }));
    const tierResults = await batchMulticall(tierCalls);
    log(`[Stax] Tiers fetched for ${tierResults.length} tokens`);

    // Build holders
    const maxTier = Math.max(...Object.keys(tiersConfig).map(Number));
    const holdersMap = new Map();

    tierResults.forEach((result, i) => {
      if (result?.status === 'success') {
        const tokenId = paginatedTokenIds[i];
        const wallet = tokenOwnerMap.get(tokenId);
        const tier = Number(result.result);

        if (tier >= 1 && tier <= maxTier && wallet) {
          if (!holdersMap.has(wallet)) {
            holdersMap.set(wallet, {
              wallet,
              total: 0,
              multiplierSum: 0,
              tiers: Array(maxTier + 1).fill(0),
              claimableRewards: 0,
            });
          }
          const holder = holdersMap.get(wallet);
          holder.total += 1;
          holder.multiplierSum += tiersConfig[tier]?.multiplier || 0;
          holder.tiers[tier] += 1;
        } else {
          log(`[Stax] Invalid tier ${tier} for token ${tokenId}`);
        }
      } else {
        log(`[Stax] Tier fetch failed for token ${paginatedTokenIds[i]}: ${result?.error || 'Unknown'}`);
      }
    });

    // Fetch rewards
    const holders = Array.from(holdersMap.values());
    const rewardCalls = holders.map(holder => {
      const tokenIds = ownerTokens.get(holder.wallet) || [];
      return {
        address: vaultAddress,
        abi: staxVaultAbi,
        functionName: 'getRewards',
        args: [tokenIds, holder.wallet],
      };
    });

    const totalRewardPoolCall = {
      address: vaultAddress,
      abi: staxVaultAbi,
      functionName: 'totalRewardPool',
      args: [],
    };

    log(`[Stax] Fetching rewards for ${holders.length} holders`);
    const [rewardResults, totalRewardPoolResult] = await Promise.all([
      rewardCalls.length ? batchMulticall(rewardCalls) : [],
      batchMulticall([totalRewardPoolCall]),
    ]);

    const totalRewardPool = totalRewardPoolResult[0]?.status === 'success'
      ? Number(totalRewardPoolResult[0].result) / 1e18
      : 0;

    holders.forEach((holder, i) => {
      if (rewardResults[i]?.status === 'success' && rewardResults[i].result) {
        const [, totalPayout] = rewardResults[i].result;
        holder.claimableRewards = Number(totalPayout) / 1e18;
        log(
          `[Stax] Rewards for ${holder.wallet.slice(0, 6)}...: ` +
          `Claimable=${holder.claimableRewards.toFixed(4)}, ` +
          `Tokens=${ownerTokens.get(holder.wallet).length}`
        );
        if (holder.claimableRewards === 0) {
          log(`[Stax] Zero rewards for ${holder.wallet}: Tokens=${ownerTokens.get(holder.wallet).join(',')}`);
        }
      } else {
        holder.claimableRewards = 0;
        log(`[Stax] Reward fetch failed for ${holder.wallet.slice(0, 6)}...: ${rewardResults[i]?.error || 'Unknown'}`);
      }
      holder.percentage = totalRewardPool ? (holder.claimableRewards / totalRewardPool) * 100 : 0;
      holder.rank = 0;
    });

    // Calculate ranks
    holders.sort((a, b) => b.multiplierSum - a.multiplierSum || b.total - a.total);
    holders.forEach((holder, index) => {
      holder.rank = index + 1;
    });

    const response = {
      holders,
      totalTokens,
      page,
      pageSize,
      totalPages: Math.ceil(totalTokens / pageSize),
    };
    cache[cacheKey] = { data: response, timestamp: Date.now() };
    log(`[Stax] Success: ${holders.length} holders`);

    return NextResponse.json(response);
  } catch (error) {
    log(`[Stax] Error: ${error.message}`);
    console.error('[Stax] Error stack:', error.stack);
    let status = 500;
    let message = 'Failed to fetch Stax data';
    if (error.message.includes('Rate limit')) {
      status = 429;
      message = 'Alchemy rate limit exceeded';
    }
    return NextResponse.json({ error: message, details: error.message }, { status });
  }
}
----- app/api/holders/[contract]/progress/route.js -----

import { NextResponse } from 'next/server';
import { logger } from '@/app/lib/logger';
import { ProgressResponseSchema } from '@/app/lib/schemas';
import { getCacheState } from '@/app/api/holders/cache/state';
import config from '@/contracts/config';
import { sanitizeBigInt } from '@/app/api/holders/cache/holders'; // Added for BigInt handling

export async function GET(_request, { params }) {
  const { contract } = await params;
  const contractKey = contract.toLowerCase();
  // Retrieve chain dynamically from config.nftContracts (from file1)
  const chain = config.nftContracts[contractKey]?.chain || 'eth';

  // Validate contract key
  if (!config.contractDetails[contractKey]) {
    logger.error('progress', `Invalid contract: ${contractKey}`, chain, contractKey);
    return NextResponse.json({ error: `Invalid contract: ${contractKey}` }, { status: 400 });
  }

  // Check if contract is disabled
  if (config.contractDetails[contractKey].disabled) {
    return NextResponse.json({ error: `${contractKey} contract not deployed` }, { status: 400 });
  }

  try {
    // Fetch cache state
    const state = await getCacheState(contractKey);
    // Fallback response for missing cache state (from file1)
    if (!state || !state.progressState) {
      logger.warn('progress', `No cache state found for ${contractKey}`, chain, contractKey);
      return NextResponse.json(
        sanitizeBigInt({
          isPopulating: false,
          totalLiveHolders: 0,
          totalOwners: 0,
          phase: 'Idle',
          progressPercentage: '0.0',
          lastProcessedBlock: null,
          lastUpdated: null,
          error: null,
          errorLog: [],
          globalMetrics: {},
          isErrorLogTruncated: false,
          status: 'idle', // Added status field (from file1)
        }),
        { status: 200 }
      );
    }

    // Calculate progress percentage (retained from existing code)
    let progressPercentage = '0.0';
    if (state.progressState.error) {
      progressPercentage = '0.0';
    } else if (state.progressState.step === 'completed') {
      progressPercentage = '100.0';
    } else if (state.progressState.totalNfts > 0) {
      if (state.progressState.step === 'fetching_owners') {
        const ownerProgress = (state.progressState.processedNfts / state.progressState.totalNfts) * 50;
        progressPercentage = Math.min(ownerProgress, 50).toFixed(1);
      } else if (state.progressState.step === 'fetching_tiers') {
        const tierProgress = (state.progressState.processedTiers / state.progressState.totalTiers) * 50;
        progressPercentage = Math.min(50 + tierProgress, 100).toFixed(1);
      }
    } else if (['fetching_owners', 'fetching_tiers'].includes(state.progressState.step)) {
      logger.debug('progress', 'No NFTs found, progress remains at 0.0', chain, contractKey);
    }

    // Build response
    const response = {
      isPopulating: state.isPopulating,
      totalLiveHolders: state.totalOwners,
      totalOwners: state.totalOwners,
      phase: state.progressState.step
        ? state.progressState.step.charAt(0).toUpperCase() + state.progressState.step.slice(1)
        : 'Unknown',
      progressPercentage,
      lastProcessedBlock: state.lastProcessedBlock,
      lastUpdated: state.lastUpdated,
      error: state.progressState.error || null,
      errorLog: (state.progressState.errorLog || []).slice(-50),
      isErrorLogTruncated: (state.progressState.errorLog || []).length > 50,
      globalMetrics: state.globalMetrics,
      // Added status field (from file1)
      status: state.progressState.error ? 'error' : state.isPopulating ? 'pending' : 'success',
    };

    // Validate response schema (retained from existing code)
    const validation = ProgressResponseSchema.safeParse(response);
    if (!validation.success) {
      logger.error('progress', `Invalid response data: ${JSON.stringify(validation.error.errors)}`, chain, contractKey);
      return NextResponse.json({ error: 'Invalid response data' }, { status: 500 });
    }

    // Apply sanitizeBigInt to response (from file1)
    return NextResponse.json(sanitizeBigInt(response));
  } catch (error) {
    logger.error('progress', `GET /api/holders/${contractKey}/progress: ${error.message}`, { stack: error.stack }, chain, contractKey);
    return NextResponse.json({ error: error.message }, { status: 500 });
  }
}
----- app/api/holders/[contract]/route.js -----

// app/api/holders/[contract]/route.js
import { NextResponse } from 'next/server';
import config from '@/contracts/config.js';
import { logger } from '@/app/lib/logger';
import { HoldersResponseSchema } from '@/app/lib/schemas';
import { getCacheState, saveCacheStateContract } from '@/app/api/holders/cache/state';
import { populateHoldersMapCache } from '@/app/api/holders/cache/holders';
import { validateContract, getCache } from '@/app/api/utils/cache.js';
import { sanitizeBigInt } from '@/app/api/holders/cache/holders';

export async function GET(request, { params }) {
  const { contract } = await params;
  const contractKey = contract.toLowerCase();

  if (!config.nftContracts[contractKey]) {
    logger.error('holders', `Invalid contract: ${contractKey}`, 'eth', contractKey);
    return NextResponse.json({ error: 'Invalid contract' }, { status: 400 });
  }

  const { contractAddress, abi } = config.nftContracts[contractKey];
  const cacheState = await getCacheState(contractKey);

  const { searchParams } = new URL(request.url);
  const page = parseInt(searchParams.get('page') || '0', 10);
  const pageSize = parseInt(searchParams.get('pageSize') || config.contractDetails[contractKey].pageSize, 10);

  const cachedData = await getCache(`${contractKey}_holders`, contractKey);
  const isBurnContract = ['stax', 'element280', 'element369'].includes(contractKey);

  if (cachedData) {
    const holders = cachedData.holders.slice(page * pageSize, (page + 1) * pageSize);
    const totalPages = Math.ceil(cachedData.holders.length / pageSize);
    const totalTokens = cachedData.holders.reduce((sum, h) => sum + h.total, 0);
    const totalBurned = isBurnContract ? Number(cachedData.totalBurned) || 0 : 0;
    const maxTier = Math.max(...Object.keys(config.nftContracts[contractKey]?.tiers || {}).map(Number), 0);
    const response = {
      holders: sanitizeBigInt(holders),
      totalPages,
      totalTokens,
      totalBurned,
      summary: {
        totalLive: totalTokens,
        totalBurned,
        totalMinted: totalTokens + totalBurned,
        tierDistribution: cachedData.holders.reduce((acc, h) => {
          h.tiers.forEach((count, i) => (acc[i] = (acc[i] || 0) + count));
          return acc;
        }, Array(contractKey === 'ascendant' ? maxTier + 1 : maxTier).fill(0)),
        multiplierPool: cachedData.holders.reduce((sum, h) => sum + h.multiplierSum, 0),
        ...(contractKey === 'ascendant' ? { rarityDistribution: cacheState.globalMetrics.rarityDistribution || Array(3).fill(0) } : {}),
      },
      globalMetrics: cacheState.globalMetrics || {},
    };
    logger.debug('holders', `GET response: holders=${holders.length}, totalPages=${totalPages}`, 'eth', contractKey);
    return NextResponse.json(response);
  }

  const { status, holders } = await populateHoldersMapCache(contractKey, contractAddress, abi, null, null);
  if (status === 'error') {
    logger.error('holders', `Cache population failed for ${contractKey}`, 'eth', contractKey);
    return NextResponse.json({ error: 'Cache population failed' }, { status: 500 });
  }

  const paginatedHolders = holders.slice(page * pageSize, (page + 1) * pageSize);
  const totalPages = Math.ceil(holders.length / pageSize);
  const cachedDataAfterPopulation = await getCache(`${contractKey}_holders`, contractKey);
  const totalBurned = isBurnContract ? Number(cachedDataAfterPopulation?.totalBurned) || 0 : 0;
  const totalTokens = holders.reduce((sum, h) => sum + h.total, 0);
  const maxTier = Math.max(...Object.keys(config.nftContracts[contractKey]?.tiers || {}).map(Number), 0);
  const response = {
    holders: sanitizeBigInt(paginatedHolders),
    totalPages,
    totalTokens,
    totalBurned,
    summary: {
      totalLive: totalTokens,
      totalBurned,
      totalMinted: totalTokens + totalBurned,
      tierDistribution: holders.reduce((acc, h) => {
        h.tiers.forEach((count, i) => (acc[i] = (acc[i] || 0) + count));
        return acc;
      }, Array(contractKey === 'ascendant' ? maxTier + 1 : maxTier).fill(0)),
      multiplierPool: holders.reduce((sum, h) => sum + h.multiplierSum, 0),
      ...(contractKey === 'ascendant' ? { rarityDistribution: cacheState.globalMetrics.rarityDistribution || Array(3).fill(0) } : {}),
    },
    globalMetrics: cacheState.globalMetrics || {},
  };
  logger.debug('holders', `GET response: holders=${paginatedHolders.length}, totalPages=${totalPages}`, 'eth', contractKey);
  return NextResponse.json(response);
}

export async function POST(request, { params }) {
  const { contract } = await params;
  const contractKey = contract.toLowerCase();
  const chain = config.nftContracts[contractKey]?.chain || 'eth';
  const { forceUpdate = false } = await request.json().catch(() => ({}));

  logger.debug('holders', `POST request received for ${contractKey}, forceUpdate=${forceUpdate}`, chain, contractKey);

  if (!config.nftContracts[contractKey]) {
    logger.error('holders', `Invalid contract: ${contractKey}. Available: ${Object.keys(config.nftContracts).join(', ')}`, chain, contractKey);
    return NextResponse.json({ message: `Invalid contract: ${contractKey}`, status: 'error' }, { status: 400 });
  }

  let contractAddress, abi, vaultAddress, vaultAbi;
  try {
    const contractConfig = config.nftContracts[contractKey];
    ({ contractAddress, abi, vaultAddress, vaultAbi } = contractConfig);
    logger.debug('holders', `Calling validateContract for ${contractKey}`, chain, contractKey);
    const isValid = await validateContract(contractKey);
    if (!isValid) {
      logger.error('holders', `Contract validation failed for ${contractKey}`, chain, contractKey);
      throw new Error(`Invalid contract address for ${contractKey}`);
    }
  } catch (error) {
    logger.error('holders', `Validation error for ${contractKey}: ${error.message}`, { stack: error.stack }, chain, contractKey);
    return NextResponse.json({ message: `Validation error: ${error.message}`, status: 'error' }, { status: 400 });
  }

  let cacheState = await getCacheState(contractKey);
  try {
    logger.debug('holders', `Triggering cache population for ${contractKey}`, chain, contractKey);
    const result = await populateHoldersMapCache(contractKey, contractAddress, abi, vaultAddress, vaultAbi, forceUpdate);

    if (result.status === 'pending') {
      logger.info('holders', `Cache population in progress for ${contractKey}`, chain, contractKey);
      cacheState = await getCacheState(contractKey);
      return NextResponse.json(
        { message: 'Cache population in progress', status: 'pending', cacheState: sanitizeBigInt(cacheState) },
        { status: 202 }
      );
    }

    logger.info('holders', `Cache population completed for ${contractKey}: ${result.holders.length} holders`, chain, contractKey);
    cacheState = await getCacheState(contractKey);
    return NextResponse.json(
      {
        message: 'Cache population completed',
        status: 'success',
        cacheState: sanitizeBigInt(cacheState),
        holders: sanitizeBigInt(result.holders),
      },
      { status: 200 }
    );
  } catch (error) {
    logger.error('holders', `Failed to populate cache for ${contractKey}: ${error.message}`, { stack: error.stack }, chain, contractKey);
    cacheState.isPopulating = false;
    cacheState.progressState.step = 'error';
    cacheState.progressState.error = error.message;
    cacheState.progressState.errorLog = cacheState.progressState.errorLog || [];
    cacheState.progressState.errorLog.push({
      timestamp: new Date().toISOString(),
      phase: 'post_handler',
      error: error.message,
    });
    await saveCacheStateContract(contractKey, cacheState);
    return NextResponse.json(
      { message: `Failed to populate cache: ${error.message}`, status: 'error' },
      { status: 500 }
    );
  }
}
----- app/api/holders/blockchain/events.js -----

// app/api/holders/blockchain/events.js
import { parseAbiItem } from 'viem';
import pLimit from 'p-limit';
import config from '@/contracts/config';
import { client } from '@/app/api/utils/client';
import { retry } from '@/app/api/utils/retry';
import { getCache, setCache } from '@/app/api/utils/cache';
import { logger } from '@/app/lib/logger';
import { getCacheState, saveCacheStateContract } from '@/app/api/holders/cache/state';

const concurrencyLimit = pLimit(2); // Reduced for Alchemy Free Tier

export async function getNewEvents(contractKey, contractAddress, fromBlock, errorLog) {
  const burnAddress = config.burnAddress || '0x0000000000000000000000000000000000000000';
  const prefix = contractKey.toLowerCase();
  const chain = config.nftContracts[prefix]?.chain || 'eth';
  const cacheKey = `${prefix}_events_${contractAddress}_${fromBlock}`;
  let cachedEvents = await getCache(cacheKey, prefix);

  if (cachedEvents) {
    logger.info(
      'utils',
      `Events cache hit: ${cacheKey}, burns: ${cachedEvents.burnedTokenIds.length}, transfers: ${cachedEvents.transferTokenIds?.length || 0}`,
      chain,
      contractKey
    );
    return cachedEvents;
  }

  let burnedTokenIds = [];
  let transferTokenIds = [];
  let endBlock;
  try {
    endBlock = await retry(
      () => client.getBlockNumber(),
      { retries: 3, delay: 1000, backoff: true }
    );
  } catch (error) {
    logger.error('utils', `Failed to fetch block number: ${error.message}`, { stack: error.stack }, chain, contractKey);
    errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_block_number', error: error.message });
    throw error;
  }

  if (fromBlock >= endBlock) {
    logger.info('utils', `No new blocks: fromBlock ${fromBlock} >= endBlock ${endBlock}`, chain, contractKey);
    return { burnedTokenIds, transferTokenIds, lastBlock: Number(endBlock), timestamp: Date.now() };
  }

  const maxBlockRange = 200; // Reduced for Alchemy Free Tier
  const transferEvent = parseAbiItem('event Transfer(address indexed from, address indexed to, uint256 indexed tokenId)');
  const blockRanges = [];
  let currentFromBlock = BigInt(fromBlock);

  // Generate block ranges
  while (currentFromBlock <= endBlock) {
    const toBlock = BigInt(Math.min(Number(currentFromBlock) + maxBlockRange, Number(endBlock)));
    blockRanges.push({ fromBlock: currentFromBlock, toBlock });
    currentFromBlock = toBlock + 1n;
  }

  logger.info('utils', `Processing ${blockRanges.length} block ranges from ${fromBlock} to ${endBlock}`, chain, contractKey);

  // Load cache state for lastProcessedBlock updates
  let cacheState = await getCacheState(contractKey);

  // Process block ranges sequentially to avoid rate limits
  for (const range of blockRanges) {
    const rangeCacheKey = `${prefix}_events_${contractAddress}_${range.fromBlock}_${range.toBlock}`;
    let rangeEvents = await getCache(rangeCacheKey, prefix);

    if (rangeEvents) {
      logger.debug(
        'utils',
        `Range cache hit: ${rangeCacheKey}, burns: ${rangeEvents.burnedTokenIds.length}, transfers: ${rangeEvents.transferTokenIds?.length || 0}`,
        chain,
        contractKey
      );
      burnedTokenIds.push(...rangeEvents.burnedTokenIds);
      transferTokenIds.push(...rangeEvents.transferTokenIds);
      cacheState.lastProcessedBlock = Number(range.toBlock);
      cacheState.progressState.lastProcessedBlock = Number(range.toBlock);
      cacheState.progressState.lastUpdated = Date.now();
      await saveCacheStateContract(contractKey, cacheState);
      logger.debug('utils', `Updated lastProcessedBlock to ${range.toBlock} for range ${rangeCacheKey}`, chain, contractKey);
      continue;
    }

    let attempt = 0;
    const maxAttempts = 2;
    let currentToBlock = range.toBlock;
    let dynamicRange = maxBlockRange;

    while (attempt < maxAttempts) {
      try {
        logger.debug(
          'utils',
          `Fetching logs from block ${range.fromBlock} to ${currentToBlock} (attempt ${attempt + 1}/${maxAttempts})`,
          chain,
          contractKey
        );
        const logs = await retry(
          () =>
            client.getLogs({
              address: contractAddress,
              event: transferEvent,
              fromBlock: range.fromBlock,
              toBlock: currentToBlock,
            }),
          { retries: 2, delay: 1000, backoff: true }
        );

        const newBurned = logs
          .filter(log => log.args.to.toLowerCase() === burnAddress.toLowerCase())
          .map(log => Number(log.args.tokenId));
        const newTransfers = logs
          .filter(log => log.args.to.toLowerCase() !== burnAddress.toLowerCase())
          .map(log => ({
            tokenId: Number(log.args.tokenId),
            from: log.args.from.toLowerCase(),
            to: log.args.to.toLowerCase(),
          }));

        rangeEvents = {
          burnedTokenIds: newBurned,
          transferTokenIds: newTransfers,
          lastBlock: Number(currentToBlock),
          timestamp: Date.now(),
        };
        await setCache(rangeCacheKey, rangeEvents, config.cache.nodeCache.stdTTL || 86400, prefix);
        burnedTokenIds.push(...newBurned);
        transferTokenIds.push(...newTransfers);
        logger.info(
          'utils',
          `Fetched ${newBurned.length} burn events and ${newTransfers.length} transfer events for blocks ${range.fromBlock} to ${currentToBlock}`,
          chain,
          contractKey
        );

        // Update lastProcessedBlock
        cacheState.lastProcessedBlock = Number(currentToBlock);
        cacheState.progressState.lastProcessedBlock = Number(currentToBlock);
        cacheState.progressState.lastUpdated = Date.now();
        await saveCacheStateContract(contractKey, cacheState);
        logger.debug('utils', `Updated lastProcessedBlock to ${currentToBlock} for range ${rangeCacheKey}`, chain, contractKey);
        break;
      } catch (error) {
        attempt++;
        logger.warn(
          'utils',
          `Attempt ${attempt}/${maxAttempts} failed for blocks ${range.fromBlock} to ${currentToBlock}: ${error.message}`,
          chain,
          contractKey
        );

        // Parse error for suggested block range
        const suggestedRangeMatch = error.message.match(/this block range should work: \[(0x[a-fA-F0-9]+), (0x[a-fA-F0-9]+)\]/);
        if (suggestedRangeMatch && attempt < maxAttempts) {
          const [, , suggestedTo] = suggestedRangeMatch;
          currentToBlock = BigInt(parseInt(suggestedTo, 16));
          dynamicRange = Number(currentToBlock - range.fromBlock);
          logger.info(
            'utils',
            `Adjusted to suggested block range: ${range.fromBlock} to ${currentToBlock} (${dynamicRange} blocks)`,
            chain,
            contractKey
          );
          continue;
        }

        // Reduce range size if error persists
        if (error.message.includes('Log response size exceeded') && dynamicRange > 50) {
          dynamicRange = Math.max(50, Math.floor(dynamicRange / 2));
          currentToBlock = range.fromBlock + BigInt(dynamicRange);
          logger.info(
            'utils',
            `Reduced block range to ${dynamicRange} blocks: ${range.fromBlock} to ${currentToBlock}`,
            chain,
            contractKey
          );
          continue;
        }

        errorLog.push({
          timestamp: new Date().toISOString(),
          phase: 'fetch_events',
          fromBlock: Number(range.fromBlock),
          toBlock: Number(currentToBlock),
          error: error.message,
        });

        if (attempt >= maxAttempts) {
          logger.error(
            'utils',
            `Max attempts reached for blocks ${range.fromBlock} to ${currentToBlock}: ${error.message}`,
            { stack: error.stack },
            chain,
            contractKey
          );
          rangeEvents = {
            burnedTokenIds: [],
            transferTokenIds: [],
            lastBlock: Number(range.fromBlock),
            timestamp: Date.now(),
          };
          await setCache(rangeCacheKey, rangeEvents, config.cache.nodeCache.stdTTL || 86400, prefix);
          cacheState.lastProcessedBlock = Number(range.fromBlock);
          cacheState.progressState.lastProcessedBlock = Number(range.fromBlock);
          cacheState.progressState.lastUpdated = Date.now();
          await saveCacheStateContract(contractKey, cacheState);
          logger.debug('utils', `Updated lastProcessedBlock to ${range.fromBlock} on failure for range ${rangeCacheKey}`, chain, contractKey);
          break;
        }
      }
    }
  }

  const lastBlock = Number(endBlock);
  const cacheData = { burnedTokenIds, transferTokenIds, lastBlock, timestamp: Date.now() };
  await setCache(cacheKey, cacheData, config.cache.nodeCache.stdTTL || 86400, prefix);
  logger.info(
    'utils',
    `Cached events: ${cacheKey}, burns: ${burnedTokenIds.length}, transfers: ${transferTokenIds.length}, lastBlock: ${lastBlock}`,
    chain,
    contractKey
  );

  return cacheData;
}
----- app/api/holders/blockchain/multicall.js -----

import pLimit from 'p-limit';
import { client } from '@/app/api/utils/client';
import { logger } from '@/app/lib/logger';
import config from '@/contracts/config';

const concurrencyLimit = pLimit(3);

export async function batchMulticall(calls, batchSize = config.alchemy.batchSize || 10) {
  const results = [];
  const delay = async () => new Promise(resolve => setTimeout(resolve, config.alchemy.batchDelayMs || 500));

  const batchPromises = [];
  for (let i = 0; i < calls.length; i += batchSize) {
    const batch = calls.slice(i, i + batchSize);
    batchPromises.push(
      concurrencyLimit(async () => {
        try {
          await delay();
          const batchResults = await client.multicall({
            contracts: batch.map(call => ({
              address: call.address,
              abi: call.abi,
              functionName: call.functionName,
              args: call.args || [],
            })),
            allowFailure: true,
          });

          const batchResult = batchResults.map((result, index) => ({
            status: result.status === 'success' ? 'success' : 'failure',
            result: result.status === 'success' ? result.result : null,
            error: result.status === 'failure' ? result.error?.message || 'Unknown error' : null,
          }));
          return batchResult;
        } catch (error) {
          logger.error('blockchain/multicall', `Batch multicall failed: ${error.message}`, { stack: error.stack }, 'eth', 'general');
          return batch.map(() => ({
            status: 'failure',
            result: null,
            error: error.message,
          }));
        }
      })
    );
  }

  const batchResults = (await Promise.all(batchPromises)).flat();
  results.push(...batchResults);
  return results;
}
----- app/api/holders/blockchain/owners.js -----

import { Alchemy } from 'alchemy-sdk';
import { logger } from '@/app/lib/logger';
import config from '@/contracts/config';

const alchemy = new Alchemy({
  apiKey: config.alchemy.apiKey,
  network: 'eth-mainnet',
});

export async function getOwnersForContract(contractAddress, abi, options = {}) {
  let owners = [];
  let pageKey = options.pageKey || null;
  const maxPages = options.maxPages || 10;
  let pageCount = 0;

  logger.debug(
    'utils',
    `Fetching owners for contract: ${contractAddress} with options: ${JSON.stringify(options)}`,
    'eth',
    'general'
  );

  do {
    try {
      const response = await alchemy.nft.getOwnersForContract(contractAddress, {
        withTokenBalances: options.withTokenBalances || false,
        pageKey,
      });

      logger.debug(
        'utils',
        `Raw Alchemy response: ownersExists=${!!response.owners}, isArray=${Array.isArray(response.owners)}, ownersLength=${
          response.owners?.length || 0
        }, pageKey=${response.pageKey || null}, responseKeys=${Object.keys(response)}, sampleOwners=${JSON.stringify(
          response.owners?.slice(0, 2) || []
        )}`,
        'eth',
        'general'
      );

      if (!response.owners || !Array.isArray(response.owners)) {
        logger.error('utils', `Invalid Alchemy response for ${contractAddress}: ${JSON.stringify(response)}`, {}, 'eth', 'general');
        throw new Error('Invalid owners response from Alchemy API');
      }

      for (const owner of response.owners) {
        const tokenBalances = owner.tokenBalances || [];
        logger.debug(
          'utils',
          `Processing owner: ${owner.ownerAddress}, tokenBalancesCount=${tokenBalances.length}`,
          'eth',
          'general'
        );

        if (tokenBalances.length > 0) {
          const validBalances = tokenBalances.filter(tb => tb.tokenId && Number(tb.balance) > 0);
          if (validBalances.length > 0) {
            owners.push({
              ownerAddress: owner.ownerAddress.toLowerCase(),
              tokenBalances: validBalances.map(tb => ({
                tokenId: Number(tb.tokenId),
                balance: Number(tb.balance),
              })),
            });
          }
        }
      }

      pageKey = response.pageKey || null;
      pageCount++;
      logger.debug('utils', `Fetched page ${pageCount}, owners: ${owners.length}, pageKey: ${pageKey}`, 'eth', 'general');

      if (pageCount >= maxPages) {
        logger.warn('utils', `Reached max pages (${maxPages}) for owner fetching`, 'eth', 'general');
        break;
      }
    } catch (error) {
      logger.error('utils', `Failed to fetch owners for ${contractAddress}: ${error.message}`, { stack: error.stack }, 'eth', 'general');
      throw error;
    }
  } while (pageKey);

  logger.debug('utils', `Processed owners: count=${owners.length}, sample=${JSON.stringify(owners.slice(0, 2))}`, 'eth', 'general');
  logger.info('utils', `Fetched ${owners.length} owners for contract: ${contractAddress}`, 'eth', 'general');
  return owners;
}
----- app/api/holders/cache/holders.js -----

// app/api/holders/cache/holders.js
import { parseAbiItem, formatUnits, getAddress } from 'viem';
import pLimit from 'p-limit';
import config from '@/contracts/config.js';
import { logger } from '@/app/lib/logger';
import { getCacheState, saveCacheStateContract } from '@/app/api/holders/cache/state';
import { getNewEvents } from '@/app/api/holders/blockchain/events';
import { getOwnersForContract } from '@/app/api/holders/blockchain/owners';
import { client } from '@/app/api/utils/client';
import { batchMulticall } from '@/app/api/holders/blockchain/multicall';
import { retry } from '@/app/api/utils/retry';
import { mkdir } from 'fs/promises';
import { join } from 'path';
import { getCache, setCache, validateContract } from '@/app/api/utils/cache';

const limit = pLimit(5);
const ownershipChunkLimit = pLimit(2); // Reduced for Alchemy Free Tier

// Ensure cache directory exists
async function ensureCacheDirectory() {
  const cacheDir = join(process.cwd(), 'cache');
  const chain = 'eth';
  const collection = 'general';
  try {
    logger.debug('holders', `Ensuring cache directory at: ${cacheDir}`, chain, collection);
    await mkdir(cacheDir, { recursive: true });
    logger.info('holders', `Cache directory created or exists: ${cacheDir}`, chain, collection);
  } catch (error) {
    logger.error('holders', `Failed to create cache directory: ${error.message}`, { stack: error.stack }, chain, collection);
    throw new Error(`Cache directory creation failed: ${error.message}`);
  }
}

// Utility to safely stringify objects for logging
function safeStringify(obj) {
  try {
    return JSON.stringify(obj);
  } catch (e) {
    return String(obj);
  }
}

function sanitizeBigInt(obj) {
  if (typeof obj === 'bigint') return obj.toString();
  if (Array.isArray(obj)) return obj.map(item => sanitizeBigInt(item));
  if (typeof obj === 'object' && obj !== null) {
    const sanitized = {};
    for (const [key, value] of Object.entries(obj)) {
      sanitized[key] = sanitizeBigInt(value);
    }
    return sanitized;
  }
  return obj;
}

export async function getHoldersMap(contractKey, contractAddress, abi, vaultAddress, vaultAbi, cacheState, forceUpdate = false) {
  if (!contractAddress) throw new Error('Contract address missing');
  if (!abi) throw new Error(`${contractKey} ABI missing`);

  contractKey = contractKey.toLowerCase();
  const chain = config.nftContracts[contractKey]?.chain || 'eth';
  logger.info('holders', `Starting getHoldersMap: contractKey=${contractKey}, forceUpdate=${forceUpdate}, contractAddress=${contractAddress}`, chain, contractKey);

  const requiredFunctions = contractKey === 'ascendant' ? ['getNFTAttribute', 'userRecords', 'totalShares', 'toDistribute', 'batchClaimableAmount'] : ['totalSupply', 'totalBurned', 'ownerOf', 'getNftTier'];
  const missingFunctions = requiredFunctions.filter(fn => !abi.some(item => item.name === fn && item.type === 'function'));
  if (missingFunctions.length > 0) throw new Error(`Missing ABI functions: ${missingFunctions.join(', ')}`);

  const burnAddress = config.burnAddress || '0x0000000000000000000000000000000000000000';
  let holdersMap = new Map();
  let totalBurned = cacheState.totalBurned || 0;
  let errorLog = cacheState.progressState.errorLog || [];
  let totalLockedAscendant = 0;
  let totalShares = 0;
  let toDistributeDay8 = 0;
  let toDistributeDay28 = 0;
  let toDistributeDay90 = 0;
  let totalTokens = 0;
  let tokenOwnerMap = new Map();
  const cachedTokenTiers = new Map();

  const contractTiers = config.nftContracts[contractKey]?.tiers || {};
  const maxTier = Math.max(...Object.keys(contractTiers).map(Number), 0);
  let rarityDistribution = contractKey === 'ascendant' ? Array(3).fill(0) : [];
  let tierDistribution = Array(maxTier + 1).fill(0);

  cacheState.progressState.step = 'checking_cache';
  cacheState.progressState.progressPercentage = '0%';
  await saveCacheStateContract(contractKey, cacheState);
  logger.debug('holders', `Progress state updated to checking_cache for ${contractKey}`, chain, contractKey);

  let currentBlock;
  try {
    logger.debug('holders', `Fetching current block number for ${contractKey}`, chain, contractKey);
    currentBlock = await retry(
      () => client.getBlockNumber(),
      { retries: 3, delay: 1000, backoff: true }
    );
    logger.debug('holders', `Fetched current block: ${currentBlock}`, chain, contractKey);
  } catch (error) {
    errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_block_number', error: error.message });
    logger.error('holders', `Failed to fetch block number: ${error.message}`, { stack: error.stack }, chain, contractKey);
    throw error;
  }

  // Initialize lastProcessedBlock if not set
  if (!cacheState.lastProcessedBlock) {
    cacheState.lastProcessedBlock = config.nftContracts[contractKey]?.deploymentBlock || 0;
    cacheState.progressState.lastProcessedBlock = cacheState.lastProcessedBlock;
    await saveCacheStateContract(contractKey, cacheState);
    logger.debug('holders', `Initialized lastProcessedBlock to ${cacheState.lastProcessedBlock} for ${contractKey}`, chain, contractKey);
  }

  // Apply event-based updates for all configured contracts
  if (config.nftContracts[contractKey]) {
    let fromBlock = BigInt(cacheState.lastProcessedBlock);
    logger.debug('holders', `Initial fromBlock: ${fromBlock}, deploymentBlock: ${config.nftContracts[contractKey].deploymentBlock}`, chain, contractKey);
    const maxBlockRange = 200; // Match events.js for Alchemy Free Tier
    let burnedTokenIds = [];
    let transferTokenIds = [];
    let cachedHolders, cachedTiers;
    let updatedTokenIds = new Set();
    let lastBlock = fromBlock;

    // Load cached data if available and not forceUpdate
    if (!forceUpdate) {
      try {
        logger.debug('holders', `Attempting to load cache for ${contractKey}_holders`, chain, contractKey);
        cachedHolders = await getCache(`${contractKey}_holders`, contractKey);
        cachedTiers = await getCache(`${contractKey}_tiers`, contractKey) || {};
        if (cachedHolders?.holders && Array.isArray(cachedHolders.holders) && Number.isInteger(cachedHolders.totalBurned)) {
          holdersMap = new Map(cachedHolders.holders.map(h => [h.wallet, h]));
          totalBurned = cachedHolders.totalBurned || totalBurned;
          totalTokens = cacheState.progressState.totalNfts || 0;
          holdersMap.forEach(holder => {
            holder.tokenIds.forEach(tokenId => tokenOwnerMap.set(Number(tokenId), holder.wallet));
          });
          Object.entries(cachedTiers).forEach(([tokenId, tierData]) => {
            if (tierData && typeof tierData.tier === 'number') {
              cachedTokenTiers.set(Number(tokenId), tierData);
              tierDistribution[tierData.tier] += 1;
            }
          });
          logger.info(
            'holders',
            `Cache hit: holders=${holdersMap.size}, tiers=${cachedTokenTiers.size}, lastBlock=${cacheState.lastProcessedBlock}`,
            chain,
            contractKey
          );
        } else {
          logger.warn('holders', `Invalid holders cache data for ${contractKey}: ${safeStringify(cachedHolders)}`, chain, contractKey);
          cachedHolders = null;
        }
      } catch (error) {
        logger.error('holders', `Failed to load cache for ${contractKey}: ${error.message}`, { stack: error.stack }, chain, contractKey);
        errorLog.push({ timestamp: new Date().toISOString(), phase: 'load_cache', error: error.message });
        cachedHolders = null;
      }
    }

    // Fetch new events sequentially
    logger.debug('holders', `Checking event-based update for ${contractKey}, lastProcessedBlock=${cacheState.lastProcessedBlock}`, chain, contractKey);
    while (fromBlock < currentBlock) {
      const toBlock = BigInt(Math.min(Number(fromBlock) + maxBlockRange, Number(currentBlock)));
      try {
        logger.debug('holders', `Fetching events from ${fromBlock} to ${toBlock}`, chain, contractKey);
        const events = await getNewEvents(contractKey, contractAddress, Number(fromBlock), errorLog);
        burnedTokenIds.push(...events.burnedTokenIds);
        transferTokenIds.push(...events.transferTokenIds);
        lastBlock = BigInt(events.lastBlock);
        fromBlock = toBlock + 1n;

        // Update lastProcessedBlock
        cacheState.lastProcessedBlock = Number(lastBlock);
        cacheState.progressState.lastProcessedBlock = Number(lastBlock);
        cacheState.progressState.lastUpdated = Date.now();
        await saveCacheStateContract(contractKey, cacheState);
        logger.debug('holders', `Processed events and updated lastProcessedBlock to ${lastBlock} for blocks ${fromBlock} to ${toBlock}`, chain, contractKey);
      } catch (error) {
        logger.error(
          'holders',
          `Failed to fetch events for blocks ${fromBlock} to ${toBlock}: ${error.message}`,
          { stack: error.stack },
          chain,
          contractKey
        );
        errorLog.push({
          timestamp: new Date().toISOString(),
          phase: 'fetch_events',
          fromBlock: Number(fromBlock),
          toBlock: Number(toBlock),
          error: error.message,
        });
        fromBlock = toBlock + 1n;
        lastBlock = toBlock;
        cacheState.lastProcessedBlock = Number(lastBlock);
        cacheState.progressState.lastProcessedBlock = Number(lastBlock);
        cacheState.progressState.lastUpdated = Date.now();
        await saveCacheStateContract(contractKey, cacheState);
        logger.debug('holders', `Updated lastProcessedBlock to ${lastBlock} after error`, chain, contractKey);
        continue;
      }
    }

    // Final state update
    cacheState.progressState.lastProcessedBlock = Number(lastBlock);
    cacheState.lastProcessedBlock = Number(lastBlock);
    cacheState.progressState.lastUpdated = Date.now();
    await saveCacheStateContract(contractKey, cacheState);
    logger.debug('holders', `Final lastProcessedBlock update to ${lastBlock} for ${contractKey}`, chain, contractKey);

    logger.debug(
      'holders',
      `New events: burns=${burnedTokenIds.length}, transfers=${transferTokenIds.length}, fromBlock=${cacheState.lastProcessedBlock}, toBlock=${lastBlock}`,
      chain,
      contractKey
    );

    if (cachedHolders && !forceUpdate) {
      // Process burns
      burnedTokenIds.forEach(tokenId => {
        const wallet = tokenOwnerMap.get(tokenId);
        if (wallet) {
          const holder = holdersMap.get(wallet);
          if (holder) {
            holder.tokenIds = holder.tokenIds.filter(id => id !== tokenId);
            holder.total -= 1;
            const tier = cachedTokenTiers.get(tokenId)?.tier || 0;
            holder.tiers[tier] -= 1;
            holder.multiplierSum -= contractTiers[tier + 1]?.multiplier || tier + 1;
            if (holder.total === 0) holdersMap.delete(wallet);
            tokenOwnerMap.delete(tokenId);
            cachedTokenTiers.delete(tokenId);
            totalTokens -= 1;
            totalBurned += 1;
            tierDistribution[tier] -= 1;
          }
        }
      });

      // Process transfers
      transferTokenIds.forEach(({ tokenId, from, to }) => {
        updatedTokenIds.add(tokenId);
        const oldHolder = holdersMap.get(from);
        if (oldHolder) {
          oldHolder.tokenIds = oldHolder.tokenIds.filter(id => id !== tokenId);
          oldHolder.total -= 1;
          const tier = cachedTokenTiers.get(tokenId)?.tier || 0;
          oldHolder.tiers[tier] -= 1;
          oldHolder.multiplierSum -= contractTiers[tier + 1]?.multiplier || tier + 1;
          if (oldHolder.total === 0) holdersMap.delete(from);
        }
        let newHolder =
          holdersMap.get(to) ||
          {
            wallet: to,
            tokenIds: [],
            tiers: Array(maxTier + 1).fill(0),
            total: 0,
            multiplierSum: 0,
            claimableRewards: 0,
          };
        newHolder.tokenIds.push(tokenId);
        newHolder.total += 1;
        const tier = cachedTokenTiers.get(tokenId)?.tier || 0;
        newHolder.tiers[tier] += 1;
        newHolder.multiplierSum += contractTiers[tier + 1]?.multiplier || tier + 1;
        holdersMap.set(to, newHolder);
        tokenOwnerMap.set(tokenId, to);
      });

      // Verify token ownership
      cacheState.progressState.step = 'verifying_ownership';
      cacheState.progressState.progressPercentage = '45%';
      await saveCacheStateContract(contractKey, cacheState);
      logger.debug('holders', `Verifying ownership for ${contractKey}`, chain, contractKey);

      const tokenIds = Array.from(tokenOwnerMap.keys());
      const ownershipCalls = tokenIds.map(tokenId => ({
        address: contractAddress,
        abi,
        functionName: 'ownerOf',
        args: [BigInt(tokenId)],
      }));

      const ownershipResults = [];
      const chunkSize = config.nftContracts[contractKey]?.maxTokensPerOwnerQuery || 200;
      const totalChunks = Math.ceil(ownershipCalls.length / chunkSize);
      for (let i = 0; i < ownershipCalls.length; i += chunkSize) {
        const chunk = ownershipCalls.slice(i, i + chunkSize);
        try {
          const results = await retry(
            () => batchMulticall(chunk, config.alchemy.batchSize || 50),
            { retries: 3, delay: 1000, backoff: true }
          );
          ownershipResults.push(...results);
          cacheState.progressState.progressPercentage = `${Math.round(45 + (i / ownershipCalls.length) * 5)}%`;
          await saveCacheStateContract(contractKey, cacheState);
          logger.debug(
            'holders',
            `Processed ownership chunk ${Math.floor(i / chunkSize) + 1}/${totalChunks} for ${chunk.length} tokens`,
            chain,
            contractKey
          );
        } catch (error) {
          logger.error(
            'holders',
            `Failed to process ownership chunk ${i / chunkSize + 1}: ${error.message}`,
            { stack: error.stack },
            chain,
            contractKey
          );
          errorLog.push({
            timestamp: new Date().toISOString(),
            phase: 'verify_ownership',
            chunk: i / chunkSize + 1,
            error: error.message,
          });
          ownershipResults.push(...chunk.map(() => ({ status: 'failure', error: error.message })));
        }
      }

      const validTokenIds = tokenIds.filter((tokenId, i) => {
        const result = ownershipResults[i];
        if (result.status === 'success' && result.result.toLowerCase() !== burnAddress.toLowerCase()) {
          return true;
        }
        logger.warn(
          'holders',
          `Skipping burned or invalid token ${tokenId} for ${contractKey}: owner=${result.result || 'unknown'}`,
          chain,
          contractKey
        );
        tokenOwnerMap.delete(tokenId);
        totalTokens -= 1;
        totalBurned += 1;
        return false;
      });

      logger.info(
        'holders',
        `Verified ownership for ${validTokenIds.length} tokens, excluded ${tokenIds.length - validTokenIds.length} burned/invalid tokens`,
        chain,
        contractKey
      );

      if (totalTokens === 0) {
        logger.info('holders', `No live tokens found for ${contractKey}, writing empty holders`, chain, contractKey);
        cacheState.progressState.step = 'completed';
        cacheState.progressState.progressPercentage = '100%';
        cacheState.globalMetrics = {
          totalMinted: totalTokens + totalBurned,
          totalLive: totalTokens,
          totalBurned,
          tierDistribution,
        };
        await saveCacheStateContract(contractKey, cacheState);
        await setCache(`${contractKey}_holders`, { holders: [], totalBurned, timestamp: Date.now() }, 0, contractKey);
        await setCache(`${contractKey}_tiers`, {}, config.cache.nodeCache.stdTTL || 86400, contractKey);
        logger.info('holders', `Wrote empty holders to cache for ${contractKey}`, chain, contractKey);
        return { holdersMap, totalBurned, lastBlock: Number(lastBlock), errorLog, rarityDistribution };
      }

      // Fetch tiers for updated or missing tokens
      const missingTierTokenIds = validTokenIds.filter(tokenId => !cachedTokenTiers.has(tokenId) || updatedTokenIds.has(tokenId));
      if (missingTierTokenIds.length > 0) {
        cacheState.progressState.step = 'fetching_updated_tiers';
        cacheState.progressState.processedTiers = 0;
        cacheState.progressState.totalTiers = missingTierTokenIds.length;
        cacheState.progressState.progressPercentage = '50%';
        await saveCacheStateContract(contractKey, cacheState);
        logger.debug('holders', `Fetching tiers for ${missingTierTokenIds.length} updated or missing tokens`, chain, contractKey);

        const tierCalls = missingTierTokenIds.map(tokenId => ({
          address: contractAddress,
          abi,
          functionName: 'getNftTier',
          args: [BigInt(tokenId)],
        }));

        const tierResults = [];
        for (let i = 0; i < tierCalls.length; i += chunkSize) {
          const chunk = tierCalls.slice(i, i + chunkSize);
          try {
            const results = await retry(
              () => batchMulticall(chunk, config.alchemy.batchSize || 50),
              { retries: 3, delay: 1000, backoff: true }
            );
            tierResults.push(...results);
            cacheState.progressState.processedTiers = Math.min(i + chunkSize, tierCalls.length);
            cacheState.progressState.progressPercentage = `${Math.round(50 + (i / tierCalls.length) * 20)}%`;
            await saveCacheStateContract(contractKey, cacheState);
            logger.debug(
              'holders',
              `Processed updated tiers for ${cacheState.progressState.processedTiers}/${tierCalls.length} tokens`,
              chain,
              contractKey
            );
          } catch (error) {
            logger.error(
              'holders',
              `Failed to process tier chunk ${i / chunkSize + 1}: ${error.message}`,
              { stack: error.stack },
              chain,
              contractKey
            );
            errorLog.push({
              timestamp: new Date().toISOString(),
              phase: 'fetch_updated_tier',
              chunk: i / chunkSize + 1,
              error: error.message,
            });
            tierResults.push(...chunk.map(() => ({ status: 'failure', error: error.message })));
          }
        }

        tierResults.forEach((result, i) => {
          const tokenId = missingTierTokenIds[i];
          if (result.status === 'success') {
            const tier = Number(result.result) || 0;
            cachedTokenTiers.set(tokenId, { tier, timestamp: Date.now() });
            tierDistribution[tier] += 1;
          } else {
            errorLog.push({
              timestamp: new Date().toISOString(),
              phase: 'fetch_updated_tier',
              tokenId,
              error: result.error || 'unknown error',
            });
          }
        });
        await setCache(`${contractKey}_tiers`, Object.fromEntries(cachedTokenTiers), config.cache.nodeCache.stdTTL || 86400, contractKey);
        logger.debug('holders', `Saved ${cachedTokenTiers.size} tiers to cache for ${contractKey}`, chain, contractKey);
      }

      cacheState.progressState.totalNfts = totalTokens;
      cacheState.progressState.totalTiers = totalTokens;
      cacheState.progressState.totalLiveHolders = totalTokens;
      cacheState.globalMetrics = {
        totalMinted: totalTokens + totalBurned,
        totalLive: totalTokens,
        totalBurned,
        tierDistribution,
      };
      cacheState.progressState.isPopulating = false;
      cacheState.progressState.step = 'completed';
      cacheState.progressState.processedNfts = totalTokens;
      cacheState.progressState.processedTiers = missingTierTokenIds.length;
      cacheState.progressState.progressPercentage = '100%';
      cacheState.progressState.lastUpdated = Date.now();
      cacheState.progressState.lastBlockSynced = Number(lastBlock);
      await saveCacheStateContract(contractKey, cacheState);
      logger.debug('holders', `Progress state updated to completed for ${contractKey}`, chain, contractKey);

      const holderList = Array.from(holdersMap.values());
      holderList.sort((a, b) => b.multiplierSum - a.multiplierSum || b.total - a.total);
      holderList.forEach((holder, index) => {
        holder.rank = index + 1;
        holder.percentage = (holder.total / totalTokens * 100) || 0;
        holder.displayMultiplierSum = holder.multiplierSum;
      });

      logger.debug('holders', `Writing cache for ${contractKey}_holders, holders=${holderList.length}`, chain, contractKey);
      await setCache(`${contractKey}_holders`, { holders: holderList, totalBurned, timestamp: Date.now() }, 0, contractKey);
      logger.info(
        'holders',
        `Updated cached holders for ${contractKey}, lastBlock=${cacheState.lastProcessedBlock}, updatedTokens=${missingTierTokenIds.length}`,
        chain,
        contractKey
      );
      logger.info(
        'holders',
        `Completed getHoldersMap: holdersMap.size=${holdersMap.size}, totalTokens=${totalTokens}, totalBurned=${totalBurned}`,
        chain,
        contractKey
      );
      return { holdersMap, totalBurned, lastBlock: Number(lastBlock), errorLog, rarityDistribution };
    }
  }

  // Full rebuild for cache miss or forceUpdate
  cacheState.progressState.step = 'fetching_supply';
  cacheState.progressState.isPopulating = true;
  cacheState.progressState.progressPercentage = '10%';
  await saveCacheStateContract(contractKey, cacheState);
  logger.debug('holders', `Progress state updated to fetching_supply for ${contractKey}`, chain, contractKey);

  const isBurnContract = ['stax', 'element280', 'element369'].includes(contractKey);
  if (contractKey === 'ascendant') {
    try {
      logger.debug('holders', `Fetching ascendant metrics for ${contractKey}`, chain, contractKey);
      const [totalSharesRaw, toDistributeDay8Raw, toDistributeDay28Raw, toDistributeDay90Raw] = await retry(
        () =>
          Promise.all([
            client.readContract({ address: contractAddress, abi, functionName: 'totalShares' }),
            client.readContract({ address: contractAddress, abi, functionName: 'toDistribute', args: [0] }),
            client.readContract({ address: contractAddress, abi, functionName: 'toDistribute', args: [1] }),
            client.readContract({ address: contractAddress, abi, functionName: 'toDistribute', args: [2] }),
          ]),
        { retries: 3, delay: 1000, backoff: true }
      );
      totalShares = parseFloat(formatUnits(totalSharesRaw, 18));
      toDistributeDay8 = parseFloat(formatUnits(toDistributeDay8Raw, 18));
      toDistributeDay28 = parseFloat(formatUnits(toDistributeDay28Raw, 18));
      toDistributeDay90 = parseFloat(formatUnits(toDistributeDay90Raw, 18));
      logger.debug('holders', `Ascendant metrics: totalShares=${totalShares}, toDistributeDay8=${toDistributeDay8}`, chain, contractKey);
    } catch (error) {
      logger.error('holders', `Failed to fetch ascendant metrics: ${error.message}`, { stack: error.stack }, chain, contractKey);
      errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_ascendant_metrics', error: error.message });
      throw error;
    }
  } else {
    try {
      logger.debug('holders', `Fetching totalSupply and totalBurned for ${contractKey}`, chain, contractKey);
      const [totalSupply, burnedCount] = await retry(
        () =>
          Promise.all([
            client.readContract({ address: contractAddress, abi, functionName: 'totalSupply' }),
            client.readContract({ address: contractAddress, abi, functionName: 'totalBurned' }).catch(() => 0),
          ]),
        { retries: 3, delay: 1000, backoff: true }
      );
      totalTokens = Number(totalSupply);
      totalBurned = Number(burnedCount);
      logger.info('holders', `Contract state: totalSupply=${totalSupply}, totalBurned=${totalBurned}, totalLive=${totalTokens}`, chain, contractKey);
    } catch (error) {
      logger.error('holders', `Supply fetch error: ${error.message}`, { stack: error.stack }, chain, contractKey);
      errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_supply', error: error.message });
      throw error;
    }
  }

  cacheState.progressState.step = 'fetching_holders';
  cacheState.progressState.progressPercentage = '20%';
  await saveCacheStateContract(contractKey, cacheState);
  logger.debug('holders', `Progress state updated to fetching_holders for ${contractKey}`, chain, contractKey);

  try {
    logger.debug('holders', `Fetching owners for ${contractKey} via getOwnersForContract`, chain, contractKey);
    const owners = await retry(
      () => getOwnersForContract(contractAddress, abi, { withTokenBalances: true, maxPages: 100 }),
      { retries: 3, delay: 1000, backoff: true }
    );
    logger.info('holders', `Fetched ${owners.length} owners, filtering burn address`, chain, contractKey);

    const filteredOwners = owners.filter(
      owner => owner?.ownerAddress && owner.ownerAddress.toLowerCase() !== burnAddress.toLowerCase() && owner.tokenBalances?.length > 0
    );
    logger.info('holders', `Filtered ${filteredOwners.length} valid owners`, chain, contractKey);

    tokenOwnerMap.clear();
    totalTokens = 0;
    const seenTokenIds = new Set();

    filteredOwners.forEach(owner => {
      if (!owner.ownerAddress) return;
      let wallet;
      try {
        wallet = getAddress(owner.ownerAddress).toLowerCase();
      } catch (e) {
        logger.warn('holders', `Invalid wallet address: ${owner.ownerAddress}`, chain, contractKey);
        errorLog.push({
          timestamp: new Date().toISOString(),
          phase: 'process_owner',
          ownerAddress: owner.ownerAddress,
          error: 'Invalid wallet address',
        });
        return;
      }
      owner.tokenBalances.forEach(tb => {
        if (!tb.tokenId) return;
        const tokenId = Number(tb.tokenId);
        if (seenTokenIds.has(tokenId)) {
          logger.warn('holders', `Duplicate tokenId ${tokenId} for wallet ${wallet}`, chain, contractKey);
          errorLog.push({ timestamp: new Date().toISOString(), phase: 'process_token', tokenId, wallet, error: 'Duplicate tokenId' });
          return;
        }
        seenTokenIds.add(tokenId);
        tokenOwnerMap.set(tokenId, wallet);
        totalTokens++;
      });
    });
    logger.debug('holders', `Total tokens (Alchemy): ${totalTokens}, unique tokenIds: ${seenTokenIds.size}`, chain, contractKey);
  } catch (error) {
    logger.warn(
      'holders',
      `Failed to fetch owners via getOwnersForContract: ${error.message}, falling back to Transfer events`,
      chain,
      contractKey
    );
    errorLog.push({ timestamp: new Date().toISOString(), phase: 'fetch_owners_alchemy', error: error.message });

    const fromBlock = BigInt(config.nftContracts[contractKey].deploymentBlock || 0);
    tokenOwnerMap.clear();
    totalTokens = 0;
    const seenTokenIds = new Set();

    let currentFromBlock = fromBlock;
    const maxBlockRange = 200;
    while (currentFromBlock <= currentBlock) {
      const toBlock = BigInt(Math.min(Number(currentFromBlock) + maxBlockRange, Number(currentBlock)));
      try {
        const transferLogs = await retry(
          async () => {
            const logs = await client.getLogs({
              address: contractAddress,
              event: parseAbiItem('event Transfer(address indexed from, address indexed to, uint256 indexed tokenId)'),
              fromBlock: currentFromBlock,
              toBlock,
            });
            return logs;
          },
          { retries: 3, delay: 1000, backoff: true }
        );
        for (const log of transferLogs) {
          const from = log.args.from.toLowerCase();
          const to = log.args.to.toLowerCase();
          const tokenId = Number(log.args.tokenId);
          if (to === burnAddress.toLowerCase()) {
            totalBurned += 1;
            tokenOwnerMap.delete(tokenId);
            seenTokenIds.delete(tokenId);
            continue;
          }
          if (from === '0x0000000000000000000000000000000000000000') {
            if (!seenTokenIds.has(tokenId)) {
              tokenOwnerMap.set(tokenId, to);
              seenTokenIds.add(tokenId);
              totalTokens++;
            }
          } else {
            tokenOwnerMap.set(tokenId, to);
            seenTokenIds.add(tokenId);
          }
        }
        currentFromBlock = toBlock + 1n;
        lastBlock = toBlock;
        cacheState.lastProcessedBlock = Number(lastBlock);
        cacheState.progressState.lastProcessedBlock = Number(lastBlock);
        cacheState.progressState.lastUpdated = Date.now();
        await saveCacheStateContract(contractKey, cacheState);
        logger.debug('holders', `Processed transfer logs and updated lastProcessedBlock to ${lastBlock} for blocks ${currentFromBlock} to ${toBlock}`, chain, contractKey);
      } catch (error) {
        logger.error(
          'holders',
          `Failed to fetch transfer logs for blocks ${currentFromBlock} to ${toBlock}: ${error.message}`,
          { stack: error.stack },
          chain,
          contractKey
        );
        errorLog.push({
          timestamp: new Date().toISOString(),
          phase: 'fetch_transfer_logs',
          fromBlock: Number(currentFromBlock),
          toBlock: Number(toBlock),
          error: error.message,
        });
        currentFromBlock = toBlock + 1n;
        lastBlock = toBlock;
        cacheState.lastProcessedBlock = Number(lastBlock);
        cacheState.progressState.lastProcessedBlock = Number(lastBlock);
        cacheState.progressState.lastUpdated = Date.now();
        await saveCacheStateContract(contractKey, cacheState);
        logger.debug('holders', `Updated lastProcessedBlock to ${lastBlock} after error in transfer logs`, chain, contractKey);
        continue;
      }
    }
  }

  cacheState.progressState.totalNfts = totalTokens;
  cacheState.progressState.totalTiers = totalTokens;
  cacheState.progressState.totalLiveHolders = totalTokens;
  cacheState.progressState.progressPercentage = '30%';
  await saveCacheStateContract(contractKey, cacheState);
  logger.debug('holders', `Progress state updated to totalNfts=${totalTokens} for ${contractKey}`, chain, contractKey);

  if (totalTokens === 0) {
    logger.info('holders', `No live tokens found for ${contractKey}, writing empty holders`, chain, contractKey);
    cacheState.progressState.step = 'completed';
    cacheState.progressState.progressPercentage = '100%';
    cacheState.globalMetrics = {
      totalMinted: totalTokens + totalBurned,
      totalLive: totalTokens,
      totalBurned,
      tierDistribution,
      ...(contractKey === 'ascendant'
        ? {
            totalLockedAscendant: 0,
            totalShares: 0,
            toDistributeDay8: 0,
            toDistributeDay28: 0,
            toDistributeDay90: 0,
            pendingRewards: 0,
            rarityDistribution: Array(3).fill(0),
          }
        : {}),
    };
    await saveCacheStateContract(contractKey, cacheState);
    await setCache(`${contractKey}_holders`, { holders: [], totalBurned, timestamp: Date.now() }, 0, contractKey);
    await setCache(`${contractKey}_tiers`, {}, config.cache.nodeCache.stdTTL || 86400, contractKey);
    logger.info('holders', `Wrote empty holders to cache for ${contractKey}`, chain, contractKey);
    return { holdersMap, totalBurned, lastBlock: Number(lastBlock), errorLog, rarityDistribution };
  }

  cacheState.progressState.step = 'verifying_ownership';
  cacheState.progressState.progressPercentage = '40%';
  await saveCacheStateContract(contractKey, cacheState);
  logger.debug('holders', `Progress state updated to verifying_ownership for ${contractKey}`, chain, contractKey);

  const tokenIds = Array.from(tokenOwnerMap.keys());
  const ownershipCalls = tokenIds.map(tokenId => ({
    address: contractAddress,
    abi,
    functionName: 'ownerOf',
    args: [BigInt(tokenId)],
  }));

  const ownershipResults = [];
  const chunkSize = config.nftContracts[contractKey]?.maxTokensPerOwnerQuery || 200;
  const totalChunks = Math.ceil(ownershipCalls.length / chunkSize);
  for (let i = 0; i < ownershipCalls.length; i += chunkSize) {
    const chunk = ownershipCalls.slice(i, i + chunkSize);
    try {
      const results = await retry(
        () => batchMulticall(chunk, config.alchemy.batchSize || 50),
        { retries: 3, delay: 1000, backoff: true }
      );
      ownershipResults.push(...results);
      cacheState.progressState.progressPercentage = `${Math.round(40 + (i / ownershipCalls.length) * 10)}%`;
      await saveCacheStateContract(contractKey, cacheState);
      logger.debug(
        'holders',
        `Processed ownership chunk ${Math.floor(i / chunkSize) + 1}/${totalChunks} for ${chunk.length} tokens`,
        chain,
        contractKey
      );
    } catch (error) {
      logger.error(
        'holders',
        `Failed to process ownership chunk ${i / chunkSize + 1}: ${error.message}`,
        { stack: error.stack },
        chain,
        contractKey
      );
      errorLog.push({
        timestamp: new Date().toISOString(),
        phase: 'verify_ownership',
        chunk: i / chunkSize + 1,
        error: error.message,
      });
      ownershipResults.push(...chunk.map(() => ({ status: 'failure', error: error.message })));
    }
  }

  const validTokenIds = tokenIds.filter((tokenId, i) => {
    const result = ownershipResults[i];
    if (result.status === 'success' && result.result.toLowerCase() !== burnAddress.toLowerCase()) {
      return true;
    }
    logger.warn(
      'holders',
      `Skipping burned or invalid token ${tokenId} for ${contractKey}: owner=${result.result || 'unknown'}`,
      chain,
      contractKey
    );
    tokenOwnerMap.delete(tokenId);
    totalTokens -= 1;
    totalBurned += 1;
    return false;
  });

  logger.info(
    'holders',
    `Verified ownership for ${validTokenIds.length} tokens, excluded ${tokenIds.length - validTokenIds.length} burned/invalid tokens`,
    chain,
    contractKey
  );

  if (totalTokens === 0) {
    logger.info('holders', `No live tokens found for ${contractKey}, writing empty holders`, chain, contractKey);
    cacheState.progressState.step = 'completed';
    cacheState.progressState.progressPercentage = '100%';
    cacheState.globalMetrics = {
      totalMinted: totalTokens + totalBurned,
      totalLive: totalTokens,
      totalBurned,
      tierDistribution,
      ...(contractKey === 'ascendant'
        ? {
            totalLockedAscendant: 0,
            totalShares: 0,
            toDistributeDay8: 0,
            toDistributeDay28: 0,
            toDistributeDay90: 0,
            pendingRewards: 0,
            rarityDistribution: Array(3).fill(0),
          }
        : {}),
    };
    await saveCacheStateContract(contractKey, cacheState);
    await setCache(`${contractKey}_holders`, { holders: [], totalBurned, timestamp: Date.now() }, 0, contractKey);
    await setCache(`${contractKey}_tiers`, {}, config.cache.nodeCache.stdTTL || 86400, contractKey);
    logger.info('holders', `Wrote empty holders to cache for ${contractKey}`, chain, contractKey);
    return { holdersMap, totalBurned, lastBlock: Number(lastBlock), errorLog, rarityDistribution };
  }

  cacheState.progressState.step = 'fetching_records';
  cacheState.progressState.progressPercentage = '50%';
  await saveCacheStateContract(contractKey, cacheState);
  logger.debug('holders', `Progress state updated to fetching_records for ${contractKey}`, chain, contractKey);

  const recordCalls = contractKey === 'ascendant' ? validTokenIds.map(tokenId => ({
    address: contractAddress,
    abi,
    functionName: 'userRecords',
    args: [BigInt(tokenId)],
  })) : [];
  const recordResults = contractKey === 'ascendant' ? [] : validTokenIds.map(() => ({ status: 'success', result: [] }));
  if (contractKey === 'ascendant') {
    for (let i = 0; i < recordCalls.length; i += chunkSize) {
      const chunk = recordCalls.slice(i, i + chunkSize);
      try {
        const results = await retry(
          () => batchMulticall(chunk, config.alchemy.batchSize || 50),
          { retries: 3, delay: 1000, backoff: true }
        );
        recordResults.push(...results);
        cacheState.progressState.progressPercentage = `${Math.round(50 + (i / recordCalls.length) * 10)}%`;
        await saveCacheStateContract(contractKey, cacheState);
      } catch (error) {
        logger.error(
          'holders',
          `Failed to process record chunk ${i / chunkSize + 1}: ${error.message}`,
          { stack: error.stack },
          chain,
          contractKey
        );
        errorLog.push({
          timestamp: new Date().toISOString(),
          phase: 'fetch_records',
          chunk: i / chunkSize + 1,
          error: error.message,
        });
        recordResults.push(...chunk.map(() => ({ status: 'failure', error: error.message })));
      }
    }
  }

  cacheState.progressState.step = 'fetching_tiers';
  cacheState.progressState.processedTiers = 0;
  cacheState.progressState.totalTiers = validTokenIds.length;
  cacheState.progressState.progressPercentage = '60%';
  await saveCacheStateContract(contractKey, cacheState);
  logger.debug('holders', `Progress state updated to fetching_tiers for ${contractKey}`, chain, contractKey);

  if (['element280', 'stax', 'element369'].includes(contractKey)) {
    const cachedTiers = await getCache(`${contractKey}_tiers`, contractKey) || {};
    Object.entries(cachedTiers).forEach(([tokenId, tierData]) => {
      if (tierData && typeof tierData.tier === 'number') {
        cachedTokenTiers.set(Number(tokenId), tierData);
      }
    });
    logger.debug(
      'holders',
      `Cached tiers loaded: ${cachedTokenTiers.size}, missing tiers for ${validTokenIds.length - cachedTokenTiers.size} tokens`,
      chain,
      contractKey
    );
  }

  const missingTierTokenIds = ['element280', 'stax', 'element369'].includes(contractKey) ? validTokenIds.filter(tokenId => !cachedTokenTiers.has(tokenId)) : validTokenIds;
  const tierCalls = missingTierTokenIds.map(tokenId => ({
    address: contractAddress,
    abi,
    functionName: contractKey === 'ascendant' ? 'getNFTAttribute' : 'getNftTier',
    args: [BigInt(tokenId)],
  }));

  const tierResults = [];
  for (let i = 0; i < tierCalls.length; i += chunkSize) {
    const chunk = tierCalls.slice(i, i + chunkSize);
    try {
      const results = await retry(
        () => batchMulticall(chunk, config.alchemy.batchSize || 50),
        { retries: 3, delay: 1000, backoff: true }
      );
      tierResults.push(...results);
      cacheState.progressState.processedTiers = Math.min(i + chunkSize, missingTierTokenIds.length);
      cacheState.progressState.progressPercentage = `${Math.round(60 + (i / tierCalls.length) * 20)}%`;
      await saveCacheStateContract(contractKey, cacheState);
      logger.debug(
        'holders',
        `Processed tiers for ${cacheState.progressState.processedTiers}/${missingTierTokenIds.length} tokens`,
        chain,
        contractKey
      );
    } catch (error) {
      logger.error(
        'holders',
        `Failed to process tier chunk ${i / chunkSize + 1}: ${error.message}`,
        { stack: error.stack },
        chain,
        contractKey
      );
      errorLog.push({
        timestamp: new Date().toISOString(),
        phase: 'fetch_tier',
        chunk: i / chunkSize + 1,
        error: error.message,
      });
      tierResults.push(...chunk.map(() => ({ status: 'failure', error: error.message })));
    }
  }

  if (['element280', 'stax', 'element369'].includes(contractKey)) {
    tierResults.forEach((result, i) => {
      const tokenId = missingTierTokenIds[i];
      if (result.status === 'success') {
        const tier = Number(result.result) || 0;
        cachedTokenTiers.set(tokenId, { tier, timestamp: Date.now() });
      } else {
        errorLog.push({
          timestamp: new Date().toISOString(),
          phase: 'fetch_tier',
          tokenId,
          error: result.error || 'unknown error',
        });
      }
    });
    await setCache(`${contractKey}_tiers`, Object.fromEntries(cachedTokenTiers), config.cache.nodeCache.stdTTL || 86400, contractKey);
    logger.debug('holders', `Saved ${cachedTokenTiers.size} tiers to cache for ${contractKey}`, chain, contractKey);
  }

  const allTierResults = ['element280', 'stax', 'element369'].includes(contractKey) ? validTokenIds.map(tokenId => {
    if (cachedTokenTiers.has(tokenId)) {
      const tierData = cachedTokenTiers.get(tokenId);
      return { status: 'success', result: tierData.tier };
    }
    const index = missingTierTokenIds.indexOf(tokenId);
    return index >= 0 ? tierResults[index] : { status: 'failure', error: 'Missing tier data' };
  }) : tierResults;

  cacheState.progressState.step = 'fetching_rewards';
  cacheState.progressState.progressPercentage = '80%';
  await saveCacheStateContract(contractKey, cacheState);
  logger.debug('holders', `Progress state updated to fetching_rewards for ${contractKey}`, chain, contractKey);

  const rewardCalls = contractKey === 'ascendant' ? [
    {
      address: contractAddress,
      abi,
      functionName: 'batchClaimableAmount',
      args: [validTokenIds.map(id => BigInt(id))],
    },
    {
      address: contractAddress,
      abi,
      functionName: 'toDistribute',
      args: [0],
    },
    {
      address: contractAddress,
      abi,
      functionName: 'toDistribute',
      args: [1],
    },
    {
      address: contractAddress,
      abi,
      functionName: 'toDistribute',
      args: [2],
    },
    {
      address: contractAddress,
      abi,
      functionName: 'totalShares',
      args: [],
    },
  ] : [];

  const rewardResults = contractKey === 'ascendant' ? await retry(
    () => batchMulticall(rewardCalls, config.alchemy.batchSize || 50),
    { retries: 3, delay: 1000, backoff: true }
  ) : [];

  if (contractKey === 'ascendant') {
    if (rewardResults[0].status === 'success') {
      const claimable = parseFloat(formatUnits(rewardResults[0].result || 0, 18));
      holdersMap.forEach(holder => {
        holder.claimableRewards = claimable / totalTokens * holder.total;
      });
    }
    toDistributeDay8 = rewardResults[1].status === 'success' ? parseFloat(formatUnits(rewardResults[1].result || 0, 18)) : toDistributeDay8;
    toDistributeDay28 = rewardResults[2].status === 'success' ? parseFloat(formatUnits(rewardResults[2].result || 0, 18)) : toDistributeDay28;
    toDistributeDay90 = rewardResults[3].status === 'success' ? parseFloat(formatUnits(rewardResults[3].result || 0, 18)) : toDistributeDay90;
    totalShares = rewardResults[4].status === 'success' ? parseFloat(formatUnits(rewardResults[4].result || 0, 18)) : totalShares;
  }

  cacheState.progressState.step = 'building_holders';
  cacheState.progressState.progressPercentage = '90%';
  await saveCacheStateContract(contractKey, cacheState);
  logger.debug('holders', `Progress state updated to building_holders for ${contractKey}`, chain, contractKey);

  validTokenIds.forEach((tokenId, i) => {
    const wallet = tokenOwnerMap.get(tokenId);
    if (!wallet) {
      logger.warn('holders', `No owner found for token ${tokenId}`, chain, contractKey);
      errorLog.push({ timestamp: new Date().toISOString(), phase: 'process_token', tokenId, error: 'No owner found' });
      return;
    }

    let shares = 0;
    let lockedAscendant = 0;
    if (contractKey === 'ascendant') {
      const recordResult = recordResults[i];
      if (recordResult.status === 'success' && Array.isArray(recordResult.result)) {
        shares = parseFloat(formatUnits(recordResult.result[0] || 0, 18));
        lockedAscendant = parseFloat(formatUnits(recordResult.result[1] || 0, 18));
        totalLockedAscendant += lockedAscendant;
      } else {
        logger.error(
          'holders',
          `Failed to fetch userRecords for token ${tokenId}: ${recordResult.error || 'unknown error'}`,
          chain,
          contractKey
        );
        errorLog.push({
          timestamp: new Date().toISOString(),
          phase: 'fetch_records',
          tokenId,
          wallet,
          error: recordResult.error || 'unknown error',
        });
        return;
      }
    }

    let tier = 0;
    let rarityNumber = 0;
    let rarity = 0;
    const tierResult = allTierResults[i];
    logger.debug('holders', `Raw tierResult for token ${tokenId}: status=${tierResult.status}, result=${safeStringify(tierResult.result)}`, chain, contractKey);

    if (tierResult.status === 'success') {
      if (contractKey === 'ascendant') {
        const result = tierResult.result;
        let parsedResult;
        if (Array.isArray(result) && result.length >= 3) {
          parsedResult = {
            rarityNumber: Number(result[0]) || 0,
            tier: Number(result[1]) || 0,
            rarity: Number(result[2]) || 0,
          };
        } else if (typeof result === 'object' && result !== null && 'rarityNumber' in result) {
          parsedResult = {
            rarityNumber: Number(result.rarityNumber) || 0,
            tier: Number(result.tier) || 0,
            rarity: Number(result.rarity) || 0,
          };
        } else {
          logger.warn(
            'holders',
            `Invalid getNFTAttribute result for token ${tokenId}: result=${safeStringify(result)}`,
            chain,
            contractKey
          );
          errorLog.push({
            timestamp: new Date().toISOString(),
            phase: 'fetch_tier',
            tokenId,
            wallet,
            error: `Invalid getNFTAttribute result: ${safeStringify(result)}`,
          });
          return;
        }
        rarityNumber = parsedResult.rarityNumber;
        tier = parsedResult.tier;
        rarity = parsedResult.rarity;
        logger.debug(
          'holders',
          `Parsed attributes for token ${tokenId} (ascendant): tier=${tier}, rarityNumber=${rarityNumber}, rarity=${rarity}`,
          chain,
          contractKey
        );
      } else {
        tier = typeof tierResult.result === 'bigint' ? Number(tierResult.result) : Number(tierResult.result) || 0;
      }

      if (isNaN(tier) || tier < 0 || tier > maxTier) {
        logger.warn(
          'holders',
          `Invalid tier for token ${tokenId} in ${contractKey}: tier=${tier}, maxTier=${maxTier}, defaulting to 0`,
          chain,
          contractKey
        );
        errorLog.push({
          timestamp: new Date().toISOString(),
          phase: 'fetch_tier',
          tokenId,
          wallet,
          error: `Invalid tier ${tier}`,
          details: { rawResult: safeStringify(tierResult.result), maxTier, parsedTier: tier },
        });
        tier = 0;
      }
    } else {
      logger.error(
        'holders',
        `Failed to fetch tier for token ${tokenId}: ${tierResult.error || 'unknown error'}`,
        chain,
        contractKey
      );
      errorLog.push({
        timestamp: new Date().toISOString(),
        phase: 'fetch_tier',
        tokenId,
        wallet,
        error: tierResult.error || 'unknown error',
        details: { rawResult: safeStringify(tierResult.result) },
      });
      return;
    }

    if (contractKey === 'ascendant' && rarity >= 0 && rarity < rarityDistribution.length) {
      rarityDistribution[rarity] += 1;
    } else if (contractKey === 'ascendant') {
      logger.warn(
        'holders',
        `Invalid rarity for token ${tokenId}: rarity=${rarity}, maxRarity=${rarityDistribution.length - 1}`,
        chain,
        contractKey
      );
      errorLog.push({
        timestamp: new Date().toISOString(),
        phase: 'fetch_rarity',
        tokenId,
        wallet,
        error: `Invalid rarity ${rarity}`,
      });
    }

    const holder =
      holdersMap.get(wallet) ||
      {
        wallet,
        tokenIds: [],
        tiers: Array(maxTier + 1).fill(0),
        total: 0,
        multiplierSum: 0,
        ...(contractKey === 'element369' ? { infernoRewards: 0, fluxRewards: 0, e280Rewards: 0 } : {}),
        ...(contractKey === 'element280' || contractKey === 'stax' ? { claimableRewards: 0 } : {}),
        ...(contractKey === 'ascendant'
          ? {
              shares: 0,
              lockedAscendant: 0,
              pendingDay8: toDistributeDay8 / totalTokens * 8 / 100,
              pendingDay28: toDistributeDay28 / totalTokens * 28 / 100,
              pendingDay90: toDistributeDay90 / totalTokens * 90 / 100,
              claimableRewards: 0,
              tokens: [],
            }
          : {}),
      };

    if (holder.tokenIds.includes(tokenId)) {
      logger.warn('holders', `Duplicate tokenId ${tokenId} for wallet ${wallet} in holdersMap`, chain, contractKey);
      errorLog.push({
        timestamp: new Date().toISOString(),
        phase: 'build_holders',
        tokenId,
        wallet,
        error: 'Duplicate tokenId in holdersMap',
      });
      return;
    }

    holder.tokenIds.push(tokenId);
    holder.total += 1;
    holder.tiers[tier] += 1;
    holder.multiplierSum += contractTiers[tier + 1]?.multiplier || tier + 1;
    if (contractKey === 'ascendant') {
      holder.shares += shares;
      holder.lockedAscendant += lockedAscendant;
      holder.tokens.push({
        tokenId: Number(tokenId),
        tier: tier + 1,
        rawTier: tier,
        rarityNumber,
        rarity,
      });
    }
    holdersMap.set(wallet, holder);
    tierDistribution[tier] += 1;
  });

  cacheState.progressState.step = 'finalizing';
  cacheState.progressState.progressPercentage = '90%';
  await saveCacheStateContract(contractKey, cacheState);
  logger.debug('holders', `Progress state updated to finalizing for ${contractKey}`, chain, contractKey);

  const totalLiveHolders = holdersMap.size;
  cacheState.progressState.totalOwners = totalLiveHolders;
  let holderList = Array.from(holdersMap.values());
  holderList.forEach((holder, index) => {
    holder.rank = index + 1;
    holder.percentage = (holder.total / totalTokens * 100) || 0;
    holder.displayMultiplierSum = holder.multiplierSum;
  });

  holderList.sort((a, b) => {
    if (contractKey === 'ascendant') {
      return b.shares - a.shares || b.total - a.total;
    }
    return b.total - a.total || b.multiplierSum - a.multiplierSum;
  });
  holderList.forEach((holder, index) => {
    holder.rank = index + 1;
  });

  cacheState.globalMetrics = {
    totalMinted: totalTokens + totalBurned,
    totalLive: totalTokens,
    totalBurned,
    tierDistribution,
    ...(contractKey === 'ascendant'
      ? {
          totalLockedAscendant,
          totalShares,
          toDistributeDay8,
          toDistributeDay28,
          toDistributeDay90,
          pendingRewards: toDistributeDay8 + toDistributeDay28 + toDistributeDay90,
          rarityDistribution,
        }
      : {}),
  };
  cacheState.progressState.isPopulating = false;
  cacheState.progressState.step = 'completed';
  cacheState.progressState.processedNfts = totalTokens;
  cacheState.progressState.processedTiers = validTokenIds.length;
  cacheState.progressState.progressPercentage = '100%';
  cacheState.progressState.lastUpdated = Date.now();
  await saveCacheStateContract(contractKey, cacheState);
  logger.debug('holders', `Progress state updated to completed for ${contractKey}, totalOwners=${totalLiveHolders}`, chain, contractKey);

  logger.debug('holders', `Writing cache for ${contractKey}_holders, holders=${holderList.length}`, chain, contractKey);
  await setCache(
    `${contractKey}_holders`,
    { holders: holderList, totalBurned, timestamp: Date.now(), rarityDistribution },
    0,
    contractKey
  );
  if (['element280', 'stax', 'element369'].includes(contractKey)) {
    await setCache(`${contractKey}_tiers`, Object.fromEntries(cachedTokenTiers), config.cache.nodeCache.stdTTL || 86400, contractKey);
  }
  logger.info(
    'holders',
    `Completed holders map with ${holderList.length} holders, totalBurned=${totalBurned}, cachedTiers=${cachedTokenTiers.size}`,
    chain,
    contractKey
  );
  logger.debug('holders', `Tier distribution for ${contractKey}: ${tierDistribution}`, chain, contractKey);
  if (contractKey === 'ascendant') {
    logger.debug('holders', `Rarity distribution for ${contractKey}: ${rarityDistribution}`, chain, contractKey);
  }

  logger.info(
    'holders',
    `Completed getHoldersMap: holdersMap.size=${holdersMap.size}, totalTokens=${totalTokens}, totalBurned=${totalBurned}`,
    chain,
    contractKey
  );
  return { holdersMap, totalBurned, lastBlock: Number(lastBlock), errorLog, rarityDistribution };
}

export async function populateHoldersMapCache(contractKey, contractAddress, abi, vaultAddress, vaultAbi, forceUpdate = false) {
  let cacheState;
  const chain = config.nftContracts[contractKey.toLowerCase()]?.chain || 'eth';
  try {
    logger.debug('holders', `Starting populateHoldersMapCache for ${contractKey}, forceUpdate=${forceUpdate}, cwd=${process.cwd()}`, chain, contractKey);
    await ensureCacheDirectory();
    cacheState = await getCacheState(contractKey.toLowerCase());
    logger.debug('holders', `Cache state loaded for ${contractKey}: ${safeStringify(cacheState)}`, chain, contractKey);

    // Check for stale isPopulating flag
    const isStale = cacheState.isPopulating && (
      !cacheState.progressState.lastUpdated ||
      (Date.now() - cacheState.progressState.lastUpdated > 10 * 60 * 1000) || // 10 minutes
      (cacheState.progressState.step === 'starting' && cacheState.progressState.progressPercentage === '0%')
    );
    if (isStale) {
      logger.warn('holders', `Detected stale isPopulating flag for ${contractKey}, resetting`, chain, contractKey);
      cacheState.isPopulating = false;
      cacheState.progressState.step = 'initializing';
      cacheState.progressState.error = 'Reset due to stale state';
      await saveCacheStateContract(contractKey.toLowerCase(), cacheState);
    }

    if (!forceUpdate && cacheState.isPopulating) {
      logger.info('holders', `Cache population already in progress for ${contractKey}`, chain, contractKey);
      return { status: 'pending', holders: [] };
    }

    cacheState.isPopulating = true;
    cacheState.progressState.step = 'initializing';
    cacheState.progressState.progressPercentage = '0%';
    await saveCacheStateContract(contractKey.toLowerCase(), cacheState);
    logger.debug('holders', `Progress state updated to initializing for ${contractKey}`, chain, contractKey);

    // Validate contract
    logger.debug('holders', `Calling validateContract for ${contractKey}`, chain, contractKey);
    const isValid = await validateContract(contractKey);
    if (!isValid) {
      throw new Error(`Invalid contract configuration for ${contractKey}`);
    }

    logger.debug('holders', `Calling getHoldersMap for ${contractKey}`, chain, contractKey);
    const { holdersMap, totalBurned, lastBlock, errorLog } = await getHoldersMap(
      contractKey,
      contractAddress,
      abi,
      vaultAddress,
      vaultAbi,
      cacheState,
      forceUpdate
    );
    logger.debug('holders', `getHoldersMap completed for ${contractKey}, holders=${holdersMap.size}, totalBurned=${totalBurned}`, chain, contractKey);

    const holderList = [];
    for (const [wallet, data] of holdersMap) {
      holderList.push({
        wallet,
        total: data.total,
        tokenIds: data.tokenIds,
        tiers: data.tiers,
        multiplierSum: data.multiplierSum,
        shares: data.shares || 0,
        lockedAscendant: data.lockedAscendant || 0,
        claimableRewards: data.claimableRewards || 0,
        pendingDay8: data.pendingDay8 || 0,
        pendingDay28: data.pendingDay28 || 0,
        pendingDay90: data.pendingDay90 || 0,
        infernoRewards: data.infernoRewards || 0,
        fluxRewards: data.fluxRewards || 0,
        e280Rewards: data.e280Rewards || 0,
      });
    }

    cacheState.isPopulating = false;
    cacheState.progressState.step = 'completed';
    cacheState.progressState.progressPercentage = '100%';
    cacheState.progressState.lastUpdated = Date.now();
    await saveCacheStateContract(contractKey.toLowerCase(), cacheState);
    logger.info('holders', `Cache population completed for ${contractKey}, holders=${holderList.length}`, chain, contractKey);

    return { status: 'completed', holders: holderList, totalBurned, lastBlock, errorLog };
  } catch (error) {
    logger.error('holders', `Failed to populate holders map for ${contractKey}: ${error.message}`, { stack: error.stack }, chain, contractKey);
    cacheState = cacheState || (await getCacheState(contractKey.toLowerCase()));
    cacheState.isPopulating = false;
    cacheState.progressState.step = 'failed';
    cacheState.progressState.error = error.message;
    cacheState.progressState.lastUpdated = Date.now();
    await saveCacheStateContract(contractKey.toLowerCase(), cacheState);
    throw error;
  }
}

export { sanitizeBigInt };
----- app/api/holders/cache/state.js -----

import { logger } from '@/app/lib/logger';
import { loadCacheState, saveCacheState } from '@/app/api/utils/cache';
import fs from 'fs/promises';
import path from 'path';
import Redis from 'ioredis';

// Cache directory for filesystem storage
const cacheDir = path.join(process.cwd(), 'cache');

// Redis client initialization
const redisEnabled = !!process.env.UPSTASH_REDIS_REST_URL;
let redis = null;
if (redisEnabled) {
  try {
    redis = new Redis(process.env.UPSTASH_REDIS_REST_URL);
    logger.info('cache', 'Redis client initialized successfully', 'eth', 'general');
  } catch (error) {
    logger.error('cache', `Failed to initialize Redis client: ${error.message}`, { stack: error.stack }, 'eth', 'general');
    redis = null;
  }
}

// Ensure cache directory exists
async function ensureCacheDir() {
  try {
    await fs.mkdir(cacheDir, { recursive: true });
    await fs.chmod(cacheDir, 0o755);
    logger.debug('cache/file', `Ensured cache directory exists: ${cacheDir}`, 'eth', 'general');
  } catch (error) {
    logger.error('cache/file', `Failed to create cache directory ${cacheDir}: ${error.message}`, { stack: error.stack }, 'eth', 'general');
    throw error;
  }
}

// Get cache state for a contract
export async function getCacheState(contractKey) {
  const cacheState = {
    isPopulating: false,
    totalOwners: 0,
    totalLiveHolders: 0,
    progressState: {
      step: 'idle',
      processedNfts: 0,
      totalNfts: 0,
      processedTiers: 0,
      totalTiers: 0,
      error: null,
      errorLog: [],
      progressPercentage: '0%',
      totalLiveHolders: 0,
      totalOwners: 0,
      lastProcessedBlock: null,
      lastUpdated: null,
    },
    lastUpdated: null,
    lastProcessedBlock: null,
    globalMetrics: {},
  };
  try {
    logger.debug('cache/state', `Loading cache state for ${contractKey}`, 'eth', contractKey);
    const savedState = await loadCacheState(contractKey, contractKey.toLowerCase());
    if (savedState && typeof savedState === 'object') {
      cacheState.isPopulating = savedState.isPopulating ?? false;
      cacheState.totalOwners = savedState.totalOwners ?? 0;
      cacheState.totalLiveHolders = savedState.totalLiveHolders ?? 0;
      cacheState.progressState = {
        ...cacheState.progressState,
        ...savedState.progressState,
      };
      cacheState.lastUpdated = savedState.lastUpdated ?? null;
      cacheState.lastProcessedBlock = savedState.lastProcessedBlock ?? null;
      cacheState.globalMetrics = savedState.globalMetrics ?? {};
      logger.info(
        'cache/state',
        `Loaded cache state for ${contractKey}: totalOwners=${cacheState.totalOwners}, step=${cacheState.progressState.step}`,
        'eth',
        contractKey
      );
    } else {
      logger.warn('cache/state', `No valid cache state found for ${contractKey}, returning default`, 'eth', contractKey);
    }
  } catch (error) {
    logger.error(
      'cache/state',
      `Failed to load cache state for ${contractKey}: ${error.message}`,
      { stack: error.stack },
      'eth',
      contractKey
    );
    cacheState.progressState.error = `Failed to load cache state: ${error.message}`;
    cacheState.progressState.errorLog.push({
      timestamp: new Date().toISOString(),
      phase: 'load_cache_state',
      error: error.message,
    });
  }
  return cacheState;
}

// Save cache state for a contract
export async function saveCacheStateContract(contractKey, cacheState) {
  try {
    const updatedState = {
      ...cacheState,
      lastProcessedBlock: cacheState.progressState.lastProcessedBlock ?? cacheState.lastProcessedBlock,
      progressState: {
        ...cacheState.progressState,
        lastProcessedBlock: cacheState.progressState.lastProcessedBlock ?? cacheState.lastProcessedBlock,
      },
    };
    logger.debug(
      'cache/state',
      `Saving cache state for ${contractKey}: totalOwners=${updatedState.totalOwners}, step=${updatedState.progressState.step}`,
      'eth',
      contractKey
    );
    await saveCacheState(contractKey, updatedState, contractKey.toLowerCase());
    logger.info(
      'cache/state',
      `Saved cache state for ${contractKey}: totalOwners=${updatedState.totalOwners}, step=${updatedState.progressState.step}`,
      'eth',
      contractKey
    );
  } catch (error) {
    logger.error(
      'cache/state',
      `Failed to save cache state for ${contractKey}: ${error.message}`,
      { stack: error.stack },
      'eth',
      contractKey
    );
    throw error;
  }
}

// Get cache data
export async function getCache(key, prefix) {
  const chain = 'eth';
  const cacheKey = `${prefix.toLowerCase()}_${key}`;
  logger.debug('cache', `Attempting to get cache for key=${cacheKey}`, chain, prefix.toLowerCase());

  // Try Redis first if enabled
  if (redisEnabled && redis && process.env[`DISABLE_${prefix.toUpperCase()}_REDIS`] !== 'true') {
    try {
      const redisData = await redis.get(cacheKey);
      if (redisData) {
        logger.info('cache', `Retrieved ${cacheKey} from Redis`, chain, prefix.toLowerCase());
        return JSON.parse(redisData);
      }
      logger.debug('cache', `No data found in Redis for ${cacheKey}, checking filesystem`, chain, prefix.toLowerCase());
    } catch (error) {
      logger.error(
        'cache',
        `Failed to get ${cacheKey} from Redis: ${error.message}`,
        { stack: error.stack },
        chain,
        prefix.toLowerCase()
      );
    }
  }

  // Fallback to filesystem
  const cacheFile = path.join(cacheDir, `${prefix.toLowerCase()}_${key}.json`);
  try {
    const data = await fs.readFile(cacheFile, 'utf-8');
    logger.info('cache/file', `Retrieved ${cacheKey} from ${cacheFile}`, chain, prefix.toLowerCase());
    return JSON.parse(data);
  } catch (error) {
    if (error.code === 'ENOENT') {
      logger.debug('cache/file', `Cache file ${cacheFile} not found`, chain, prefix.toLowerCase());
    } else {
      logger.error(
        'cache/file',
        `Failed to read cache file ${cacheFile}: ${error.message}`,
        { stack: error.stack },
        chain,
        prefix.toLowerCase()
      );
    }
    return null;
  }
}

// Set cache data
export async function setCache(key, value, ttl, prefix) {
  const chain = 'eth';
  const cacheKey = `${prefix.toLowerCase()}_${key}`;
  logger.debug('cache', `Setting cache for key=${cacheKey}, ttl=${ttl}`, chain, prefix.toLowerCase());

  // Handle holders specifically
  if (key.endsWith('_holders')) {
    const holdersCount = value.holders ? value.holders.length : 0;
    logger.info('cache', `Persisting ${cacheKey} with ${holdersCount} holders`, chain, prefix.toLowerCase());

    // Write to Redis if enabled
    if (redisEnabled && redis && process.env[`DISABLE_${prefix.toUpperCase()}_REDIS`] !== 'true') {
      try {
        await redis.set(cacheKey, JSON.stringify(value), 'EX', ttl || 86400);
        logger.info('cache', `Persisted ${cacheKey} to Redis with ${holdersCount} holders`, chain, prefix.toLowerCase());
      } catch (error) {
        logger.error(
          'cache',
          `Failed to persist ${cacheKey} to Redis: ${error.message}`,
          { stack: error.stack },
          chain,
          prefix.toLowerCase()
        );
      }
    }

    // Always write to filesystem
    const cacheFile = path.join(cacheDir, `${prefix.toLowerCase()}_holders.json`);
    try {
      await ensureCacheDir();
      logger.debug('cache/file', `Writing ${cacheKey} to ${cacheFile}`, chain, prefix.toLowerCase());
      await fs.writeFile(cacheFile, JSON.stringify(value, null, 2));
      await fs.chmod(cacheFile, 0o644);
      logger.info('cache/file', `Persisted ${cacheKey} to ${cacheFile} with ${holdersCount} holders`, chain, prefix.toLowerCase());
    } catch (error) {
      logger.error(
        'cache/file',
        `Failed to write cache file ${cacheFile}: ${error.message}`,
        { stack: error.stack },
        chain,
        prefix.toLowerCase()
      );
      throw error;
    }
    return;
  }

  // Handle other cache keys
  if (redisEnabled && redis && process.env[`DISABLE_${prefix.toUpperCase()}_REDIS`] !== 'true') {
    try {
      await redis.set(cacheKey, JSON.stringify(value), 'EX', ttl || 86400);
      logger.info('cache', `Persisted ${cacheKey} to Redis`, chain, prefix.toLowerCase());
    } catch (error) {
      logger.error(
        'cache',
        `Failed to persist ${cacheKey} to Redis: ${error.message}`,
        { stack: error.stack },
        chain,
        prefix.toLowerCase()
      );
    }
  } else {
    const cacheFile = path.join(cacheDir, `${prefix.toLowerCase()}_${key}.json`);
    try {
      await ensureCacheDir();
      logger.debug('cache/file', `Writing ${cacheKey} to ${cacheFile}`, chain, prefix.toLowerCase());
      await fs.writeFile(cacheFile, JSON.stringify(value, null, 2));
      await fs.chmod(cacheFile, 0o644);
      logger.info('cache/file', `Persisted ${cacheKey} to ${cacheFile}`, chain, prefix.toLowerCase());
    } catch (error) {
      logger.error(
        'cache/file',
        `Failed to write cache file ${cacheFile}: ${error.message}`,
        { stack: error.stack },
        chain,
        prefix.toLowerCase()
      );
      throw error;
    }
  }
}

// Test cache write (for debugging)
export async function testCacheWrite() {
  const testFile = path.join(cacheDir, 'test.json');
  const testData = { test: 'data', timestamp: Date.now() };
  try {
    await ensureCacheDir();
    logger.debug('cache/file', `Testing write to ${testFile}`, 'eth', 'general');
    await fs.writeFile(testFile, JSON.stringify(testData, null, 2));
    await fs.chmod(testFile, 0o644);
    logger.info('cache/file', `Test write to ${testFile} succeeded`, 'eth', 'general');
    return true;
  } catch (error) {
    logger.error(
      'cache/file',
      `Test write to ${testFile} failed: ${error.message}`,
      { stack: error.stack },
      'eth',
      'general'
    );
    return false;
  }
}
----- app/api/utils.js -----

// app/api/utils.js
import { createPublicClient, http, parseAbi } from 'viem';
import { mainnet } from 'viem/chains';
import { Alchemy, Network } from 'alchemy-sdk';

// Shared cache for routes that import it
export const cache = {};

// Import all ABI JSON files using @ notation
import staxNFTAbi from '@/abi/staxNFT.json';
import element369Abi from '@/abi/element369.json';
import element369VaultAbi from '@/abi/element369Vault.json';
import staxVaultAbi from '@/abi/staxVault.json';
import ascendantNFTAbi from '@/abi/ascendantNFT.json';
import element280Abi from '@/abi/element280.json';
import element280VaultAbi from '@/abi/element280Vault.json';

export const alchemy = new Alchemy({
  apiKey: process.env.NEXT_PUBLIC_ALCHEMY_API_KEY || (() => { throw new Error('Alchemy API key missing'); })(),
  network: Network.ETH_MAINNET,
});

export const client = createPublicClient({
  chain: mainnet,
  transport: http(
    process.env.ETH_RPC_URL ||
    `https://eth-mainnet.g.alchemy.com/v2/${process.env.NEXT_PUBLIC_ALCHEMY_API_KEY}`
  ),
});

// Generic NFT ABI for common functions
export const nftAbi = parseAbi([
  'function ownerOf(uint256 tokenId) view returns (address)',
  'function getNftTier(uint256 tokenId) view returns (uint8)',
]);

// Ascendant NFT ABI with specific functions
export const ascendantAbi = parseAbi([
  'function ownerOf(uint256 tokenId) view returns (address)',
  'function getNFTAttribute(uint256 tokenId) view returns (uint256 rarityNumber, uint8 tier, uint8 rarity)',
  'function userRecords(uint256 tokenId) view returns (uint256 shares, uint256 lockedAscendant, uint256 rewardDebt, uint32 startTime, uint32 endTime)',
  'function totalShares() view returns (uint256)',
  'function toDistribute(uint8 pool) view returns (uint256)',
  'function rewardPerShare() view returns (uint256)',
  'error NonExistentToken(uint256 tokenId)',
]);

// Export all ABIs
export {
  staxNFTAbi,
  element369Abi,
  element369VaultAbi,
  staxVaultAbi,
  ascendantNFTAbi,
  element280Abi,
  element280VaultAbi,
};

export const CACHE_TTL = 5 * 60 * 1000; // 5 minutes

export function log(message) {
  console.log(`[PROD_DEBUG] ${message}`);
}

export async function batchMulticall(calls, batchSize = 50) {
  log(`batchMulticall: Processing ${calls.length} calls in batches of ${batchSize}`);
  const results = [];
  for (let i = 0; i < calls.length; i += batchSize) {
    const batch = calls.slice(i, i + batchSize);
    try {
      const batchResults = await client.multicall({ contracts: batch });
      results.push(...batchResults);
      log(`batchMulticall: Batch ${i}-${i + batchSize - 1} completed with ${batchResults.length} results`);
    } catch (error) {
      console.error(`[PROD_ERROR] batchMulticall failed for batch ${i}-${i + batchSize - 1}: ${error.message}`);
      results.push(...batch.map(() => ({ status: 'failure', result: null })));
    }
  }
  log(`batchMulticall: Completed with ${results.length} results`);
  return results;
}
----- app/api/utils/cache.js -----

import NodeCache from 'node-cache';
import fs from 'fs/promises';
import path from 'path';
import { Redis } from '@upstash/redis';
import config from '@/contracts/config';
import { getAddress } from 'viem';
import { logger } from '@/app/lib/logger';
import { client } from '@/app/api/utils/client.js';

// Log config.nftContracts at startup
logger.info(
  'cache',
  `Loaded config.nftContracts: keys=${Object.keys(config.nftContracts).join(', ')}`,
  'general',
  'general'
);

const cache = new NodeCache({
  stdTTL: 0,
  checkperiod: 120,
});

const cacheDir = path.join(process.cwd(), 'cache');
const redisEnabled = Object.keys(config.nftContracts).some(
  contract => process.env[`DISABLE_${contract.toUpperCase()}_REDIS`] !== 'true' && process.env.UPSTASH_REDIS_REST_URL && process.env.UPSTASH_REDIS_REST_TOKEN
);
let redis = null;

if (redisEnabled) {
  try {
    redis = new Redis({
      url: process.env.UPSTASH_REDIS_REST_URL,
      token: process.env.UPSTASH_REDIS_REST_TOKEN,
    });
    logger.info('cache', 'Upstash Redis initialized', 'general', 'general');
  } catch (error) {
    logger.error('cache', `Failed to initialize Upstash Redis: ${error.message}`, { stack: error.stack }, 'general', 'general');
    redis = null;
  }
}

// Log Redis status
logger.info(
  'cache',
  `Redis enabled: ${redisEnabled}, contracts with disabled Redis: ${Object.keys(config.nftContracts).filter(c => process.env[`DISABLE_${c.toUpperCase()}_REDIS`] === 'true').join(', ')}`,
  'general',
  'general'
);

async function ensureCacheDir(collectionKey = 'general') {
  const chain = config.nftContracts[collectionKey.toLowerCase()]?.chain || 'eth';
  const collection = collectionKey.toLowerCase();
  try {
    logger.debug('cache', `Ensuring cache directory at: ${cacheDir}`, chain, collection);
    await fs.mkdir(cacheDir, { recursive: true });
    await fs.chmod(cacheDir, 0o755);
    logger.info('cache', `Created/chmod cache directory: ${cacheDir}`, chain, collection);
  } catch (error) {
    logger.error('cache', `Failed to create/chmod cache directory ${cacheDir}: ${error.message}`, { stack: error.stack }, chain, collection);
    throw error;
  }
}

export async function initializeCache() {
  const collection = 'general';
  const chain = 'general'; // General context for initialization
  try {
    logger.info('cache', `Starting cache initialization, contracts=${Object.keys(config.nftContracts).join(', ')}`, chain, collection);
    await ensureCacheDir();

    const testKey = 'test_node_cache';
    const testValue = { ready: true };
    const nodeCacheSuccess = cache.set(testKey, testValue);
    if (nodeCacheSuccess) {
      logger.info('cache', 'Node-cache is ready', chain, collection);
      cache.del(testKey);
    } else {
      logger.error('cache', 'Node-cache failed to set test key', {}, chain, collection);
    }

    if (redisEnabled && redis) {
      try {
        await redis.set('test_redis', JSON.stringify(testValue));
        const redisData = await redis.get('test_redis');
        if (redisData && JSON.parse(redisData).ready) {
          logger.info('cache', 'Redis cache is ready', chain, collection);
          await redis.del('test_redis');
        } else {
          logger.error('cache', 'Redis cache test failed: invalid data', {}, chain, collection);
        }
      } catch (error) {
        logger.error('cache', `Redis cache test failed: ${error.message}`, { stack: error.stack }, chain, collection);
      }
    }

    const collections = Object.keys(config.nftContracts)
      .filter(key => !config.nftContracts[key].disabled)
      .map(key => key.toLowerCase());
    for (const collectionKey of collections) {
      const chainKey = config.nftContracts[collectionKey]?.chain || 'eth';
      const cacheFile = path.join(cacheDir, `${collectionKey}_holders.json`);
      logger.debug('cache', `Checking cache file for ${collectionKey}: ${cacheFile}`, chainKey, collectionKey);
      try {
        await fs.access(cacheFile);
        logger.info('cache', `Cache file exists: ${cacheFile}`, chainKey, collectionKey);
      } catch (error) {
        if (error.code === 'ENOENT') {
          await ensureCacheDir(collectionKey);
          await fs.writeFile(cacheFile, JSON.stringify({ holders: [], totalBurned: 0, timestamp: Date.now() }));
          await fs.chmod(cacheFile, 0o644);
          logger.info('cache', `Created empty cache file: ${cacheFile}`, chainKey, collectionKey);
        } else {
          logger.error('cache', `Failed to access cache file ${cacheFile}: ${error.message}`, { stack: error.stack }, chainKey, collectionKey);
        }
      }
    }

    logger.info('cache', 'Cache initialization completed', chain, collection);
    return true;
  } catch (error) {
    logger.error('cache', `Cache initialization error: ${error.message}`, { stack: error.stack }, chain, collection);
    return false;
  }
}

export async function getCache(key, prefix) {
  const chain = config.nftContracts[prefix.toLowerCase()]?.chain || 'eth';
  try {
    const cacheKey = `${prefix}_${key}`;
    let data = cache.get(cacheKey);
    if (data !== undefined) {
      logger.debug(
        'cache',
        `Cache hit: ${cacheKey}, holders: ${data.holders?.length || 'unknown'}`,
        chain,
        prefix.toLowerCase()
      );
      return data;
    }

    if (key === `${prefix.toLowerCase()}_holders` && Object.keys(config.nftContracts).map(k => k.toLowerCase()).includes(prefix.toLowerCase())) {
      if (redisEnabled && redis && process.env[`DISABLE_${prefix.toUpperCase()}_REDIS`] !== 'true') {
        try {
          logger.debug('cache', `Attempting to load ${cacheKey} from Redis`, chain, prefix.toLowerCase());
          const redisData = await redis.get(cacheKey);
          if (redisData) {
            const parsed = typeof redisData === 'string' ? JSON.parse(redisData) : redisData;
            if (parsed && Array.isArray(parsed.holders) && Number.isInteger(parsed.totalBurned)) {
              const success = cache.set(cacheKey, parsed);
              logger.info(
                'cache',
                `Loaded ${cacheKey} from Redis, cached: ${success}, holders: ${parsed.holders.length}`,
                chain,
                prefix.toLowerCase()
              );
              return parsed;
            } else {
              logger.warn('cache', `Invalid data in Redis for ${cacheKey}`, chain, prefix.toLowerCase());
            }
          }
        } catch (error) {
          logger.error(
            'cache',
            `Failed to load cache from Redis for ${cacheKey}: ${error.message}`,
            { stack: error.stack },
            chain,
            prefix.toLowerCase()
          );
        }
      }

      const cacheFile = path.join(cacheDir, `${prefix.toLowerCase()}_holders.json`);
      logger.debug('cache', `Attempting to read cache from ${cacheFile}`, chain, prefix.toLowerCase());
      try {
        const fileData = await fs.readFile(cacheFile, 'utf8');
        const parsed = JSON.parse(fileData);
        if (parsed && Array.isArray(parsed.holders) && Number.isInteger(parsed.totalBurned)) {
          const success = cache.set(cacheKey, parsed);
          logger.info(
            'cache',
            `Loaded ${cacheKey} from ${cacheFile}, cached: ${success}, holders: ${parsed.holders.length}`,
            chain,
            prefix.toLowerCase()
          );
          return parsed;
        } else {
          logger.warn('cache', `Invalid data in ${cacheFile}`, chain, prefix.toLowerCase());
        }
      } catch (error) {
        if (error.code !== 'ENOENT') {
          logger.error(
            'cache',
            `Failed to load cache from ${cacheFile}: ${error.message}`,
            { stack: error.stack },
            chain,
            prefix.toLowerCase()
          );
        } else {
          logger.debug('cache', `No cache file at ${cacheFile}`, chain, prefix.toLowerCase());
        }
      }
    }

    logger.info('cache', `Cache miss: ${cacheKey}`, chain, prefix.toLowerCase());
    return null;
  } catch (error) {
    logger.error('cache', `Failed to get cache ${prefix}_${key}: ${error.message}`, { stack: error.stack }, chain, prefix.toLowerCase());
    return null;
  }
}

export async function setCache(key, value, ttl, prefix) {
  const chain = config.nftContracts[prefix.toLowerCase()]?.chain || 'eth';
  try {
    const cacheKey = `${prefix}_${key}`;
    const success = cache.set(cacheKey, value); // NodeCache
    logger.info(
      'cache',
      `Set in-memory cache: ${cacheKey}, success: ${success}, holders: ${value.holders?.length || 'unknown'}`,
      chain,
      prefix.toLowerCase()
    );

    if (key === `${prefix.toLowerCase()}_holders` && Object.keys(config.nftContracts).map(k => k.toLowerCase()).includes(prefix.toLowerCase())) {
      const holdersCount = value.holders ? value.holders.length : 0;
      logger.info(
        'cache',
        `Persisting ${cacheKey} with ${holdersCount} holders, data: ${JSON.stringify(value).slice(0, 1000)}...`,
        chain,
        prefix.toLowerCase()
      );

      // Write to Redis if enabled
      if (redisEnabled && redis && process.env[`DISABLE_${prefix.toUpperCase()}_REDIS`] !== 'true') {
        try {
          logger.debug('cache', `Attempting to persist ${cacheKey} to Redis`, chain, prefix.toLowerCase());
          await redis.set(cacheKey, JSON.stringify(value));
          logger.info(
            'cache',
            `Persisted ${cacheKey} to Redis, holders: ${holdersCount}`,
            chain,
            prefix.toLowerCase()
          );
        } catch (error) {
          logger.error(
            'cache',
            `Failed to persist ${cacheKey} to Redis: ${error.message}`,
            { stack: error.stack },
            chain,
            prefix.toLowerCase()
          );
        }
      } else {
        logger.debug('cache', `Redis disabled for ${prefix} or not enabled, using filesystem`, chain, prefix.toLowerCase());
      }

      // Always write holders to filesystem
      const cacheFile = path.join(cacheDir, `${prefix.toLowerCase()}_holders.json`);
      logger.debug('cache', `Attempting to write ${cacheKey} to ${cacheFile}`, chain, prefix.toLowerCase());
      await ensureCacheDir(prefix);
      try {
        await fs.writeFile(cacheFile, JSON.stringify(value, null, 2));
        await fs.chmod(cacheFile, 0o644);
        logger.info(
          'cache',
          `Persisted ${cacheKey} to ${cacheFile}, holders: ${holdersCount}`,
          chain,
          prefix.toLowerCase()
        );
      } catch (error) {
        logger.error(
          'cache',
          `Failed to write cache file ${cacheFile}: ${error.message}`,
          { stack: error.stack },
          chain,
          prefix.toLowerCase()
        );
        throw error;
      }
    }
    // Non-holders keys are only stored in NodeCache (unless custom logic exists for events)
    return success;
  } catch (error) {
    logger.error('cache', `Failed to set cache ${prefix}_${key}: ${error.message}`, { stack: error.stack }, chain, prefix.toLowerCase());
    return false;
  }
}

export async function saveCacheState(key, state, prefix) {
  const chain = config.nftContracts[prefix.toLowerCase()]?.chain || 'eth';
  try {
    const cacheKey = `state_${key}`;
    cache.set(cacheKey, state, 0);
    if (redisEnabled && redis) {
      await redis.set(`state:${key}`, JSON.stringify(state), 'EX', 0);
    }
    logger.debug('cache', `Saved cache state for key: ${key}`, chain, prefix.toLowerCase());
    return true;
  } catch (error) {
    logger.error('cache', `Failed to save cache state for ${key}: ${error.message}`, { stack: error.stack }, chain, prefix.toLowerCase());
    throw error;
  }
}

export async function loadCacheState(key, prefix) {
  const chain = config.nftContracts[prefix.toLowerCase()]?.chain || 'eth';
  try {
    const cacheKey = `state_${key}`;
    let state = cache.get(cacheKey);
    if (state === undefined && redisEnabled && redis) {
      const redisData = await redis.get(`state:${key}`);
      state = redisData ? JSON.parse(redisData) : null;
      if (state) cache.set(cacheKey, state, 0);
    }
    logger.debug('cache', `Loaded cache state for key: ${key}`, chain, prefix.toLowerCase());
    return state;
  } catch (error) {
    logger.error('cache', `Failed to load cache state for ${key}: ${error.message}`, { stack: error.stack }, chain, prefix.toLowerCase());
    return null;
  }
}

export async function getTransactionReceipt(transactionHash) {
  try {
    const receipt = await client.getTransactionReceipt({ hash: transactionHash });
    logger.debug('utils', `Fetched transaction receipt for ${transactionHash}`, 'eth', 'general');
    return receipt;
  } catch (error) {
    logger.error('utils', `Failed to fetch transaction receipt for ${transactionHash}: ${error.message}`, { stack: error.stack }, 'eth', 'general');
    throw error;
  }
}

export async function validateContract(contractKey) {
  const chain = config.nftContracts[contractKey.toLowerCase()]?.chain || 'eth';
  const collection = contractKey.toLowerCase();
  try {
    // Log available contract keys for debugging
    const availableContracts = Object.keys(config.nftContracts);
    logger.debug(
      'cache',
      `Validating contract: received contractKey=${contractKey}, available contracts=${availableContracts.join(', ')}`,
      chain,
      collection
    );

    // Try exact match first
    let contractConfig = config.nftContracts[contractKey.toLowerCase()];
    let contractName = contractConfig?.name || contractKey;

    // If not found, try case-insensitive match
    if (!contractConfig) {
      const lowerKey = contractKey.toLowerCase();
      const matchingKey = availableContracts.find(key => key.toLowerCase() === lowerKey);
      if (matchingKey) {
        contractConfig = config.nftContracts[matchingKey];
        contractName = contractConfig.name || matchingKey;
        logger.warn(
          'cache',
          `Case mismatch for contractKey=${contractKey}, using matching key=${matchingKey}`,
          chain,
          collection
        );
      }
    }

    if (!contractConfig || !contractConfig.contractAddress) {
      logger.error(
        'cache',
        `No configuration found for contract key: ${contractKey}. Available contracts: ${JSON.stringify(Object.keys(config.nftContracts))}`,
        { configKeys: Object.keys(config.nftContracts), config: config.nftContracts },
        chain,
        collection
      );
      return false;
    }

    const address = contractConfig.contractAddress;
    logger.debug(
      'cache',
      `Validating contract: key=${contractKey}, name=${contractName}, address=${address}`,
      chain,
      collection
    );

    try {
      // Validate address format
      const formattedAddress = getAddress(address);
      if (!formattedAddress) {
        logger.error(
          'cache',
          `Invalid contract address format for ${contractName} (${contractKey}): ${address}`,
          {},
          chain,
          collection
        );
        return false;
      }

      const code = await client.getBytecode({ address: formattedAddress });
      const isValid = !!code && code !== '0x';
      logger.info(
        'cache',
        `Contract validation for ${contractName} (${address}): ${isValid ? 'valid' : 'invalid'}`,
        chain,
        collection
      );
      return isValid;
    } catch (error) {
      logger.error(
        'cache',
        `Failed to validate contract ${contractName} (${address}): ${error.message}`,
        { stack: error.stack },
        chain,
        collection
      );
      return false;
    }
  } catch (error) {
    logger.error(
      'cache',
      `Unexpected error validating contract ${contractKey}: ${error.message}`,
      { stack: error.stack },
      chain,
      collection
    );
    return false;
  }
}
----- app/api/utils/client.js -----

import { createPublicClient, http } from 'viem';
import { mainnet } from 'viem/chains';
import { Alchemy } from 'alchemy-sdk';
import config from '@/contracts/config';
import { logger } from '@/app/lib/logger';

const alchemyApiKey = config.alchemy.apiKey || process.env.NEXT_PUBLIC_ALCHEMY_API_KEY;
if (!alchemyApiKey) {
  logger.error('utils/client', 'Alchemy API key is missing', {}, 'eth', 'general');
  throw new Error('Alchemy API key is missing');
}

export const client = createPublicClient({
  chain: mainnet,
  transport: http(`https://eth-mainnet.g.alchemy.com/v2/${alchemyApiKey}`),
});

export const alchemy = new Alchemy({
  apiKey: config.alchemy.apiKey,
  network: 'eth-mainnet',
});
----- app/api/utils/logging.js -----

import { logger } from '@/app/lib/logger';

export async function log(scope, message, chain = 'eth', collection = 'general') {
  await logger.info(scope, message, chain, collection);
}
----- app/api/utils/retry.js -----

import { logger } from '@/app/lib/logger';

export async function retry(operation, { retries = 3, delay = 1000, backoff = false } = {}) {
  let lastError;
  for (let attempt = 1; attempt <= retries; attempt++) {
    try {
      return await operation();
    } catch (error) {
      lastError = error;
      if (error.message.includes('429') && attempt === retries) {
        logger.error('utils/retry', `Circuit breaker: Rate limit exceeded after ${retries} attempts`, {}, 'eth', 'general');
        throw new Error('Rate limit exceeded');
      }
      logger.warn('utils/retry', `Retry attempt ${attempt}/${retries} failed: ${error.message}`, 'eth', 'general');
      const waitTime = backoff ? delay * Math.pow(2, attempt - 1) : delay * Math.min(attempt, 3);
      await new Promise(resolve => setTimeout(resolve, waitTime));
    }
  }
  throw lastError;
}

================= Includes the following JS files under ./server =================
app/api/holders/Ascendant/route copy.js
app/api/holders/Ascendant/route.js
app/api/holders/E280/route.js
app/api/holders/Element280/route.js
app/api/holders/Element280/validate-burned/route.js
app/api/holders/Element369/route.js
app/api/holders/Stax/route.js
app/api/holders/[contract]/progress/route.js
app/api/holders/[contract]/route.js
app/api/holders/blockchain/events.js
app/api/holders/blockchain/multicall.js
app/api/holders/blockchain/owners.js
app/api/holders/cache/holders.js
app/api/holders/cache/state.js
app/api/utils.js
app/api/utils/cache.js
app/api/utils/client.js
app/api/utils/logging.js
app/api/utils/retry.js
